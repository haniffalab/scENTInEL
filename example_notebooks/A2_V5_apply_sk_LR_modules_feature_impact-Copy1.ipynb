{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mineral-courage",
   "metadata": {},
   "source": [
    "# SCENTInEL\n",
    "#### sc ElasticNeT integrative ensemble learning\n",
    "\n",
    "# SCENTEL\n",
    "#### sc ElasticNeT ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-carry",
   "metadata": {},
   "source": [
    "# LR multi-tissue cross-comparison\n",
    "\n",
    "##### Ver:: A1_V5\n",
    "##### Author(s) : Issac Goh\n",
    "##### Date : 220823;YYMMDD\n",
    "### Author notes\n",
    "    - Current defaults scrpae data from web, so leave as default and run\n",
    "    - slices model and anndata to same feature shape, scales anndata object\n",
    "    - added some simple benchmarking\n",
    "    - creates dynamic cutoffs for probability score (x*sd of mean) in place of more memory intensive confidence scoring\n",
    "    - Does not have majority voting set on as default, but module does exist\n",
    "    - Multinomial logistic relies on the (not always realistic) assumption of independence of irrelevant alternatives whereas a series of binary logistic predictions does not. collinearity is assumed to be relatively low, as it becomes difficult to differentiate between the impact of several variables if this is not the case\n",
    "    \n",
    "### Features to add\n",
    "    - Add ability to consume anndata zar format for sequential learning\n",
    "       - Feature assessment weighted by classifications made in query data based on bayes factor of variable expression \n",
    "    - Bayesian sampling (KNN-based)\n",
    "    - Bayesian optimisation layer\n",
    "    - Bayesian scoring of pribabilities in match samples\n",
    "    - Weighted R2 per-class model performance scoring, compared to global.\n",
    "        - In joint latent representation; \n",
    "        - Single Bayesian optimisded model\n",
    "        - R2(hat) * Prob(hat) computed per model\n",
    "        i.e for 3 data case:\n",
    "        - Three probs: \n",
    "            - local model self-projected probs\n",
    "            - Global model projection probs\n",
    "            - Every other model inductive projection across\n",
    "        - Does global agree with local/other model? \n",
    "        - Aggregate the global and each projection? -- Does the model perform well in global space & in each individual model? \n",
    "        - Probability of harmonisation\n",
    "            Aggregate? concat?\n",
    "### Modes to run in\n",
    "    - Run in training mode\n",
    "    - Run in projection mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electronic-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# import pkg_resources\n",
    "# required = {'harmonypy','sklearn','scanpy','pandas', 'numpy', 'scipy', 'matplotlib', 'seaborn' ,'scipy'}\n",
    "# installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "# missing = required - installed\n",
    "# if missing:\n",
    "#    print(\"Installing missing packages:\" )\n",
    "#    print(missing)\n",
    "#    python = sys.executable\n",
    "#    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "#from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import psutil\n",
    "import random\n",
    "import threading\n",
    "import tracemalloc\n",
    "import itertools\n",
    "import math\n",
    "import warnings\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "final-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "'pan_fetal':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/adifa_lr/celltypist_model.Pan_Fetal_Human.pkl',\n",
    "'pan_fetal_wget':'https://celltypist.cog.sanger.ac.uk/models/Pan_Fetal_Suo/v2/Pan_Fetal_Human.pkl',\n",
    "'adata_scvi':'/nfs/team205/ig7/mount/gdrive/g_cloud/projects/amniontic_fluid/scvi_low_dim_model.sav',\n",
    "'adata_ldvae':'/nfs/team205/ig7/mount/gdrive/g_cloud/projects/amniontic_fluid/ldvae_low_dim_model.sav',\n",
    "'adata_harmony':'/nfs/team205/ig7/work_backups/backup_210306/projects/amiotic_fluid/train_low_dim_model/organ_low_dim_model.sav',\n",
    "'test_low_dim_ipsc_ys':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_030522_notebooks/Integrating_HM_data_030522/YS_logit/lr_model.sav',\n",
    "'YS_X':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/resources/YS_X_model_080922.sav',\n",
    "'YS_X_V3':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/rebuttal_figs_010922/train_YS_full_X_model/YS_X_A2_V12_lvl3_ELASTICNET_YS.sav',\n",
    "'SK_model':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/LR_app_format/hudaa_skin/for_hudaa_A1_V2',\n",
    "'Hudaa_model_trained':'/nfs/team298/hg6/Fetal_skin/LR_15012023/train-all_model.pkl',\n",
    "\n",
    "}\n",
    "\n",
    "adatas_dict = {\n",
    "'Fetal_skin_raw': '/nfs/team298/hg6/Fetal_skin/data/FS_raw_sub.h5ad',\n",
    "'vascular_organoid': '/nfs/team298/hg6/Fetal_skin/data/vasc_org_raw.h5ad',\n",
    "'YS':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V5_scvi_YS_integrated/A2_V5_scvi_YS_integrated_raw_qc_scr_umap.h5ad',\n",
    "'YS_test':'/nfs/team205/ig7/resources/scripts_dont_modify/logit_regression_models/LR_app_format/ys_test_data.h5ad',\n",
    "'YS_A2_V10_X_raw':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V10_scvi_YS_integrated/A2_V10_raw_counts_full_no_obs.h5ad',\n",
    "'YS_A2_V10_X':'/nfs/team205/ig7/work_backups/backup_210306/projects/YS/YS_data/Submission_2_data/A2_V10_scvi_YS_integrated/A2_V10_qc_raw.h5ad'\n",
    "}\n",
    "\n",
    "# Variable assignment\n",
    "train_model = False\n",
    "feat_use = 'joint_annotation_20220202'\n",
    "adata_key = 'Fetal_skin_raw'#'fliv_wget_test' # key for dictionary entry containing local or web path to adata/s can be either url or local \n",
    "data_merge = False # read and merge multiple adata (useful, but keep false for now)\n",
    "model_key = 'SK_model'#'test_low_dim_ipsc_ys'# key for model of choice can be either url or local \n",
    "train_x_partition = 'X' # what partition was the data trained on? To keep simple, for now only accepts 'X'\n",
    "dyn_std = 1.96 # Dynamic cutoffs using std of the mean for each celltype probability, gives a column notifying user of uncertain labels 1 == 68Ci, 1.96 = 95CI\n",
    "freq_redist = 'joint_annotation_20220202'#'cell.labels'#'False#'cell.labels'#False # False or key of column in anndata object which contains labels/clusters // not currently implemented\n",
    "partial_scale = True # should data be scaled in batches?\n",
    "QC_normalise = True # should data be normalised?\n",
    "\n",
    "# training variables\n",
    "penalty='elasticnet' # can be [\"l1\",\"l2\",\"elasticnet\"]\n",
    "sparcity=0.5 # C penalty for degree of regularisation\n",
    "thread_num = -1\n",
    "l1_ratio = 0.5 # ratio between L1 and L2 regulrisatiuon depending on penatly method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-findings",
   "metadata": {},
   "source": [
    "# Partial scaling ver\n",
    "- scale across 10 mini bulks/every 100,000 cells\n",
    "- sequential learning for scaling\n",
    "- sequential application of scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "congressional-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "#from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import psutil\n",
    "import random\n",
    "import threading\n",
    "import tracemalloc\n",
    "import itertools\n",
    "import math\n",
    "import warnings\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "import mygene\n",
    "import gseapy as gp\n",
    "import mygene\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_models(model_dict,model_run):\n",
    "    if (Path(model_dict[model_run])).is_file():\n",
    "        # Load data (deserialize)\n",
    "        model = pkl.load(open(model_dict[model_run], \"rb\"))\n",
    "        return model\n",
    "    elif 'http' in model_dict[model_run]:\n",
    "        print('Loading model from web source')\n",
    "        r_get = requests.get(model_dict[model_run])\n",
    "        fpath = './model_temp.sav'\n",
    "        open(fpath , 'wb').write(r_get.content)\n",
    "        model = pkl.load(open(fpath, \"rb\"))\n",
    "        return model\n",
    "\n",
    "def load_adatas(adatas_dict,data_merge, data_key_use,QC_normalise):\n",
    "    if data_merge == True:\n",
    "        # Read\n",
    "        gene_intersect = {} # unused here\n",
    "        adatas = {}\n",
    "        for dataset in adatas_dict.keys():\n",
    "            if 'https' in adatas_dict[dataset]:\n",
    "                print('Loading anndata from web source')\n",
    "                adatas[dataset] = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[dataset])\n",
    "            adatas[dataset] = sc.read(data[dataset])\n",
    "            adatas[dataset].var_names_make_unique()\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            gene_intersect[dataset] = list(adatas[dataset].var.index)\n",
    "        adata = list(adatas.values())[0].concatenate(list(adatas.values())[1:],join='inner')\n",
    "        return adatas, adata\n",
    "    elif data_merge == False:\n",
    "        if 'https' in adatas_dict[data_key_use]:\n",
    "            print('Loading anndata from web source')\n",
    "            adata = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[data_key_use])\n",
    "        else: \n",
    "            adata = sc.read(adatas_dict[data_key_use])\n",
    "    if QC_normalise == True:\n",
    "        print('option to apply standardisation to data detected, performing basic QC filtering')\n",
    "        sc.pp.filter_cells(adata, min_genes=200)\n",
    "        sc.pp.filter_genes(adata, min_cells=3)\n",
    "        sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        \n",
    "    return adata\n",
    "\n",
    "# resource usage logger\n",
    "class DisplayCPU(threading.Thread):\n",
    "    def run(self):\n",
    "        tracemalloc.start()\n",
    "        starting, starting_peak = tracemalloc.get_traced_memory()\n",
    "        self.running = True\n",
    "        self.starting = starting\n",
    "        currentProcess = psutil.Process()\n",
    "        cpu_pct = []\n",
    "        peak_cpu = 0\n",
    "        while self.running:\n",
    "            peak_cpu = 0\n",
    "#           time.sleep(3)\n",
    "#             print('CPU % usage = '+''+ str(currentProcess.cpu_percent(interval=1)))\n",
    "#             cpu_pct.append(str(currentProcess.cpu_percent(interval=1)))\n",
    "            cpu = currentProcess.cpu_percent()\n",
    "        # track the peak utilization of the process\n",
    "            if cpu > peak_cpu:\n",
    "                peak_cpu = cpu\n",
    "                peak_cpu_per_core = peak_cpu/psutil.cpu_count()\n",
    "        self.peak_cpu = peak_cpu\n",
    "        self.peak_cpu_per_core = peak_cpu_per_core\n",
    "        \n",
    "    def stop(self):\n",
    "#        cpu_pct = DisplayCPU.run(self)\n",
    "        self.running = False\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        return current, peak\n",
    "    \n",
    "# projection module\n",
    "def reference_projection(adata, model, dyn_std,partial_scale):\n",
    "    \n",
    "    class adata_temp:\n",
    "        pass\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    print('Determining model flavour')\n",
    "    try:\n",
    "        model_lr =  model['Model']\n",
    "        print('Consuming celltypist model')\n",
    "    except:# hasattr(model, 'coef_'):\n",
    "        print('Consuming non-celltypist model')\n",
    "        model_lr =  model\n",
    "    print(model_lr)\n",
    "    if train_x_partition == 'X':\n",
    "        print('Matching reference genes in the model')\n",
    "        k_x = np.isin(list(adata.var.index), list(model_lr.features))\n",
    "        if k_x.sum() == 0:\n",
    "            raise ValueError(f\"ðŸ›‘ No features overlap with the model. Please provide gene symbols\")\n",
    "        print(f\"ðŸ§¬ {k_x.sum()} features used for prediction\")\n",
    "        #slicing adata\n",
    "        k_x_idx = np.where(k_x)[0]\n",
    "        # adata_temp = adata[:,k_x_idx]\n",
    "        adata_temp.var = adata[:,k_x_idx].var\n",
    "        adata_temp.X = adata[:,k_x_idx].X\n",
    "        adata_temp.obs = adata[:,k_x_idx].obs\n",
    "        lr_idx = pd.DataFrame(model_lr.features, columns=['features']).reset_index().set_index('features').loc[list(adata_temp.var.index)].values\n",
    "        # adata_arr = adata_temp.X[:,list(lr_idexes['index'])]\n",
    "        # slice and reorder model\n",
    "        ni, fs, cf = model_lr.n_features_in_, model_lr.features, model_lr.coef_\n",
    "        model_lr.n_features_in_ = lr_idx.size\n",
    "        model_lr.features = np.array(model_lr.features)[lr_idx]\n",
    "        model_lr.coef_ = np.squeeze(model_lr.coef_[:,lr_idx]) #model_lr.coef_[:, lr_idx]\n",
    "        if partial_scale == True:\n",
    "            print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "            # Partial scaling alg\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "            n = adata_temp.X.shape[0]  # number of rows\n",
    "            # set dyn scale packet size\n",
    "            x_len = len(adata_temp.var)\n",
    "            y_len = len(adata.obs)\n",
    "            if y_len < 100000:\n",
    "                dyn_pack = int(x_len/10)\n",
    "                pack_size = dyn_pack\n",
    "            else:\n",
    "                # 10 pack for every 100,000\n",
    "                dyn_pack = int((y_len/100000)*10)\n",
    "                pack_size = int(x_len/dyn_pack)\n",
    "            batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "            index = 0  # helper-var\n",
    "            while index < n:\n",
    "                partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "                partial_x = adata_temp.X[index:index+partial_size]\n",
    "                scaler.partial_fit(partial_x)\n",
    "                index += partial_size\n",
    "            adata_temp.X = scaler.transform(adata_temp.X)\n",
    "    # model projections\n",
    "    print('Starting reference projection!')\n",
    "    if train_x_partition == 'X':\n",
    "        train_x = adata_temp.X\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "        \n",
    "    elif train_x_partition in list(adata.obsm.keys()): \n",
    "        print('{low_dim: this partition modality is still under development!}')\n",
    "        train_x = adata.obsm[train_x_partition]\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "    \n",
    "    else:\n",
    "        print('{this partition modality is still under development!}')\n",
    "    ## insert modules for low dim below\n",
    "\n",
    "    # Simple dynamic confidence calling\n",
    "    pred_out['confident_calls'] = pred_out['predicted']\n",
    "    pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'] = pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'].astype(str) + '_uncertain'\n",
    "    # means_ = self.model.scaler.mean_[lr_idx] if self.model.scaler.with_mean else 0\n",
    "    return(pred_out,train_x,model_lr,adata_temp)\n",
    "\n",
    "def freq_redist_68CI(adata,clusters_reassign):\n",
    "    if freq_redist != False:\n",
    "        print('Frequency redistribution commencing')\n",
    "        cluster_prediction = \"consensus_clus_prediction\"\n",
    "        lr_predicted_col = 'predicted'\n",
    "        pred_out[clusters_reassign] = adata.obs[clusters_reassign].astype(str)\n",
    "        reassign_classes = list(pred_out[clusters_reassign].unique())\n",
    "        lm = 1 # lambda value\n",
    "        pred_out[cluster_prediction] = pred_out[clusters_reassign]\n",
    "        for z in pred_out[clusters_reassign][pred_out[clusters_reassign].isin(reassign_classes)].unique():\n",
    "            df = pred_out\n",
    "            df = df[(df[clusters_reassign].isin([z]))]\n",
    "            df_count = pd.DataFrame(df[lr_predicted_col].value_counts())\n",
    "            # Look for classificationds > 68CI\n",
    "            if len(df_count) > 1:\n",
    "                df_count_temp = df_count[df_count[lr_predicted_col]>int(int(df_count.mean()) + (df_count.std()*lm))]\n",
    "                if len(df_count_temp >= 1):\n",
    "                    df_count = df_count_temp\n",
    "            #print(df_count)     \n",
    "            freq_arranged = df_count.index\n",
    "            cat = freq_arranged[0]\n",
    "        #Make the cluster assignment first\n",
    "            pred_out[cluster_prediction] = pred_out[cluster_prediction].astype(str)\n",
    "            pred_out.loc[pred_out[clusters_reassign] == z, [cluster_prediction]] = cat\n",
    "        # Create assignments for any classification >68CI\n",
    "            for cats in freq_arranged:\n",
    "                #print(cats)\n",
    "                cats_assignment = cats#.replace(data1,'') + '_clus_prediction'\n",
    "                pred_out.loc[(pred_out[clusters_reassign] == z) & (pred_out[lr_predicted_col] == cats),[cluster_prediction]] = cats_assignment\n",
    "        min_counts = pd.DataFrame((pred_out[cluster_prediction].value_counts()))\n",
    "        reassign = list(min_counts.index[min_counts[cluster_prediction]<=2])\n",
    "        pred_out[cluster_prediction] = pred_out[cluster_prediction].str.replace(str(''.join(reassign)),str(''.join(pred_out.loc[pred_out[clusters_reassign].isin(list(pred_out.loc[(pred_out[cluster_prediction].isin(reassign)),clusters_reassign])),lr_predicted_col].value_counts().head(1).index.values)))\n",
    "        return pred_out\n",
    "\n",
    "### Feature importance notes\n",
    "#- If we increase the x feature one unit, then the prediction will change e to the power of its weight. We can apply this rule to the all weights to find the feature importance.\n",
    "#- We will calculate the Euler number to the power of its coefficient to find the importance.\n",
    "#- To sum up an increase of x feature by one unit increases the odds of being versicolor class by a factor of x[importance] when all other features remain the same.\n",
    "#- For low-dim, we look at the distribution of e^coef per class, we extract the \n",
    "# class coef_extract:\n",
    "#     def __init__(self, model,features, pos):\n",
    "# #         self.w = list(itertools.chain(*(model.coef_[pos]).tolist())) #model.coef_[pos]\n",
    "#         self.w = model.coef_[class_pred_pos]\n",
    "#         self.features = features \n",
    "def long_format_features(top_loadings):\n",
    "    p = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_e^coef\")]\n",
    "    p = pd.melt(p)\n",
    "    n = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_feature\")]\n",
    "    n = pd.melt(n)\n",
    "    l = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_coef\")]\n",
    "    l = pd.melt(l)\n",
    "    n = n.replace(regex=r'_feature', value='')\n",
    "    n = n.rename(columns={\"variable\": \"class\", \"value\": \"feature\"})\n",
    "    p = (p.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"e^coef\"})\n",
    "    l = (l.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"coef\"})\n",
    "    concat = pd.concat([n,p,l],axis=1)\n",
    "    return concat\n",
    "\n",
    "def model_feature_sf(long_format_feature_importance, coef_use):\n",
    "        long_format_feature_importance[str(coef_use) + '_pval'] = 'NaN'\n",
    "        for class_lw in long_format_feature_importance['class'].unique():\n",
    "            df_loadings = long_format_feature_importance[long_format_feature_importance['class'].isin([class_lw])]\n",
    "            comps = coef_use #'e^coef'\n",
    "            U = np.mean(df_loadings[comps])\n",
    "            std = np.std(df_loadings[comps])\n",
    "            med =  np.median(df_loadings[comps])\n",
    "            mad = np.median(np.absolute(df_loadings[comps] - np.median(df_loadings[comps])))\n",
    "            # Survival function scaled by 1.4826 of MAD (approx norm)\n",
    "            pvals = scipy.stats.norm.sf(df_loadings[comps], loc=med, scale=1.4826*mad) # 95% CI of MAD <10,000 samples\n",
    "            #pvals = scipy.stats.norm.sf(df_loadings[comps], loc=U, scale=1*std)\n",
    "            df_loadings[str(comps) +'_pval'] = pvals\n",
    "            long_format_feature_importance.loc[long_format_feature_importance.index.isin(df_loadings.index)] = df_loadings\n",
    "        long_format_feature_importance['is_significant_sf'] = False\n",
    "        long_format_feature_importance.loc[long_format_feature_importance[coef_use+ '_pval']<0.05,'is_significant_sf'] = True\n",
    "        return long_format_feature_importance\n",
    "# Apply SF to e^coeff mat data\n",
    "#         pval_mat = pd.DataFrame(columns = mat.columns)\n",
    "#         for class_lw in mat.index:\n",
    "#             df_loadings = mat.loc[class_lw]\n",
    "#             U = np.mean(df_loadings)\n",
    "#             std = np.std(df_loadings)\n",
    "#             med =  np.median(df_loadings)\n",
    "#             mad = np.median(np.absolute(df_loadings - np.median(df_loadings)))\n",
    "#             pvals = scipy.stats.norm.sf(df_loadings, loc=med, scale=1.96*U)\n",
    "\n",
    "class estimate_important_features: # This calculates feature effect sizes of the model\n",
    "    def __init__(self, model, top_n):\n",
    "        print('Estimating feature importance')\n",
    "        classes =  list(model.classes_)\n",
    "         # get feature names\n",
    "        try:\n",
    "            model_features = list(itertools.chain(*list(model.features)))\n",
    "        except:\n",
    "            warnings.warn('no features recorded in data, naming features by position')\n",
    "            print('if low-dim lr was submitted, run linear decoding function to obtain true feature set')\n",
    "            model_features = list(range(0,model.coef_.shape[1]))\n",
    "            model.features = model_features\n",
    "        print('Calculating the Euler number to the power of coefficients')\n",
    "        impt_ = pow(math.e,model.coef_)\n",
    "        try:\n",
    "            self.euler_pow_mat = pd.DataFrame(impt_,columns = list(itertools.chain(*list(model.features))),index = list(model.classes_))\n",
    "        except:\n",
    "            self.euler_pow_mat = pd.DataFrame(impt_,columns = list(model.features),index = list(model.classes_))\n",
    "        self.top_n_features = pd.DataFrame(index = list(range(0,top_n)))\n",
    "        # estimate per class feature importance\n",
    "        \n",
    "        print('Estimating feature importance for each class')\n",
    "        mat = self.euler_pow_mat\n",
    "        for class_pred_pos in list(range(0,len(mat.T.columns))):\n",
    "            class_pred = list(mat.T.columns)[class_pred_pos]\n",
    "            #     print(class_pred)\n",
    "            temp_mat =  pd.DataFrame(mat.T[class_pred])\n",
    "            temp_mat['coef'] = model.coef_[class_pred_pos]\n",
    "            temp_mat = temp_mat.sort_values(by = [class_pred], ascending=False)\n",
    "            temp_mat = temp_mat.reset_index()\n",
    "            temp_mat.columns = ['feature','e^coef','coef']\n",
    "            temp_mat = temp_mat[['feature','e^coef','coef']]\n",
    "            temp_mat.columns =str(class_pred)+ \"_\" + temp_mat.columns\n",
    "            self.top_n_features = pd.concat([self.top_n_features,temp_mat.head(top_n)], join=\"inner\",ignore_index = False, axis=1)\n",
    "            self.to_n_features_long = model_feature_sf(long_format_features(self.top_n_features),'e^coef')\n",
    "            \n",
    "    \n",
    "    # plot class-wise features\n",
    "def model_class_feature_plots(top_loadings, classes, comps):\n",
    "    import matplotlib.pyplot as plt\n",
    "    for class_temp in classes:\n",
    "        class_lw = class_temp\n",
    "        long_format = top_loadings\n",
    "        df_loadings = long_format[long_format['class'].isin([class_lw])]\n",
    "        plt.hist(df_loadings[comps])\n",
    "        for i in ((df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]).unique()):\n",
    "            plt.axvline(x=i,color='red')\n",
    "        med = np.median(df_loadings[comps])\n",
    "        plt.axvline(x=med,color='blue')\n",
    "        plt.xlabel('feature_importance', fontsize=12)\n",
    "        plt.title(class_lw)\n",
    "        #plt.axvline(x=med,color='pink')\n",
    "        df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]\n",
    "        print(len(df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]))\n",
    "        #Plot feature ranking\n",
    "        plot_loading = pd.DataFrame(pd.DataFrame(df_loadings[comps][df_loadings[str(comps) +'_pval']<0.05]).iloc[:,0].sort_values(ascending=False))\n",
    "        table = plt.table(cellText=plot_loading.values,colWidths = [1]*len(plot_loading.columns),\n",
    "        rowLabels= list(df_loadings['feature'][df_loadings.index.isin(plot_loading.index)].reindex(plot_loading.index)), #plot_loading.index,\n",
    "        colLabels=plot_loading.columns,\n",
    "        cellLoc = 'center', rowLoc = 'center',\n",
    "        loc='right', bbox=[1.4, -0.05, 0.5,1])\n",
    "        table.scale(1, 2)\n",
    "        table.set_fontsize(10)\n",
    "        \n",
    "def report_f1(model,train_x, train_label):\n",
    "    ## Report accuracy score\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "    from sklearn import metrics\n",
    "    import seaborn as sn\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=1)\n",
    "    # # evaluate the model and collect the scores\n",
    "    # n_scores = cross_val_score(lr, train_x, train_label, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # # report the model performance\n",
    "    # print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "    # Report Precision score\n",
    "    metric = pd.DataFrame((metrics.classification_report(train_label, model.predict(train_x), digits=2,output_dict=True))).T\n",
    "    cm = confusion_matrix(train_label, model.predict(train_x))\n",
    "    #cm = confusion_matrix(train_label, model.predict_proba(train_x))\n",
    "    df_cm = pd.DataFrame(cm, index = model.classes_,columns = model.classes_)\n",
    "    df_cm = (df_cm / df_cm.sum(axis=0))*100\n",
    "    plt.figure(figsize = (20,15))\n",
    "    sn.set(font_scale=1) # for label size\n",
    "    pal = sns.diverging_palette(240, 10, n=10)\n",
    "    #plt.suptitle(('Mean Accuracy 5 fold: %.3f std: %.3f' % (np.mean(n_scores),  np.std(n_scores))), y=1.05, fontsize=18)\n",
    "    #Plot precision recall and recall\n",
    "    table = plt.table(cellText=metric.values,colWidths = [1]*len(metric.columns),\n",
    "    rowLabels=metric.index,\n",
    "    colLabels=metric.columns,\n",
    "    cellLoc = 'center', rowLoc = 'center',\n",
    "    loc='bottom', bbox=[0.25, -0.6, 0.5, 0.3])\n",
    "    table.scale(1, 2)\n",
    "    table.set_fontsize(10)\n",
    "\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},cmap=pal) # font size\n",
    "    print(metrics.classification_report(train_label, model.predict(train_x), digits=2))\n",
    "\n",
    "def subset_top_hvgs(adata_lognorm, n_top_genes):\n",
    "    dispersion_norm = adata_lognorm.var['dispersions_norm'].values.astype('float32')\n",
    "\n",
    "    dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]\n",
    "    dispersion_norm[\n",
    "                ::-1\n",
    "            ].sort()  # interestingly, np.argpartition is slightly slower\n",
    "\n",
    "    disp_cut_off = dispersion_norm[n_top_genes - 1]\n",
    "    gene_subset = adata_lognorm.var['dispersions_norm'].values >= disp_cut_off\n",
    "    return(adata_lognorm[:,gene_subset])\n",
    "\n",
    "def prep_scVI(adata, \n",
    "              n_hvgs = 5000,\n",
    "              remove_cc_genes = True,\n",
    "              remove_tcr_bcr_genes = False\n",
    "             ):\n",
    "    ## Remove cell cycle genes\n",
    "    if remove_cc_genes:\n",
    "        adata = panfetal_utils.remove_geneset(adata,genes.cc_genes)\n",
    "\n",
    "    ## Remove TCR/BCR genes\n",
    "    if remove_tcr_bcr_genes:\n",
    "        adata = panfetal_utils.remove_geneset(adata, genes.IG_genes)\n",
    "        adata = panfetal_utils.remove_geneset(adata, genes.TCR_genes)\n",
    "        \n",
    "    ## HVG selection\n",
    "    adata = subset_top_hvgs(adata, n_top_genes=n_hvgs)\n",
    "    return(adata)\n",
    "\n",
    "# Modified LR train module, does not work with low-dim by default anymore, please use low-dim adapter\n",
    "def LR_train(adata, train_x, train_label, penalty='elasticnet', sparcity=0.2,max_iter=200,l1_ratio =0.2,tune_hyper_params =False,n_splits=5, n_repeats=3,l1_grid = [0.01,0.2,0.5,0.8], c_grid = [0.01,0.2,0.4,0.6]):\n",
    "    if tune_hyper_params == True:\n",
    "        train_labels=train_label\n",
    "        results = tune_lr_model(adata, train_x_partition = train_x, random_state = 42,  train_labels = train_labels, n_splits=n_splits, n_repeats=n_repeats,l1_grid = l1_grid, c_grid = c_grid)\n",
    "        print('hyper_params tuned')\n",
    "        sparcity = results.best_params_['C']\n",
    "        l1_ratio = results.best_params_['l1_ratio']\n",
    "    \n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, n_jobs=thread_num)\n",
    "    if (penalty == \"l1\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual = True, solver = 'liblinear',multi_class = 'ovr', n_jobs=thread_num ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'ovr', n_jobs=thread_num)\n",
    "    if train_x == 'X':\n",
    "        subset_train = adata.obs.index\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#        train_label = train_label[subset_train]\n",
    "        train_x = adata.X#[adata.obs.index.isin(list(adata.obs[subset_train].index))]\n",
    "#        predict_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_predict].index))]\n",
    "    elif train_x in adata.obsm.keys():\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#         train_label = train_label[subset_train]\n",
    "        train_x = adata.obsm[train_x]\n",
    "#        predict_x = train_x\n",
    "#        train_x = train_x[subset_train, :]\n",
    "        # Define prediction parameters\n",
    "#        predict_x = predict_x[subset_predict]\n",
    "#        predict_x = pd.DataFrame(predict_x)\n",
    "#        predict_x.index = adata.obs[subset_predict].index\n",
    "    # Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "    model = lr.fit(train_x, train_label)\n",
    "    model.features = np.array(adata.var.index)\n",
    "    return model\n",
    "\n",
    "def tune_lr_model(adata, train_x_partition = 'X', random_state = 42, use_bayes_opt=True, train_labels = None, n_splits=5, n_repeats=3,l1_grid = [0.1,0.2,0.5,0.8], c_grid = [0.1,0.2,0.4,0.6]):\n",
    "    import bless as bless\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from numpy import arange\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from skopt import BayesSearchCV\n",
    "\n",
    "    # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "    r = np.random.RandomState(random_state)\n",
    "    if train_x_partition in adata.obsm.keys():\n",
    "        lvg = bless.bless(tune_train_x, RBF(length_scale=20), lam_final = 2, qbar = 2, random_state = r, H = 10, force_cpu=True)\n",
    "    #     try:\n",
    "    #         import cupy\n",
    "    #         lvg_2 = bless(adata.obsm[train_x_partition], RBF(length_scale=10), 10, 10, r, 10, force_cpu=False)\n",
    "    #     except ImportError:\n",
    "    #         print(\"cupy not found, defaulting to numpy\")\n",
    "        adata_tuning = adata[lvg.idx]\n",
    "        tune_train_x = adata_tuning.obsm[train_x_partition][:]\n",
    "    else:\n",
    "        print('no latent representation provided, random sampling instead')\n",
    "        prop = 0.1\n",
    "        random_vertices = []\n",
    "        n_ixs = int(len(adata.obs) * prop)\n",
    "        random_vertices = random.sample(list(range(len(adata.obs))), k=n_ixs)\n",
    "        adata_tuning = adata[random_vertices]\n",
    "        tune_train_x = adata_tuning.X\n",
    "        \n",
    "    if not train_labels == None:\n",
    "        tune_train_label = adata_tuning.obs[train_labels]\n",
    "    elif train_labels == None:\n",
    "        try:\n",
    "            print('no training labels provided, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        except:\n",
    "            print('no training labels provided, no neighbors, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        tune_train_label = adata_tuning.obs['leiden']\n",
    "    ## tune regularization for multinomial logistic regression\n",
    "    print('starting tuning loops')\n",
    "    X = tune_train_x\n",
    "    y = tune_train_label\n",
    "    grid = dict()\n",
    "    # define model\n",
    "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "    #model = LogisticRegression(penalty = penalty, max_iter =  200, dual=False,solver = 'saga', multi_class = 'multinomial',)\n",
    "    model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, n_jobs=4)\n",
    "    if (penalty == \"l1\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, dual = True, solver = 'liblinear',multi_class = 'multinomial', n_jobs=4 ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'multinomial', n_jobs=4) # use multinomial class if probabilities are descrete\n",
    "        grid['l1_ratio'] = l1_grid\n",
    "    grid['C'] = c_grid\n",
    "    \n",
    "    if use_bayes_opt == True:\n",
    "        # define search space\n",
    "        search_space = {'C': (np.min(c_grid), np.max(c_grid), 'log-uniform'), \n",
    "                        'l1_ratio': (np.min(l1_grid), np.max(l1_grid), 'uniform') if 'elasticnet' in penalty else None}\n",
    "        # define search\n",
    "        search = BayesSearchCV(model, search_space, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "        # perform the search\n",
    "        results = search.fit(X, y)\n",
    "    else:\n",
    "        # define search\n",
    "        search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "        # perform the search\n",
    "        results = search.fit(X, y)\n",
    "    # summarize\n",
    "    print('MAE: %.3f' % results.best_score_)\n",
    "    print('Config: %s' % results.best_params_)\n",
    "    return results\n",
    "\n",
    "def prep_training_data(adata_temp,feat_use,batch_key, model_key, batch_correction=False, var_length = 7500,penalty='elasticnet',sparcity=0.2,max_iter = 200,l1_ratio = 0.1,partial_scale=True,train_x_partition ='X',theta = 3,tune_hyper_params=False ):\n",
    "    model_name = model_key + '_lr_model'\n",
    "    print('performing highly variable gene selection')\n",
    "    sc.pp.highly_variable_genes(adata_temp, batch_key = batch_key, subset=False)\n",
    "    adata_temp = subset_top_hvgs(adata_temp,var_length)\n",
    "    #scale the input data\n",
    "    if partial_scale == True:\n",
    "        print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "        # Partial scaling alg\n",
    "        #adata_temp.X = (adata_temp.X)\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        n = adata_temp.X.shape[0]  # number of rows\n",
    "        # set dyn scale packet size\n",
    "        x_len = len(adata_temp.var)\n",
    "        y_len = len(adata_temp.obs)\n",
    "        if y_len < 100000:\n",
    "            dyn_pack = int(x_len/10)\n",
    "            pack_size = dyn_pack\n",
    "        else:\n",
    "            # 10 pack for every 100,000\n",
    "            dyn_pack = int((y_len/100000)*10)\n",
    "            pack_size = int(x_len/dyn_pack)\n",
    "        batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "        index = 0  # helper-var\n",
    "        while index < n:\n",
    "            partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "            partial_x = adata_temp.X[index:index+partial_size]\n",
    "            scaler.partial_fit(partial_x)\n",
    "            index += partial_size\n",
    "        adata_temp.X = scaler.transform(adata_temp.X)\n",
    "#     else:\n",
    "#         sc.pp.scale(adata_temp, zero_center=True, max_value=None, copy=False, layer=None, obsm=None)\n",
    "    if (train_x_partition != 'X') & (train_x_partition in adata_temp.obsm.keys()):\n",
    "        print('train partition is not in OBSM, defaulting to PCA')\n",
    "        # Now compute PCA\n",
    "        sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "        sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "        \n",
    "        # Batch correction options\n",
    "        # The script will test later which Harmony values we should use \n",
    "        if(batch_correction == \"Harmony\"):\n",
    "            print(\"Commencing harmony\")\n",
    "            adata_temp.obs['lr_batch'] = adata_temp.obs[batch_key]\n",
    "            batch_var = \"lr_batch\"\n",
    "            # Create hm subset\n",
    "            adata_hm = adata_temp[:]\n",
    "            # Set harmony variables\n",
    "            data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "            meta_data = adata_hm.obs\n",
    "            vars_use = [batch_var]\n",
    "            # Run Harmony\n",
    "            ho = hm.run_harmony(data_mat, meta_data, vars_use,theta=theta)\n",
    "            res = (pd.DataFrame(ho.Z_corr)).T\n",
    "            res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "            # Insert coordinates back into object\n",
    "            adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "            adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "            # Run neighbours\n",
    "            #sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            adata_temp = adata_hm[:]\n",
    "            del adata_hm\n",
    "        elif(batch_correction == \"BBKNN\"):\n",
    "            print(\"Commencing BBKNN\")\n",
    "            sc.external.pp.bbknn(adata_temp, batch_key=batch_var, approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=15) \n",
    "        print(\"adata1 and adata2 are now combined and preprocessed in 'adata' obj - success!\")\n",
    "\n",
    "\n",
    "    # train model\n",
    "#    train_x = adata_temp.X\n",
    "    #train_label = adata_temp.obs[feat_use]\n",
    "    print('proceeding to train model')\n",
    "    model = LR_train(adata_temp, train_x = train_x_partition, train_label=feat_use, penalty=penalty, sparcity=sparcity,max_iter=max_iter,l1_ratio = l1_ratio,tune_hyper_params = tune_hyper_params)\n",
    "    model.features = list(adata_temp.var.index)\n",
    "    return model\n",
    "\n",
    "def compute_label_log_losses(df, true_label, pred_columns):\n",
    "    \"\"\"\n",
    "    Compute log loss (cross-entropy loss).\n",
    "    \n",
    "    Parameters:\n",
    "    df : dataframe containing the predicted probabilities and original labels as columns\n",
    "    true_label : column or array-like of shape (n_samples,) containg cateogrical labels\n",
    "    pred_columns : columns or array-like of shape (n_samples, n_clases) containg predicted probabilities\n",
    "\n",
    "    converts to:\n",
    "    y_true : array-like of shape (n_samples,) True labels. The binary labels in a one-vs-rest fashion.\n",
    "    y_pred : array-like of shape (n_samples, n_classes) Predicted probabilities. \n",
    "        \n",
    "    Returns:\n",
    "    log_loss : dictionary of celltype key and float\n",
    "    weights : float\n",
    "    \"\"\"\n",
    "    log_losses = {}\n",
    "    y_true = (pd.get_dummies(df[true_label]))\n",
    "    y_pred = df[pred_col]\n",
    "    loss = log_loss(np.array(y_true), np.array(y_pred))\n",
    "    for label in range(y_true.shape[1]):\n",
    "        log_loss_label = log_loss(np.array(y_true)[:, label], np.array(y_pred)[:, label])\n",
    "        log_losses[list(y_true.columns)[label]] = (log_loss_label)\n",
    "    weights = 1/np.array(list(log_losses.values()))\n",
    "    weights /= np.sum(weights)\n",
    "    weights = np.array(weights)\n",
    "    return loss, log_losses, weights\n",
    "\n",
    "def regression_results(df, true_label, pred_label, pred_columns):\n",
    "    # Regression metrics\n",
    "    y_true = df[true_label]\n",
    "    y_pred = df[pred_label]\n",
    "    loss, log_losses, weights = compute_label_log_losses(df, true_label, pred_columns)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "#     r2=metrics.r2_score(y_true, y_pred)\n",
    "    print('Cross entropy loss: ', round(loss,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    print('label Cross entropy loss: ')\n",
    "    print(log_losses)  \n",
    "    return loss, log_losses, weights\n",
    "\n",
    "\n",
    "# ENSDB-HGNC Option 1 \n",
    "#from gseapy.parser import Biomart\n",
    "#bm = Biomart()\n",
    "## view validated marts#\n",
    "#marts = bm.get_marts()\n",
    "## view validated dataset\n",
    "#datasets = bm.get_datasets(mart='ENSEMBL_MART_ENSEMBL')\n",
    "## view validated attributes\n",
    "#attrs = bm.get_attributes(dataset='hsapiens_gene_ensembl')\n",
    "## view validated filters\n",
    "#filters = bm.get_filters(dataset='hsapiens_gene_ensembl')\n",
    "## query results\n",
    "#queries = ['ENSG00000125285','ENSG00000182968'] # need to be a python list\n",
    "#results = bm.query(dataset='hsapiens_gene_ensembl',\n",
    "#                       attributes=['ensembl_gene_id', 'external_gene_name', 'entrezgene_id', 'go_id'],\n",
    "#                       filters={'ensemble_gene_id': queries}\n",
    "                      \n",
    "# ENSDB-HGNC Option 2\n",
    "def convert_hgnc(input_gene_list):\n",
    "    import mygene\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    geneList = input_gene_list \n",
    "    geneSyms = mg.querymany(geneList , scopes='ensembl.gene', fields='symbol', species='human')\n",
    "    return(pd.DataFrame(geneSyms))\n",
    "# Example use: convert_hgnc(['ENSG00000148795', 'ENSG00000165359', 'ENSG00000150676'])\n",
    "\n",
    "# Scanpy_degs_to_long_format\n",
    "def convert_scanpy_degs(input_dataframe):\n",
    "    if 'concat' in locals() or 'concat' in globals():\n",
    "        del(concat)\n",
    "    degs = input_dataframe\n",
    "    n = degs.loc[:, degs.columns.str.endswith(\"_n\")]\n",
    "    n = pd.melt(n)\n",
    "    p = degs.loc[:, degs.columns.str.endswith(\"_p\")]\n",
    "    p = pd.melt(p)\n",
    "    l = degs.loc[:, degs.columns.str.endswith(\"_l\")]\n",
    "    l = pd.melt(l)\n",
    "    n = n.replace(regex=r'_n', value='')\n",
    "    n = n.rename(columns={\"variable\": \"cluster\", \"value\": \"gene\"})\n",
    "    p = (p.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"p_val\"})\n",
    "    l = (l.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"logfc\"})\n",
    "    return(pd.concat([n,p,l],axis=1))\n",
    "#Usage: convert_scanpy_degs(scanpy_degs_file)\n",
    "\n",
    "# Clean convert gene list to list\n",
    "def as_gene_list(input_df,gene_col):\n",
    "    gene_list = input_df[gene_col]\n",
    "    glist = gene_list.squeeze().str.strip().tolist()\n",
    "    return(glist)\n",
    "\n",
    "# No ranking enrichr\n",
    "def enrich_no_rank(input_gene_list,database,species=\"Human\",description=\"enr_no_rank\",outdir = \"./enr\",cutoff=0.5):\n",
    "    # list, dataframe, series inputs are supported\n",
    "    enr = gp.enrichr(gene_list=input_gene_list,\n",
    "                     gene_sets=database,\n",
    "                     organism=species, # don't forget to set organism to the one you desired! e.g. Yeast\n",
    "                     #description=description,\n",
    "                     outdir=outdir,\n",
    "                     # no_plot=True,\n",
    "                     cutoff=cutoff # test dataset, use lower value from range(0,1)\n",
    "                    )\n",
    "    return(enr)\n",
    "    #Usge: enrich_no_rank(gene_as_list)\n",
    "    \n",
    "# Custom genelist test #input long format degs or dictionary of DEGS\n",
    "def custom_local_GO_enrichment(input_gene_list,input_gmt,input_gmt_key_col,input_gmt_values,description=\"local_go\",Background='hsapiens_gene_ensembl',Cutoff=0.5):\n",
    "    \n",
    "    #Check if GMT input is a dictionary or long-format input\n",
    "    if isinstance(input_gmt, dict):\n",
    "        print(\"input gmt is a dictionary, proceeding\")\n",
    "        dic = input_gmt\n",
    "    else:\n",
    "        print(\"input gmt is not a dictionary, if is pandas df,please ensure it is long-format proceeding to convert to dictionary\")\n",
    "        dic =  input_gmt.groupby([input_gmt_key_col])[input_gmt_values].agg(lambda grp: list(grp)).to_dict()\n",
    "        \n",
    "    enr_local = gp.enrichr(gene_list=input_gene_list,\n",
    "                 description=description,\n",
    "                 gene_sets=dic,\n",
    "                 background=Background, # or the number of genes, e.g 20000\n",
    "                 cutoff=Cutoff, # only used for testing.\n",
    "                 verbose=True)\n",
    "    return(enr_local)\n",
    "    #Example_usage: custom_local_GO_enrichment(input_gene_list,input_gmt,input_gmt_key_col,input_gmt_values) #input gmt can be long-format genes and ontology name or can be dictionary of the same   \n",
    "\n",
    "    \n",
    "# Pre-ranked GS enrichment\n",
    "def pre_ranked_enr(input_gene_list,gene_and_ranking_columns,database ='GO_Biological_Process_2018',permutation_num = 1000, outdir = \"./enr_ranked\",cutoff=0.25,min_s=5,max_s=1000):\n",
    "    glist = input_gene_list[gene_and_ranking_columns]\n",
    "    pre_res = gp.prerank(glist, gene_sets=database,\n",
    "                     threads=4,\n",
    "                     permutation_num=permutation_num, # reduce number to speed up testing\n",
    "                     outdir=outdir,\n",
    "                     seed=6,\n",
    "                     min_size=min_s,\n",
    "                     max_size=max_s)\n",
    "    return(pre_res)\n",
    "    #Example usage: pre_ranked_hyper_geom(DE, gene_and_ranking_columns = [\"gene\",\"logfc\"],database=['KEGG_2016','GO_Biological_Process_2018'])\n",
    "\n",
    "    \n",
    "# GSEA module for permutation test of differentially regulated genes\n",
    "# gene set enrichment analysis (GSEA), which scores ranked genes list (usually based on fold changes) and computes permutation test to check if a particular gene set is more present in the Up-regulated genes, \n",
    "# among the DOWN_regulated genes or not differentially regulated.\n",
    "#NES = normalised enrichment scores accounting for geneset size\n",
    "def permutation_ranked_enr(input_DE,cluster_1,cluster_2,input_DE_clust_col,input_ranking_col ,input_gene_col ,database = \"GO_Biological_Process_2018\"):\n",
    "    input_DE = input_DE[input_DE[input_DE_clust_col].isin([cluster_1,cluster_2])]\n",
    "    #Make set2 negative values to represent opposing condition\n",
    "    input_DE[input_ranking_col].loc[input_DE[input_DE_clust_col].isin([cluster_2])] = -input_DE[input_ranking_col].loc[input_DE[input_DE_clust_col].isin([cluster_2])]\n",
    "    enr_perm = pre_ranked_enr(input_DE,[input_gene_col,input_ranking_col],database,permutation_num = 100, outdir = \"./enr_ranked_perm\",cutoff=0.5)\n",
    "    return(enr_perm)\n",
    "    #Example usage:permutation_ranked_enr(input_DE = DE, cluster_1 = \"BM\",cluster_2 = \"YS\",input_DE_clust_col = \"cluster\",input_ranking_col = \"logfc\",input_gene_col = \"gene\",database = \"GO_Biological_Process_2018\")\n",
    "    #input long-format list of genes and with a class for permutaion, the logfc ranking should have been derived at the same time\n",
    "\n",
    "\n",
    "#Creating similarity matrix from nested gene lists\n",
    "def create_sim_matrix_from_enr(input_df,nested_gene_column=\"Genes\",seperator=\";\",term_col=\"Term\"):\n",
    "#    input_df[gene_col] = input_df[gene_col].astype(str)\n",
    "#    input_df[gene_col] = input_df[gene_col].str.split(\";\")\n",
    "#    uni_val = list(input_df.index.unique())\n",
    "#    sim_mat = pd.DataFrame(index=uni_val, columns=uni_val)\n",
    "#    exploded_df = input_df.explode(gene_col)\n",
    "#    # Ugly loop for cosine gs similarity matrix (0-1)\n",
    "#    for i in (uni_val):\n",
    "#        row = exploded_df[exploded_df.index.isin([i])]\n",
    "#        for z in (uni_val):\n",
    "#            col = exploded_df[exploded_df.index.isin([z])]\n",
    "#            col_ls = col[gene_col]\n",
    "#            row_ls = row[gene_col]\n",
    "#            sim = len(set(col_ls) & set(row_ls)) / float(len(set(col_ls) | set(row_ls)))\n",
    "#            sim_mat.loc[i , z] = sim\n",
    "\n",
    "#    Check term col in columns else, check index as it\\s sometimes heree\n",
    "    if not term_col in list(input_df.columns):\n",
    "        input_df[term_col] = input_df.index\n",
    "\n",
    "#    Create a similarity matrix by cosine similarity\n",
    "    input_df = input_df.copy()\n",
    "    gene_col = nested_gene_column #\"ledge_genes\"\n",
    "    input_df[gene_col] = input_df[gene_col].astype(str)\n",
    "    input_df[gene_col] = input_df[gene_col].str.split(seperator)\n",
    "    term_vals = list(input_df[term_col].unique())\n",
    "    uni_val = list(input_df[term_col].unique())\n",
    "    sim_mat = pd.DataFrame(index=uni_val, columns=uni_val)\n",
    "    exploded_df = input_df.explode(gene_col)\n",
    "    arr = np.array(input_df[gene_col])\n",
    "    vals = list(exploded_df[nested_gene_column])\n",
    "    import scipy.sparse as sparse\n",
    "    def binarise(sets, full_set):\n",
    "        \"\"\"Return sparse binary matrix of given sets.\"\"\"\n",
    "        return sparse.csr_matrix([[x in s for x in full_set] for s in sets])\n",
    "    # Turn the matrix into a sparse boleen matrix of binarised values\n",
    "    sparse_matrix = binarise(arr, vals)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(sparse_matrix)\n",
    "    sim_mat = pd.DataFrame(similarities)\n",
    "    sim_mat.index = uni_val\n",
    "    sim_mat.columns = uni_val\n",
    "    return(sim_mat)\n",
    "#Example usage : sim_mat = create_sim_matrix_from_enr(enr.res2d)\n",
    "\n",
    "\n",
    "#Creating similarity matrix from GO terms\n",
    "def create_sim_matrix_from_term(input_df,nested_gene_column=\"Term\",seperator=\" \",term_col=\"Term\"):\n",
    "\n",
    "#    Check term col in columns else, check index as it\\s sometimes heree\n",
    "    if not term_col in list(input_df.columns):\n",
    "        input_df[term_col] = input_df.index\n",
    "\n",
    "#    Create a similarity matrix by cosine similarity\n",
    "    input_df = input_df.copy()\n",
    "    gene_col = nested_gene_column #\"ledge_genes\"\n",
    "    #input_df[gene_col] = input_df[gene_col].astype(str)\n",
    "    input_df[gene_col] = input_df[gene_col].str.split(seperator)\n",
    "    term_vals = list(input_df[term_col].unique())\n",
    "    uni_val = list(input_df.index.unique())\n",
    "    sim_mat = pd.DataFrame(index=uni_val, columns=uni_val)\n",
    "    exploded_df = input_df.explode(gene_col)\n",
    "    arr = np.array(input_df[gene_col])\n",
    "    vals = list(exploded_df[nested_gene_column])\n",
    "    import scipy.sparse as sparse\n",
    "    def binarise(sets, full_set):\n",
    "        \"\"\"Return sparse binary matrix of given sets.\"\"\"\n",
    "        return sparse.csr_matrix([[x in s for x in full_set] for s in sets])\n",
    "    sparse_matrix = binarise(arr, vals)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(sparse_matrix)\n",
    "    sim_mat = pd.DataFrame(similarities)\n",
    "    sim_mat.index = uni_val\n",
    "    sim_mat.columns = uni_val\n",
    "    return(sim_mat)\n",
    "\n",
    "#Creating similarity matrix from GO terms\n",
    "def create_sim_matrix_from_term2(input_df,nested_gene_column=\"Term\",seperator=\" \",term_col=\"Term\"):\n",
    "#    Horrifically bad cosine similairty estimate for word frequency\n",
    "#    Check term col in columns else, check index as it\\s sometimes heree\n",
    "    if not term_col in list(input_df.columns):\n",
    "        input_df[term_col] = input_df.index\n",
    "    input_df = input_df.copy()\n",
    "    gene_col = nested_gene_column #\"ledge_genes\"\n",
    "    #input_df[gene_col] = input_df[gene_col].astype(str)\n",
    "    term_vals = list(input_df[term_col].unique())\n",
    "    input_df[gene_col] = input_df[gene_col].str.split(seperator)\n",
    "    uni_val = list(input_df.index.unique())\n",
    "    sim_mat = pd.DataFrame(index=uni_val, columns=uni_val)\n",
    "    exploded_df = input_df.explode(gene_col)\n",
    "\n",
    "    nan_value = float(\"NaN\")\n",
    "    exploded_df.replace(\"\", nan_value, inplace=True)\n",
    "    exploded_df.dropna(subset = [gene_col], inplace=True)\n",
    "    arr = np.array(input_df[gene_col])\n",
    "\n",
    "    vals = list(exploded_df[nested_gene_column])\n",
    "\n",
    "    import scipy.sparse as sparse\n",
    "    def binarise(sets, full_set):\n",
    "        \"\"\"Return sparse binary matrix of given sets.\"\"\"\n",
    "        return sparse.csr_matrix([[x in s for x in full_set] for s in sets])\n",
    "    sparse_matrix = binarise(arr, vals)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(sparse_matrix)\n",
    "    sim_mat = pd.DataFrame(similarities)\n",
    "    sim_mat.index = uni_val\n",
    "    sim_mat.columns = uni_val\n",
    "    return(sim_mat)\n",
    "    #Example usage : sim_mat = create_sim_matrix_from_enr(enr.res2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "mechanical-romantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_grid = [0.1,0.2,0.5,0.8]\n",
    "np.min(l1_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "combined-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "search_space = {'C': (0.01, 1, 'log-uniform'), \n",
    "                'l1_ratio': (0.01, 1, 'uniform') if 'elasticnet' in penalty else None}\n",
    "# define search\n",
    "search = BayesSearchCV(model, search_space, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "partial-ensemble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option to apply standardisation to data detected, performing basic QC filtering\n"
     ]
    }
   ],
   "source": [
    "adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proper-recipient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bh_doublet_pval</th>\n",
       "      <th>cell_caller</th>\n",
       "      <th>cluster_scrublet_score</th>\n",
       "      <th>doublet_pval</th>\n",
       "      <th>mt_prop</th>\n",
       "      <th>n_counts</th>\n",
       "      <th>n_genes</th>\n",
       "      <th>sanger_id</th>\n",
       "      <th>scrublet_score</th>\n",
       "      <th>chemistry</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>pcw</th>\n",
       "      <th>sorting</th>\n",
       "      <th>sample</th>\n",
       "      <th>chemistry_sorting</th>\n",
       "      <th>annot</th>\n",
       "      <th>hierarchy1</th>\n",
       "      <th>joint_annotation_20220202</th>\n",
       "      <th>independent_annotation_refined_20220202</th>\n",
       "      <th>fig1b_annotation_20220202</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCTGGTCAGTGGA-1-4834STDY7002879</th>\n",
       "      <td>0.907861</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.157082</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.062532</td>\n",
       "      <td>5917.0</td>\n",
       "      <td>1776</td>\n",
       "      <td>4834STDY7002879</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>SC3Pv2</td>\n",
       "      <td>...</td>\n",
       "      <td>male</td>\n",
       "      <td>8</td>\n",
       "      <td>CD45P</td>\n",
       "      <td>F16_male_8+1PCW</td>\n",
       "      <td>SC3Pv2_CD45P</td>\n",
       "      <td>fs_Macrophage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>Macrophage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAGATGGTCGATTGT-1-4834STDY7002879</th>\n",
       "      <td>0.907861</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.157082</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.030894</td>\n",
       "      <td>10261.0</td>\n",
       "      <td>2750</td>\n",
       "      <td>4834STDY7002879</td>\n",
       "      <td>0.149606</td>\n",
       "      <td>SC3Pv2</td>\n",
       "      <td>...</td>\n",
       "      <td>male</td>\n",
       "      <td>8</td>\n",
       "      <td>CD45P</td>\n",
       "      <td>F16_male_8+1PCW</td>\n",
       "      <td>SC3Pv2_CD45P</td>\n",
       "      <td>fs_Monocyte</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Monocyte (activated/differentiating)</td>\n",
       "      <td>Monocyte (activated/differentiating)</td>\n",
       "      <td>Monocyte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAGCAAAGATGTGGC-1-4834STDY7002879</th>\n",
       "      <td>0.882352</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.150885</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>2308</td>\n",
       "      <td>4834STDY7002879</td>\n",
       "      <td>0.201970</td>\n",
       "      <td>SC3Pv2</td>\n",
       "      <td>...</td>\n",
       "      <td>male</td>\n",
       "      <td>8</td>\n",
       "      <td>CD45P</td>\n",
       "      <td>F16_male_8+1PCW</td>\n",
       "      <td>SC3Pv2_CD45P</td>\n",
       "      <td>fs_Macrophage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>Macrophage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAGTAGCAGATCGGA-1-4834STDY7002879</th>\n",
       "      <td>0.907861</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>0.455284</td>\n",
       "      <td>0.017443</td>\n",
       "      <td>14791.0</td>\n",
       "      <td>3099</td>\n",
       "      <td>4834STDY7002879</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>SC3Pv2</td>\n",
       "      <td>...</td>\n",
       "      <td>male</td>\n",
       "      <td>8</td>\n",
       "      <td>CD45P</td>\n",
       "      <td>F16_male_8+1PCW</td>\n",
       "      <td>SC3Pv2_CD45P</td>\n",
       "      <td>fs_Mast cell</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Eo/baso/mast cell progenitor</td>\n",
       "      <td>Eo/baso/mast cell progenitor</td>\n",
       "      <td>Progenitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAGTAGTCCGCATCT-1-4834STDY7002879</th>\n",
       "      <td>0.882352</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.201970</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.041431</td>\n",
       "      <td>7434.0</td>\n",
       "      <td>2283</td>\n",
       "      <td>4834STDY7002879</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>SC3Pv2</td>\n",
       "      <td>...</td>\n",
       "      <td>male</td>\n",
       "      <td>8</td>\n",
       "      <td>CD45P</td>\n",
       "      <td>F16_male_8+1PCW</td>\n",
       "      <td>SC3Pv2_CD45P</td>\n",
       "      <td>fs_Macrophage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>Macrophage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAAGTGAACGC-1-FCAImmP7964510</th>\n",
       "      <td>0.851322</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.109737</td>\n",
       "      <td>0.314770</td>\n",
       "      <td>0.027876</td>\n",
       "      <td>5094.0</td>\n",
       "      <td>1875</td>\n",
       "      <td>FCAImmP7964510</td>\n",
       "      <td>0.085937</td>\n",
       "      <td>SC5P-R2</td>\n",
       "      <td>...</td>\n",
       "      <td>female</td>\n",
       "      <td>14</td>\n",
       "      <td>CD45en</td>\n",
       "      <td>F71-GEX_5_SKI_45en</td>\n",
       "      <td>SC5P-R2_CD45en</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Mural cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAGTGCGAAAC-1-FCAImmP7964510</th>\n",
       "      <td>0.851322</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.125778</td>\n",
       "      <td>0.231265</td>\n",
       "      <td>0.038952</td>\n",
       "      <td>6906.0</td>\n",
       "      <td>2161</td>\n",
       "      <td>FCAImmP7964510</td>\n",
       "      <td>0.125778</td>\n",
       "      <td>SC5P-R2</td>\n",
       "      <td>...</td>\n",
       "      <td>female</td>\n",
       "      <td>14</td>\n",
       "      <td>CD45en</td>\n",
       "      <td>F71-GEX_5_SKI_45en</td>\n",
       "      <td>SC5P-R2_CD45en</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Mural cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCCATGAGT-1-FCAImmP7964510</th>\n",
       "      <td>0.851322</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.024831</td>\n",
       "      <td>0.803208</td>\n",
       "      <td>0.012081</td>\n",
       "      <td>8526.0</td>\n",
       "      <td>658</td>\n",
       "      <td>FCAImmP7964510</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>SC5P-R2</td>\n",
       "      <td>...</td>\n",
       "      <td>female</td>\n",
       "      <td>14</td>\n",
       "      <td>CD45en</td>\n",
       "      <td>F71-GEX_5_SKI_45en</td>\n",
       "      <td>SC5P-R2_CD45en</td>\n",
       "      <td>nan</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Erythroid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCGCAAGCC-1-FCAImmP7964510</th>\n",
       "      <td>0.851322</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.044693</td>\n",
       "      <td>0.705651</td>\n",
       "      <td>0.053341</td>\n",
       "      <td>10911.0</td>\n",
       "      <td>3368</td>\n",
       "      <td>FCAImmP7964510</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>SC5P-R2</td>\n",
       "      <td>...</td>\n",
       "      <td>female</td>\n",
       "      <td>14</td>\n",
       "      <td>CD45en</td>\n",
       "      <td>F71-GEX_5_SKI_45en</td>\n",
       "      <td>SC5P-R2_CD45en</td>\n",
       "      <td>nan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>LE</td>\n",
       "      <td>LE</td>\n",
       "      <td>Lymphatic endothelium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCTGCTTGC-1-FCAImmP7964510</th>\n",
       "      <td>0.851322</td>\n",
       "      <td>Both</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.799655</td>\n",
       "      <td>0.007697</td>\n",
       "      <td>8575.0</td>\n",
       "      <td>609</td>\n",
       "      <td>FCAImmP7964510</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>SC5P-R2</td>\n",
       "      <td>...</td>\n",
       "      <td>female</td>\n",
       "      <td>14</td>\n",
       "      <td>CD45en</td>\n",
       "      <td>F71-GEX_5_SKI_45en</td>\n",
       "      <td>SC5P-R2_CD45en</td>\n",
       "      <td>nan</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Erythroid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186533 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    bh_doublet_pval cell_caller  \\\n",
       "index                                                             \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879         0.907861        Both   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879         0.907861        Both   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879         0.882352        Both   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879         0.907861        Both   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879         0.882352        Both   \n",
       "...                                             ...         ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510          0.851322        Both   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510          0.851322        Both   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510          0.851322        Both   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510          0.851322        Both   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510          0.851322        Both   \n",
       "\n",
       "                                    cluster_scrublet_score  doublet_pval  \\\n",
       "index                                                                      \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879                0.157082      0.500000   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879                0.157082      0.500000   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879                0.225806      0.150885   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879                0.164557      0.455284   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879                0.201970      0.250000   \n",
       "...                                                    ...           ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                 0.109737      0.314770   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                 0.125778      0.231265   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                 0.024831      0.803208   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                 0.044693      0.705651   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                 0.025641      0.799655   \n",
       "\n",
       "                                     mt_prop  n_counts  n_genes  \\\n",
       "index                                                             \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879  0.062532    5917.0     1776   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  0.030894   10261.0     2750   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879  0.012647    7749.0     2308   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879  0.017443   14791.0     3099   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879  0.041431    7434.0     2283   \n",
       "...                                      ...       ...      ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510   0.027876    5094.0     1875   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510   0.038952    6906.0     2161   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510   0.012081    8526.0      658   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510   0.053341   10911.0     3368   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510   0.007697    8575.0      609   \n",
       "\n",
       "                                          sanger_id  scrublet_score chemistry  \\\n",
       "index                                                                           \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879  4834STDY7002879        0.225806    SC3Pv2   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  4834STDY7002879        0.149606    SC3Pv2   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879  4834STDY7002879        0.201970    SC3Pv2   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879  4834STDY7002879        0.164557    SC3Pv2   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879  4834STDY7002879        0.181818    SC3Pv2   \n",
       "...                                             ...             ...       ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510    FCAImmP7964510        0.085937   SC5P-R2   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510    FCAImmP7964510        0.125778   SC5P-R2   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510    FCAImmP7964510        0.034700   SC5P-R2   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510    FCAImmP7964510        0.052632   SC5P-R2   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510    FCAImmP7964510        0.025641   SC5P-R2   \n",
       "\n",
       "                                    ...  gender pcw  sorting  \\\n",
       "index                               ...                        \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879  ...    male   8    CD45P   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  ...    male   8    CD45P   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879  ...    male   8    CD45P   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879  ...    male   8    CD45P   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879  ...    male   8    CD45P   \n",
       "...                                 ...     ...  ..      ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510   ...  female  14   CD45en   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510   ...  female  14   CD45en   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510   ...  female  14   CD45en   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510   ...  female  14   CD45en   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510   ...  female  14   CD45en   \n",
       "\n",
       "                                                sample chemistry_sorting  \\\n",
       "index                                                                      \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879     F16_male_8+1PCW      SC3Pv2_CD45P   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879     F16_male_8+1PCW      SC3Pv2_CD45P   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879     F16_male_8+1PCW      SC3Pv2_CD45P   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879     F16_male_8+1PCW      SC3Pv2_CD45P   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879     F16_male_8+1PCW      SC3Pv2_CD45P   \n",
       "...                                                ...               ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510   F71-GEX_5_SKI_45en    SC5P-R2_CD45en   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510   F71-GEX_5_SKI_45en    SC5P-R2_CD45en   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510   F71-GEX_5_SKI_45en    SC5P-R2_CD45en   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510   F71-GEX_5_SKI_45en    SC5P-R2_CD45en   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510   F71-GEX_5_SKI_45en    SC5P-R2_CD45en   \n",
       "\n",
       "                                            annot hierarchy1  \\\n",
       "index                                                          \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879  fs_Macrophage        1.0   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879    fs_Monocyte        1.0   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879  fs_Macrophage        1.0   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879   fs_Mast cell        5.0   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879  fs_Macrophage        1.0   \n",
       "...                                           ...        ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510             nan        0.0   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510             nan        0.0   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510             nan        8.0   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510             nan        4.0   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510             nan        8.0   \n",
       "\n",
       "                                               joint_annotation_20220202  \\\n",
       "index                                                                      \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879                    LYVE1++ macrophage   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  Monocyte (activated/differentiating)   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879                    LYVE1++ macrophage   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879          Eo/baso/mast cell progenitor   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879                    LYVE1++ macrophage   \n",
       "...                                                                  ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                              Pericytes   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                              Pericytes   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                        Early erythroid   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                                     LE   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                        Early erythroid   \n",
       "\n",
       "                                   independent_annotation_refined_20220202  \\\n",
       "index                                                                        \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879                      LYVE1++ macrophage   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879    Monocyte (activated/differentiating)   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879                      LYVE1++ macrophage   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879            Eo/baso/mast cell progenitor   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879                      LYVE1++ macrophage   \n",
       "...                                                                    ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                                Pericytes   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                                Pericytes   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                          Early erythroid   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                                       LE   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                          Early erythroid   \n",
       "\n",
       "                                   fig1b_annotation_20220202  \n",
       "index                                                         \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879                Macrophage  \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879                  Monocyte  \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879                Macrophage  \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879                Progenitor  \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879                Macrophage  \n",
       "...                                                      ...  \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                 Mural cell  \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                 Mural cell  \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                  Erythroid  \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510      Lymphatic endothelium  \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                  Erythroid  \n",
       "\n",
       "[186533 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-component",
   "metadata": {},
   "source": [
    "# Read in query data for projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model == True:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "    print('adata_loaded')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    display_cpu = DisplayCPU()\n",
    "    display_cpu.start()\n",
    "    try:\n",
    "        model_trained = prep_training_data(feat_use = feat_use,\n",
    "        adata_temp = adata,\n",
    "        train_x_partition = train_x_partition,\n",
    "        model_key = model_key + '_lr_model',\n",
    "        batch_correction = 'Harmony',\n",
    "        var_length = 7500,\n",
    "        batch_key = 'donor',\n",
    "        penalty='elasticnet', # can be [\"l1\",\"l2\",\"elasticnet\"],\n",
    "        sparcity=sparcity, #If using LR without optimisation, this controls the sparsity in model\n",
    "        max_iter = 1000, #Increase if experiencing max iter issues\n",
    "        l1_ratio = l1_ratio, #If using elasticnet without optimisation, this controls the ratio between l1 and l2)\n",
    "        partial_scale = False, #partial_scale,\n",
    "        tune_hyper_params = True # Current implementation is very expensive, intentionally made rigid for now\n",
    "        )\n",
    "        filename =model_name\n",
    "        pkl.dump(model_trained, open(filename, 'wb'))\n",
    "    finally: #\n",
    "        current, peak = display_cpu.stop()\n",
    "        t1 = time.time()\n",
    "        time_s = t1-t0\n",
    "        print('training complete!')\n",
    "        time.sleep(3)\n",
    "        print('projection time was ' + str(time_s) + ' seconds')\n",
    "        print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "        print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "        print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "        print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))\n",
    "    model_lr= model_trained\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key)\n",
    "else:\n",
    "    adata =  load_adatas(adatas_dict, data_merge, adata_key,QC_normalise)\n",
    "    model = load_models(models,model_key)\n",
    "    model_lr =  model\n",
    "# run with usage logger\n",
    "import time\n",
    "t0 = time.time()\n",
    "display_cpu = DisplayCPU()\n",
    "display_cpu.start()\n",
    "try: #code here ##\n",
    "    pred_out,train_x,model_lr,adata_temp = reference_projection(adata, model_lr, dyn_std,partial_scale)\n",
    "    if freq_redist != False:\n",
    "        pred_out = freq_redist_68CI(adata,freq_redist)\n",
    "        pred_out['orig_labels'] = adata.obs[freq_redist]\n",
    "        adata.obs['consensus_clus_prediction'] = pred_out['consensus_clus_prediction']\n",
    "    adata.obs['predicted'] = pred_out['predicted']\n",
    "    adata_temp.obs = adata.obs\n",
    "    \n",
    "    # Estimate top model features for class descrimination\n",
    "    feature_importance = estimate_important_features(model_lr, 100)\n",
    "    mat = feature_importance.euler_pow_mat\n",
    "    top_loadings = feature_importance.to_n_features_long\n",
    "    \n",
    "    # Estimate dataset specific feature impact\n",
    "#     for classes in ['pDC precursor_ys_HL','AEC_ys_HL']:\n",
    "#         model_class_feature_plots(top_loadings, [str(classes)], 'e^coef')\n",
    "#         plt.show()\n",
    "finally: #\n",
    "    current, peak = display_cpu.stop()\n",
    "t1 = time.time()\n",
    "time_s = t1-t0\n",
    "print('projection complete!')\n",
    "time.sleep(3)\n",
    "print('projection time was ' + str(time_s) + ' seconds')\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "print(f\"starting memory usage is\" +'' + str(display_cpu.starting))\n",
    "print('peak CPU % usage = '+''+ str(display_cpu.peak_cpu))\n",
    "print('peak CPU % usage/core = '+''+ str(display_cpu.peak_cpu_per_core))\n",
    "\n",
    "# regression summary\n",
    "idx_map = dict(zip(  list(adata.obs[feat_use].unique()),list(range(0,len(list(adata.obs[feat_use].unique()))))))\n",
    "regression_results(adata.obs[feat_use].map(idx_map), adata.obs['predicted'].map(idx_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lesbian-movement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>ASDC</th>\n",
       "      <th>Adipocytes</th>\n",
       "      <th>Arterial</th>\n",
       "      <th>B cell</th>\n",
       "      <th>Basal</th>\n",
       "      <th>CD4 T cell</th>\n",
       "      <th>CD8 T cell</th>\n",
       "      <th>Capillary (venular tip)</th>\n",
       "      <th>Capillary/postcapillary venule</th>\n",
       "      <th>...</th>\n",
       "      <th>Suprabasal IFE</th>\n",
       "      <th>TREM2+ macrophage</th>\n",
       "      <th>Tip cell (arterial)</th>\n",
       "      <th>Treg</th>\n",
       "      <th>WNT2+ fibroblast</th>\n",
       "      <th>pDC</th>\n",
       "      <th>confident_calls</th>\n",
       "      <th>joint_annotation_20220202</th>\n",
       "      <th>consensus_clus_prediction</th>\n",
       "      <th>orig_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCTGGTCAGTGGA-1-4834STDY7002879</th>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>3.121147e-08</td>\n",
       "      <td>2.942001e-08</td>\n",
       "      <td>5.584572e-09</td>\n",
       "      <td>5.133540e-10</td>\n",
       "      <td>3.132832e-09</td>\n",
       "      <td>2.463025e-10</td>\n",
       "      <td>7.582230e-10</td>\n",
       "      <td>4.643047e-07</td>\n",
       "      <td>1.149033e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>8.762978e-08</td>\n",
       "      <td>1.004222e-04</td>\n",
       "      <td>1.232687e-06</td>\n",
       "      <td>1.720775e-09</td>\n",
       "      <td>2.245473e-12</td>\n",
       "      <td>2.170336e-07</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAGATGGTCGATTGT-1-4834STDY7002879</th>\n",
       "      <td>Monocyte</td>\n",
       "      <td>8.021268e-05</td>\n",
       "      <td>1.136223e-09</td>\n",
       "      <td>8.952242e-09</td>\n",
       "      <td>7.051683e-09</td>\n",
       "      <td>8.112603e-09</td>\n",
       "      <td>1.610157e-11</td>\n",
       "      <td>6.927696e-07</td>\n",
       "      <td>4.949447e-11</td>\n",
       "      <td>1.130580e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>5.018859e-09</td>\n",
       "      <td>1.562070e-07</td>\n",
       "      <td>8.123187e-09</td>\n",
       "      <td>1.345388e-06</td>\n",
       "      <td>2.181822e-15</td>\n",
       "      <td>9.321094e-07</td>\n",
       "      <td>Monocyte</td>\n",
       "      <td>Monocyte (activated/differentiating)</td>\n",
       "      <td>Monocyte (activated/differentiating)</td>\n",
       "      <td>Monocyte (activated/differentiating)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAGCAAAGATGTGGC-1-4834STDY7002879</th>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>4.388417e-06</td>\n",
       "      <td>2.971849e-11</td>\n",
       "      <td>7.536972e-09</td>\n",
       "      <td>2.891398e-09</td>\n",
       "      <td>3.302528e-09</td>\n",
       "      <td>1.790903e-11</td>\n",
       "      <td>3.914251e-10</td>\n",
       "      <td>1.995001e-08</td>\n",
       "      <td>2.178952e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.377020e-08</td>\n",
       "      <td>2.140535e-04</td>\n",
       "      <td>1.696714e-07</td>\n",
       "      <td>1.792986e-08</td>\n",
       "      <td>4.258448e-15</td>\n",
       "      <td>1.052342e-06</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAGTAGCAGATCGGA-1-4834STDY7002879</th>\n",
       "      <td>Eo/baso/mast cell progenitor</td>\n",
       "      <td>4.141562e-05</td>\n",
       "      <td>1.333213e-08</td>\n",
       "      <td>2.398914e-09</td>\n",
       "      <td>2.734619e-09</td>\n",
       "      <td>3.996713e-09</td>\n",
       "      <td>6.980059e-07</td>\n",
       "      <td>3.711175e-09</td>\n",
       "      <td>1.472187e-08</td>\n",
       "      <td>3.175144e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.670171e-09</td>\n",
       "      <td>3.371311e-08</td>\n",
       "      <td>1.082911e-08</td>\n",
       "      <td>7.374140e-11</td>\n",
       "      <td>1.589416e-12</td>\n",
       "      <td>1.098155e-07</td>\n",
       "      <td>Eo/baso/mast cell progenitor</td>\n",
       "      <td>Eo/baso/mast cell progenitor</td>\n",
       "      <td>Eo/baso/mast cell progenitor</td>\n",
       "      <td>Eo/baso/mast cell progenitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAAGTAGTCCGCATCT-1-4834STDY7002879</th>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>8.686127e-08</td>\n",
       "      <td>3.933139e-10</td>\n",
       "      <td>1.384100e-07</td>\n",
       "      <td>2.455699e-10</td>\n",
       "      <td>2.286061e-08</td>\n",
       "      <td>1.976017e-12</td>\n",
       "      <td>5.729960e-11</td>\n",
       "      <td>5.051004e-08</td>\n",
       "      <td>6.295254e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143184e-08</td>\n",
       "      <td>1.045524e-04</td>\n",
       "      <td>2.108652e-08</td>\n",
       "      <td>1.197810e-09</td>\n",
       "      <td>2.699467e-14</td>\n",
       "      <td>4.509701e-09</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "      <td>LYVE1++ macrophage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAAGTGAACGC-1-FCAImmP7964510</th>\n",
       "      <td>Pericytes</td>\n",
       "      <td>8.901155e-08</td>\n",
       "      <td>3.238348e-06</td>\n",
       "      <td>8.571223e-06</td>\n",
       "      <td>1.924168e-08</td>\n",
       "      <td>3.902949e-07</td>\n",
       "      <td>1.215860e-06</td>\n",
       "      <td>5.714410e-07</td>\n",
       "      <td>3.698724e-04</td>\n",
       "      <td>1.986818e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>4.664788e-07</td>\n",
       "      <td>3.326158e-07</td>\n",
       "      <td>1.879099e-07</td>\n",
       "      <td>4.614634e-08</td>\n",
       "      <td>1.732567e-17</td>\n",
       "      <td>3.238165e-07</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Pericytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCAGTGCGAAAC-1-FCAImmP7964510</th>\n",
       "      <td>Pericytes</td>\n",
       "      <td>1.477536e-08</td>\n",
       "      <td>1.577190e-03</td>\n",
       "      <td>1.236769e-05</td>\n",
       "      <td>1.134152e-07</td>\n",
       "      <td>1.215027e-06</td>\n",
       "      <td>3.224812e-08</td>\n",
       "      <td>7.083651e-08</td>\n",
       "      <td>1.135012e-06</td>\n",
       "      <td>1.699295e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.546215e-07</td>\n",
       "      <td>9.835366e-07</td>\n",
       "      <td>8.720760e-08</td>\n",
       "      <td>2.191711e-08</td>\n",
       "      <td>1.404760e-19</td>\n",
       "      <td>3.613958e-08</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Pericytes</td>\n",
       "      <td>Pericytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCCATGAGT-1-FCAImmP7964510</th>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>4.532674e-07</td>\n",
       "      <td>4.647544e-07</td>\n",
       "      <td>2.212564e-06</td>\n",
       "      <td>2.974463e-06</td>\n",
       "      <td>1.553380e-06</td>\n",
       "      <td>2.124671e-06</td>\n",
       "      <td>1.731951e-06</td>\n",
       "      <td>2.041023e-06</td>\n",
       "      <td>5.534186e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.340904e-06</td>\n",
       "      <td>9.565093e-07</td>\n",
       "      <td>1.015629e-06</td>\n",
       "      <td>1.390459e-06</td>\n",
       "      <td>2.033909e-10</td>\n",
       "      <td>1.078917e-06</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Early erythroid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCGCAAGCC-1-FCAImmP7964510</th>\n",
       "      <td>LE</td>\n",
       "      <td>2.256458e-07</td>\n",
       "      <td>2.015986e-06</td>\n",
       "      <td>1.393328e-01</td>\n",
       "      <td>6.879180e-10</td>\n",
       "      <td>4.974338e-07</td>\n",
       "      <td>2.818557e-11</td>\n",
       "      <td>2.924471e-08</td>\n",
       "      <td>4.774858e-06</td>\n",
       "      <td>7.361988e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.698554e-08</td>\n",
       "      <td>7.992969e-09</td>\n",
       "      <td>3.424883e-05</td>\n",
       "      <td>8.645525e-09</td>\n",
       "      <td>9.385402e-16</td>\n",
       "      <td>2.426987e-09</td>\n",
       "      <td>LE</td>\n",
       "      <td>LE</td>\n",
       "      <td>LE</td>\n",
       "      <td>LE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTTGTCATCTGCTTGC-1-FCAImmP7964510</th>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>1.623847e-07</td>\n",
       "      <td>2.205435e-07</td>\n",
       "      <td>8.681296e-07</td>\n",
       "      <td>2.402856e-06</td>\n",
       "      <td>3.523134e-07</td>\n",
       "      <td>9.190970e-06</td>\n",
       "      <td>3.356865e-07</td>\n",
       "      <td>7.526139e-07</td>\n",
       "      <td>2.643936e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.930269e-06</td>\n",
       "      <td>5.888280e-07</td>\n",
       "      <td>2.187473e-07</td>\n",
       "      <td>4.066969e-07</td>\n",
       "      <td>1.072499e-10</td>\n",
       "      <td>1.492302e-06</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Early erythroid</td>\n",
       "      <td>Early erythroid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186533 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       predicted  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879            LYVE1++ macrophage   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879                      Monocyte   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879            LYVE1++ macrophage   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879  Eo/baso/mast cell progenitor   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879            LYVE1++ macrophage   \n",
       "...                                                          ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                      Pericytes   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                      Pericytes   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                Early erythroid   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                             LE   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                Early erythroid   \n",
       "\n",
       "                                            ASDC    Adipocytes      Arterial  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879  3.121147e-08  2.942001e-08  5.584572e-09   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  8.021268e-05  1.136223e-09  8.952242e-09   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879  4.388417e-06  2.971849e-11  7.536972e-09   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879  4.141562e-05  1.333213e-08  2.398914e-09   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879  8.686127e-08  3.933139e-10  1.384100e-07   \n",
       "...                                          ...           ...           ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510   8.901155e-08  3.238348e-06  8.571223e-06   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510   1.477536e-08  1.577190e-03  1.236769e-05   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510   4.532674e-07  4.647544e-07  2.212564e-06   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510   2.256458e-07  2.015986e-06  1.393328e-01   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510   1.623847e-07  2.205435e-07  8.681296e-07   \n",
       "\n",
       "                                          B cell         Basal    CD4 T cell  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879  5.133540e-10  3.132832e-09  2.463025e-10   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  7.051683e-09  8.112603e-09  1.610157e-11   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879  2.891398e-09  3.302528e-09  1.790903e-11   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879  2.734619e-09  3.996713e-09  6.980059e-07   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879  2.455699e-10  2.286061e-08  1.976017e-12   \n",
       "...                                          ...           ...           ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510   1.924168e-08  3.902949e-07  1.215860e-06   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510   1.134152e-07  1.215027e-06  3.224812e-08   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510   2.974463e-06  1.553380e-06  2.124671e-06   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510   6.879180e-10  4.974338e-07  2.818557e-11   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510   2.402856e-06  3.523134e-07  9.190970e-06   \n",
       "\n",
       "                                      CD8 T cell  Capillary (venular tip)  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879  7.582230e-10             4.643047e-07   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  6.927696e-07             4.949447e-11   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879  3.914251e-10             1.995001e-08   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879  3.711175e-09             1.472187e-08   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879  5.729960e-11             5.051004e-08   \n",
       "...                                          ...                      ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510   5.714410e-07             3.698724e-04   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510   7.083651e-08             1.135012e-06   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510   1.731951e-06             2.041023e-06   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510   2.924471e-08             4.774858e-06   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510   3.356865e-07             7.526139e-07   \n",
       "\n",
       "                                    Capillary/postcapillary venule  ...  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879                    1.149033e-08  ...   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879                    1.130580e-08  ...   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879                    2.178952e-07  ...   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879                    3.175144e-09  ...   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879                    6.295254e-09  ...   \n",
       "...                                                            ...  ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                     1.986818e-07  ...   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                     1.699295e-07  ...   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                     5.534186e-06  ...   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                     7.361988e-04  ...   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                     2.643936e-07  ...   \n",
       "\n",
       "                                    Suprabasal IFE  TREM2+ macrophage  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879    8.762978e-08       1.004222e-04   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879    5.018859e-09       1.562070e-07   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879    2.377020e-08       2.140535e-04   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879    5.670171e-09       3.371311e-08   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879    1.143184e-08       1.045524e-04   \n",
       "...                                            ...                ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510     4.664788e-07       3.326158e-07   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510     2.546215e-07       9.835366e-07   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510     2.340904e-06       9.565093e-07   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510     7.698554e-08       7.992969e-09   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510     1.930269e-06       5.888280e-07   \n",
       "\n",
       "                                    Tip cell (arterial)          Treg  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879         1.232687e-06  1.720775e-09   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879         8.123187e-09  1.345388e-06   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879         1.696714e-07  1.792986e-08   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879         1.082911e-08  7.374140e-11   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879         2.108652e-08  1.197810e-09   \n",
       "...                                                 ...           ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510          1.879099e-07  4.614634e-08   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510          8.720760e-08  2.191711e-08   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510          1.015629e-06  1.390459e-06   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510          3.424883e-05  8.645525e-09   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510          2.187473e-07  4.066969e-07   \n",
       "\n",
       "                                    WNT2+ fibroblast           pDC  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879      2.245473e-12  2.170336e-07   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879      2.181822e-15  9.321094e-07   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879      4.258448e-15  1.052342e-06   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879      1.589416e-12  1.098155e-07   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879      2.699467e-14  4.509701e-09   \n",
       "...                                              ...           ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510       1.732567e-17  3.238165e-07   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510       1.404760e-19  3.613958e-08   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510       2.033909e-10  1.078917e-06   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510       9.385402e-16  2.426987e-09   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510       1.072499e-10  1.492302e-06   \n",
       "\n",
       "                                                 confident_calls  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879            LYVE1++ macrophage   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879                      Monocyte   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879            LYVE1++ macrophage   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879  Eo/baso/mast cell progenitor   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879            LYVE1++ macrophage   \n",
       "...                                                          ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                      Pericytes   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                      Pericytes   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                Early erythroid   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                             LE   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                Early erythroid   \n",
       "\n",
       "                                               joint_annotation_20220202  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879                    LYVE1++ macrophage   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  Monocyte (activated/differentiating)   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879                    LYVE1++ macrophage   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879          Eo/baso/mast cell progenitor   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879                    LYVE1++ macrophage   \n",
       "...                                                                  ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                              Pericytes   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                              Pericytes   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                        Early erythroid   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                                     LE   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                        Early erythroid   \n",
       "\n",
       "                                               consensus_clus_prediction  \\\n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879                    LYVE1++ macrophage   \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  Monocyte (activated/differentiating)   \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879                    LYVE1++ macrophage   \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879          Eo/baso/mast cell progenitor   \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879                    LYVE1++ macrophage   \n",
       "...                                                                  ...   \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                              Pericytes   \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                              Pericytes   \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                        Early erythroid   \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                                     LE   \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                        Early erythroid   \n",
       "\n",
       "                                                             orig_labels  \n",
       "AAACCTGGTCAGTGGA-1-4834STDY7002879                    LYVE1++ macrophage  \n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879  Monocyte (activated/differentiating)  \n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879                    LYVE1++ macrophage  \n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879          Eo/baso/mast cell progenitor  \n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879                    LYVE1++ macrophage  \n",
       "...                                                                  ...  \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                              Pericytes  \n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                              Pericytes  \n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                        Early erythroid  \n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                                     LE  \n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                        Early erythroid  \n",
       "\n",
       "[186533 rows x 88 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-communications",
   "metadata": {},
   "source": [
    "# Label stability scoring for harmonisation\n",
    "- here we present Log-loss function to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "generic-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "def compute_label_log_losses(df, true_label, pred_columns):\n",
    "    \"\"\"\n",
    "    Compute log loss (cross-entropy loss).\n",
    "    \n",
    "    Parameters:\n",
    "    df : dataframe containing the predicted probabilities and original labels as columns\n",
    "    true_label : column or array-like of shape (n_samples,) containg cateogrical labels\n",
    "    pred_columns : columns or array-like of shape (n_samples, n_clases) containg predicted probabilities\n",
    "\n",
    "    converts to:\n",
    "    y_true : array-like of shape (n_samples,) True labels. The binary labels in a one-vs-rest fashion.\n",
    "    y_pred : array-like of shape (n_samples, n_classes) Predicted probabilities. \n",
    "        \n",
    "    Returns:\n",
    "    log_loss : dictionary of celltype key and float\n",
    "    weights : float\n",
    "    \"\"\"\n",
    "    log_losses = {}\n",
    "    y_true = (pd.get_dummies(df[true_label]))\n",
    "    y_pred = df[pred_col]\n",
    "    loss = log_loss(np.array(y_true), np.array(y_pred))\n",
    "    for label in range(y_true.shape[1]):\n",
    "        log_loss_label = log_loss(np.array(y_true)[:, label], np.array(y_pred)[:, label])\n",
    "        log_losses[list(y_true.columns)[label]] = (log_loss_label)\n",
    "    weights = 1/np.array(list(log_losses.values()))\n",
    "    weights /= np.sum(weights)\n",
    "    weights = np.array(weights)\n",
    "    return loss, log_losses, weights\n",
    "\n",
    "def regression_results(df, true_label, pred_label, pred_columns):\n",
    "    # Regression metrics\n",
    "    y_true = df[true_label]\n",
    "    y_pred = df[pred_label]\n",
    "    loss, log_losses, weights = compute_label_log_losses(df, true_label, pred_columns)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "#     r2=metrics.r2_score(y_true, y_pred)\n",
    "    print('Cross entropy loss: ', round(loss,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    print('label Cross entropy loss: ')\n",
    "    print(log_losses)  \n",
    "    return loss, log_losses, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "hungry-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_col shape should match the pred_out original labels, so some self-projection works best here\n",
    "pred_col = list(pred_out.columns[pred_out.columns.isin(set(pred_out['orig_labels']))])\n",
    "log_losses, weights = compute_label_log_losses(pred_out, 'orig_labels', pred_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "italic-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = (pd.get_dummies(df[true_label]))\n",
    "y_pred = df[pred_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "comprehensive-congress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.211918064809838"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(np.array(y_true), np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "affecting-marshall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASDC': 0.002166725283596194,\n",
       " 'Iron-recycling macrophage': 0.17990448424623018,\n",
       " 'Adipocytes': 0.2534050368123189,\n",
       " 'Arterial': 0.028345456842149636,\n",
       " 'B cell': 0.17682387615268078,\n",
       " 'Basal': 0.11000984482862723,\n",
       " 'POSTN+ basal': 0.033250798140136956,\n",
       " 'CD4 T cell': 0.3915087085051776,\n",
       " 'CD8 T cell': 0.14488004682886188,\n",
       " 'Capillary (venular tip)': 0.04270657904168145,\n",
       " 'Capillary/postcapillary venule': 0.05345640026131761,\n",
       " 'Companion layer': 0.018185981129859276,\n",
       " 'Cuticle/cortex': 0.033050182304249914,\n",
       " 'DC1': 0.26097126075650373,\n",
       " 'DC2': 0.44995462121622004,\n",
       " 'Dermal condensate': 0.42455929943498627,\n",
       " 'Dermal papillia': 0.3954969581789894,\n",
       " 'Early LE': 0.017147790328675704,\n",
       " 'Early endothelial cell': 0.014492674947992435,\n",
       " 'Early erythroid': 0.04130081987719835,\n",
       " 'Early erythroid (embryonic)': 0.03961955728936468,\n",
       " 'FRZB+ early fibroblast': 1.2125858973233294,\n",
       " 'HOXC5+ early fibroblast': 1.4776061019723772,\n",
       " 'Early myocytes': 0.4424495807573584,\n",
       " 'Eo/baso/mast cell progenitor': 0.6373663339553516,\n",
       " 'Erythroid (embryonic)': 0.053315099113778804,\n",
       " 'Erythroid (fetal)': 0.12311730959931201,\n",
       " 'PEAR1+ fibroblast': 0.3002962104178935,\n",
       " 'WNT2+ fibroblast': 2.1000401165148617,\n",
       " 'HSC': 0.010621690158507568,\n",
       " 'ILC2': 0.031178198900406157,\n",
       " 'ILC3': 0.092742504811675,\n",
       " 'Immature basal': 0.010713407592821743,\n",
       " 'Immature suprabasal': 0.0011498296265687296,\n",
       " 'Inflammatory DC': 0.01214933807839791,\n",
       " 'Innate T type1': 0.06994374436998532,\n",
       " 'Innate T type3': 0.023713638283038255,\n",
       " 'Inner root sheath': 0.017811089892746258,\n",
       " 'LC': 0.043615535820027446,\n",
       " 'LE': 0.5604518002046341,\n",
       " 'LTi': 0.02508309879993636,\n",
       " 'Lymphoid progenitor': 0.007881632424169258,\n",
       " 'MEMP - Early erythroid': 0.0037137565668945026,\n",
       " 'MEMP - Megak': 0.054591751972061585,\n",
       " 'MHCII+ macrophage': 0.18966838019201687,\n",
       " 'LYVE1++ macrophage': 1.3397091379023416,\n",
       " 'Mast cell (earliest)': 0.01787014995458232,\n",
       " 'Mast cell (medium)': 0.07680518413643035,\n",
       " 'Mast cell (most mature)': 0.08262110101388988,\n",
       " 'Matrix/placode': 0.020897429916910258,\n",
       " 'Megakaryocyte': 0.019088580355348404,\n",
       " 'Melanoblast': 0.06957182444514351,\n",
       " 'Melanocyte': 0.07313233188420358,\n",
       " 'TREM2+ macrophage': 0.15937467091266516,\n",
       " 'Monocyte': 0.34814774264292747,\n",
       " 'Monocyte (activated/differentiating)': 0.3460835437854166,\n",
       " 'Monocyte precursor': 0.1606301048514966,\n",
       " 'LMCD1+ mural cell': 0.21129927570333143,\n",
       " 'PLN+ mural cell': 0.25470605379369243,\n",
       " 'Myelinating Schwann cells': 0.07937900704395416,\n",
       " 'Myoblasts': 0.37917568857407485,\n",
       " 'Myocytes': 0.005940270946041694,\n",
       " 'Myofibroblasts': 0.29641584148130456,\n",
       " 'NK cell': 0.6239879242995827,\n",
       " 'Neuroendocrine': 0.1308260830444963,\n",
       " 'Neuron progenitors': 0.04135717969131886,\n",
       " 'Neutrophil1': 0.07600040016288458,\n",
       " 'Neutrophil2': 0.01711824202062346,\n",
       " 'Outer root sheath': 0.06449166951607559,\n",
       " 'Pericytes': 0.49624412233201354,\n",
       " 'Periderm': 0.013968058569135659,\n",
       " 'Postcapillary venule': 0.011971565114173343,\n",
       " 'Pre B cell': 0.013642428476060488,\n",
       " 'Pre pro B cell': 0.8865464252218878,\n",
       " 'Pre-dermal condensate': 1.7671306028314284,\n",
       " 'Pro B cell': 0.03302728376149846,\n",
       " 'SPP1+ proliferating neuron proneitors': 0.030075993523056264,\n",
       " 'PID1+ schwann cellls': 0.014226133968158325,\n",
       " 'Schwann/Schwann precursors': 0.2507876438582734,\n",
       " 'Suprabasal IFE': 0.003770076147732522,\n",
       " 'Tip cell (arterial)': 0.02238541326472474,\n",
       " 'Treg': 1.0380578398285616,\n",
       " 'pDC': 0.004378755996921684}"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "nutritional-wages",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.11354782e-01, 1.34112956e-03, 9.52132696e-04, 8.51195387e-03,\n",
       "       1.36449458e-03, 2.19321481e-03, 7.25622344e-03, 6.16270381e-04,\n",
       "       1.66534472e-03, 5.64960309e-03, 4.51349548e-03, 1.32670995e-02,\n",
       "       7.30026899e-03, 9.24527936e-04, 5.36221231e-04, 5.68295692e-04,\n",
       "       6.10055820e-04, 1.40703389e-02, 1.66480806e-02, 5.84189906e-03,\n",
       "       6.08980103e-03, 1.98975777e-04, 1.63287916e-04, 5.45316871e-04,\n",
       "       3.78550306e-04, 4.52545761e-03, 1.95971811e-03, 8.03457428e-04,\n",
       "       1.14890768e-04, 2.27153322e-02, 7.73858752e-03, 2.60156033e-03,\n",
       "       2.25208664e-02, 2.09835627e-01, 1.98591248e-02, 3.44956112e-03,\n",
       "       1.01745341e-02, 1.35463479e-02, 5.53186420e-03, 4.30501286e-04,\n",
       "       9.61903563e-03, 3.06123412e-02, 6.49679689e-02, 4.41962773e-03,\n",
       "       1.27208985e-03, 1.80095227e-04, 1.35015779e-02, 3.14139239e-03,\n",
       "       2.92026151e-03, 1.15456887e-02, 1.26397677e-02, 3.46800192e-03,\n",
       "       3.29915941e-03, 1.51388687e-03, 6.93025378e-04, 6.97158895e-04,\n",
       "       1.50205481e-03, 1.14186487e-03, 9.47269283e-04, 3.03953438e-03,\n",
       "       6.36315113e-04, 4.06168714e-02, 8.13975460e-04, 3.86666491e-04,\n",
       "       1.84424402e-03, 5.83393797e-03, 3.17465724e-03, 1.40946261e-02,\n",
       "       3.74118429e-03, 4.86202678e-04, 1.72733540e-02, 2.01540249e-02,\n",
       "       1.76856504e-02, 2.72151818e-04, 1.36535025e-04, 7.30533043e-03,\n",
       "       8.02218623e-03, 1.69599992e-02, 9.62069810e-04, 6.39974397e-02,\n",
       "       1.07782339e-02, 2.32429458e-04, 5.51013167e-02])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "solved-peoples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASDC': 0.002166725283596194,\n",
       " 'Iron-recycling macrophage': 0.17990448424623018,\n",
       " 'Adipocytes': 0.2534050368123189,\n",
       " 'Arterial': 0.028345456842149636,\n",
       " 'B cell': 0.17682387615268078,\n",
       " 'Basal': 0.11000984482862723,\n",
       " 'POSTN+ basal': 0.033250798140136956,\n",
       " 'CD4 T cell': 0.3915087085051776,\n",
       " 'CD8 T cell': 0.14488004682886188,\n",
       " 'Capillary (venular tip)': 0.04270657904168145,\n",
       " 'Capillary/postcapillary venule': 0.05345640026131761,\n",
       " 'Companion layer': 0.018185981129859276,\n",
       " 'Cuticle/cortex': 0.033050182304249914,\n",
       " 'DC1': 0.26097126075650373,\n",
       " 'DC2': 0.44995462121622004,\n",
       " 'Dermal condensate': 0.42455929943498627,\n",
       " 'Dermal papillia': 0.3954969581789894,\n",
       " 'Early LE': 0.017147790328675704,\n",
       " 'Early endothelial cell': 0.014492674947992435,\n",
       " 'Early erythroid': 0.04130081987719835,\n",
       " 'Early erythroid (embryonic)': 0.03961955728936468,\n",
       " 'FRZB+ early fibroblast': 1.2125858973233294,\n",
       " 'HOXC5+ early fibroblast': 1.4776061019723772,\n",
       " 'Early myocytes': 0.4424495807573584,\n",
       " 'Eo/baso/mast cell progenitor': 0.6373663339553516,\n",
       " 'Erythroid (embryonic)': 0.053315099113778804,\n",
       " 'Erythroid (fetal)': 0.12311730959931201,\n",
       " 'PEAR1+ fibroblast': 0.3002962104178935,\n",
       " 'WNT2+ fibroblast': 2.1000401165148617,\n",
       " 'HSC': 0.010621690158507568,\n",
       " 'ILC2': 0.031178198900406157,\n",
       " 'ILC3': 0.092742504811675,\n",
       " 'Immature basal': 0.010713407592821743,\n",
       " 'Immature suprabasal': 0.0011498296265687296,\n",
       " 'Inflammatory DC': 0.01214933807839791,\n",
       " 'Innate T type1': 0.06994374436998532,\n",
       " 'Innate T type3': 0.023713638283038255,\n",
       " 'Inner root sheath': 0.017811089892746258,\n",
       " 'LC': 0.043615535820027446,\n",
       " 'LE': 0.5604518002046341,\n",
       " 'LTi': 0.02508309879993636,\n",
       " 'Lymphoid progenitor': 0.007881632424169258,\n",
       " 'MEMP - Early erythroid': 0.0037137565668945026,\n",
       " 'MEMP - Megak': 0.054591751972061585,\n",
       " 'MHCII+ macrophage': 0.18966838019201687,\n",
       " 'LYVE1++ macrophage': 1.3397091379023416,\n",
       " 'Mast cell (earliest)': 0.01787014995458232,\n",
       " 'Mast cell (medium)': 0.07680518413643035,\n",
       " 'Mast cell (most mature)': 0.08262110101388988,\n",
       " 'Matrix/placode': 0.020897429916910258,\n",
       " 'Megakaryocyte': 0.019088580355348404,\n",
       " 'Melanoblast': 0.06957182444514351,\n",
       " 'Melanocyte': 0.07313233188420358,\n",
       " 'TREM2+ macrophage': 0.15937467091266516,\n",
       " 'Monocyte': 0.34814774264292747,\n",
       " 'Monocyte (activated/differentiating)': 0.3460835437854166,\n",
       " 'Monocyte precursor': 0.1606301048514966,\n",
       " 'LMCD1+ mural cell': 0.21129927570333143,\n",
       " 'PLN+ mural cell': 0.25470605379369243,\n",
       " 'Myelinating Schwann cells': 0.07937900704395416,\n",
       " 'Myoblasts': 0.37917568857407485,\n",
       " 'Myocytes': 0.005940270946041694,\n",
       " 'Myofibroblasts': 0.29641584148130456,\n",
       " 'NK cell': 0.6239879242995827,\n",
       " 'Neuroendocrine': 0.1308260830444963,\n",
       " 'Neuron progenitors': 0.04135717969131886,\n",
       " 'Neutrophil1': 0.07600040016288458,\n",
       " 'Neutrophil2': 0.01711824202062346,\n",
       " 'Outer root sheath': 0.06449166951607559,\n",
       " 'Pericytes': 0.49624412233201354,\n",
       " 'Periderm': 0.013968058569135659,\n",
       " 'Postcapillary venule': 0.011971565114173343,\n",
       " 'Pre B cell': 0.013642428476060488,\n",
       " 'Pre pro B cell': 0.8865464252218878,\n",
       " 'Pre-dermal condensate': 1.7671306028314284,\n",
       " 'Pro B cell': 0.03302728376149846,\n",
       " 'SPP1+ proliferating neuron proneitors': 0.030075993523056264,\n",
       " 'PID1+ schwann cellls': 0.014226133968158325,\n",
       " 'Schwann/Schwann precursors': 0.2507876438582734,\n",
       " 'Suprabasal IFE': 0.003770076147732522,\n",
       " 'Tip cell (arterial)': 0.02238541326472474,\n",
       " 'Treg': 1.0380578398285616,\n",
       " 'pDC': 0.004378755996921684}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "df_i = pred_out.copy() # df which contains proba, orig and pred labels\n",
    "orig_labels = 'orig_labels'\n",
    "pred_labels = 'predicted'\n",
    "\n",
    "# function\n",
    "class_loss = {}\n",
    "proba = df_i[df_i.columns[(df_i.columns.isin(list(df_i[pred_labels])))]]\n",
    "idx_map = dict(zip(list((set(list(df_i[orig_labels]) + list(df_i[pred_labels])))),list(range(0,len(list(set(list(df_i[orig_labels]) + list(df_i[pred_labels]))))))))\n",
    "for class_n in list(set(df_i[orig_labels])):\n",
    "    class_n_df = df_i.loc[df_i[orig_labels].isin([class_n]),[orig_labels,pred_labels]]\n",
    "    class_n_proba = proba\n",
    "    #class_R2[class_n] = regression_results(class_n_df[orig_labels].map(idx_map),class_n_df[pred_labels].map(idx_map))\n",
    "    class_loss[class_n] = compute_log_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "outer-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba = df_i[df_i.columns[(df_i.columns.isin(list(df_i[pred_labels])))]]\n",
    "# compute_log_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "entire-absorption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTAACTTCACCGAAAG-1-FCAImmP7241241    82\n",
       "ATCGAGTCAAGTTGTC-1-FCAImmP7528290    82\n",
       "CAGCGACGTTCTCATT-1-FCAImmP7528291    82\n",
       "TCATTACCAGCGAACA-1-FCAImmP7555848    82\n",
       "TCGTAGATCTAACTGG-1-FCAImmP7579213    82\n",
       "                                     ..\n",
       "GTGCATATCCTTGCCA-1-FCAImmP7964510    82\n",
       "GTGGGTCAGGGAACGG-1-FCAImmP7964510    82\n",
       "GTGTTAGAGCAGCCTC-1-FCAImmP7964510    82\n",
       "TACTTACAGGCGTACA-1-FCAImmP7964510    82\n",
       "TGGGCGTAGTCGTTTG-1-FCAImmP7964510    82\n",
       "Name: orig_labels, Length: 65, dtype: category\n",
       "Categories (83, int64): [57, 23, 42, 49, ..., 79, 68, 9, 40]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_n_df = df_i.loc[df_i[orig_labels].isin([class_n]),[orig_labels,pred_labels]]\n",
    "class_n_df[orig_labels].map(idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "frozen-clause",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAACCTGGTCAGTGGA-1-4834STDY7002879              LYVE1++ macrophage\n",
       "AAAGATGGTCGATTGT-1-4834STDY7002879                        Monocyte\n",
       "AAAGCAAAGATGTGGC-1-4834STDY7002879              LYVE1++ macrophage\n",
       "AAAGTAGCAGATCGGA-1-4834STDY7002879    Eo/baso/mast cell progenitor\n",
       "AAAGTAGTCCGCATCT-1-4834STDY7002879              LYVE1++ macrophage\n",
       "                                                  ...             \n",
       "TTTGTCAAGTGAACGC-1-FCAImmP7964510                        Pericytes\n",
       "TTTGTCAGTGCGAAAC-1-FCAImmP7964510                        Pericytes\n",
       "TTTGTCATCCATGAGT-1-FCAImmP7964510                  Early erythroid\n",
       "TTTGTCATCGCAAGCC-1-FCAImmP7964510                               LE\n",
       "TTTGTCATCTGCTTGC-1-FCAImmP7964510                  Early erythroid\n",
       "Name: predicted, Length: 186533, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get regression results per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "understood-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance:  0.8923\n",
      "mean_squared_log_error:  0.1743\n",
      "r2:  0.8898\n",
      "MAE:  1.5957\n",
      "MSE:  50.5453\n",
      "RMSE:  7.1095\n"
     ]
    }
   ],
   "source": [
    "regression_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-klein",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-citizenship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-helena",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression summary\n",
    "idx_map = dict(zip(  list(adata.obs[feat_use].unique()),list(range(0,len(list(adata.obs[feat_use].unique()))))))\n",
    "regression_results(adata.obs[feat_use].map(idx_map), adata.obs['predicted'].map(idx_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['confident_calls'] = pred_out['confident_calls']\n",
    "adata.obs[cluster_prediction] = adata.obs.index\n",
    "for z in adata.obs[clusters_reassign].unique():\n",
    "    df = adata.obs\n",
    "    df = df[(df[clusters_reassign].isin([z]))]\n",
    "    df_count = pd.DataFrame(df[lr_predicted_col].value_counts())\n",
    "    freq_arranged = df_count.index\n",
    "    cat = freq_arranged[0]\n",
    "    df.loc[:,cluster_prediction] = cat\n",
    "    adata.obs.loc[adata.obs[clusters_reassign] == z, [cluster_prediction]] = cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-assets",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-submission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-salem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "right-doctor",
   "metadata": {},
   "source": [
    "# View by median probabilities per classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_probs = pred_out.loc[:, pred_out.columns != 'predicted'].groupby('orig_labels').median()\n",
    "model_mean_probs = model_mean_probs*100\n",
    "model_mean_probs = model_mean_probs.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "crs_tbl = model_mean_probs.copy()\n",
    "# Sort df columns by rows\n",
    "index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "crs_tbl = crs_tbl.loc[index_order]\n",
    "crs_tbl = crs_tbl[col_order]\n",
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.5)\n",
    "g = sns.heatmap(crs_tbl, cmap='viridis_r',  annot=False,vmin=0, vmax=max(np.max(crs_tbl)), linewidths=1, center=max(np.max(crs_tbl))/2, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "\n",
    "plt.ylabel(\"Original labels\")\n",
    "plt.xlabel(\"Training labels\")\n",
    "#plt.savefig('./ldvae_ver5_lr_model_means_subclusters.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-persian",
   "metadata": {},
   "source": [
    "# View by label assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=feat_use\n",
    "y = 'predicted'\n",
    "\n",
    "y_attr = adata_temp.obs[y]\n",
    "x_attr = adata_temp.obs[x]\n",
    "crs = pd.crosstab(x_attr, y_attr)\n",
    "crs_tbl = crs\n",
    "for col in crs_tbl :\n",
    "    crs_tbl[col] = crs_tbl[col].div(crs_tbl[col].sum(axis=0)).multiply(100).round(2)\n",
    "    \n",
    "index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "crs_tbl = crs_tbl.loc[index_order]\n",
    "crs_tbl = crs_tbl[col_order]\n",
    "\n",
    "#plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap='viridis_r', vmin=0, vmax=100, linewidths=1, center=50, square=True, cbar_kws={\"shrink\": 0.3})\n",
    "plt.xlabel(\"Original labels\")\n",
    "plt.ylabel(\"Predicted labels\")\n",
    "# plt.savefig(save_path + \"/LR_predictions_consensus.pdf\")\n",
    "# crs_tbl.to_csv(save_path + \"/post-freq_LR_predictions_consensus_supp_table.csv\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "x='consensus_clus_prediction'\n",
    "y = 'predicted'\n",
    "\n",
    "y_attr = adata_temp.obs[y]\n",
    "x_attr = adata_temp.obs[x]\n",
    "crs = pd.crosstab(x_attr, y_attr)\n",
    "crs_tbl = crs\n",
    "for col in crs_tbl :\n",
    "    crs_tbl[col] = crs_tbl[col].div(crs_tbl[col].sum(axis=0)).multiply(100).round(2)\n",
    "    \n",
    "index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "crs_tbl = crs_tbl.loc[index_order]\n",
    "crs_tbl = crs_tbl[col_order]\n",
    "\n",
    "#plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap='viridis_r', vmin=0, vmax=100, linewidths=1, center=50, square=True, cbar_kws={\"shrink\": 0.3})\n",
    "plt.xlabel(\"Original labels\")\n",
    "plt.ylabel(\"Predicted labels\")\n",
    "# plt.savefig(save_path + \"/LR_predictions_consensus.pdf\")\n",
    "# crs_tbl.to_csv(save_path + \"/post-freq_LR_predictions_consensus_supp_table.csv\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-prayer",
   "metadata": {},
   "source": [
    "# Save predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out.to_csv('./A1_V3_sk_sk_pred_outs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter unlikely predictions\n",
    "# filtered = pred_out[np.max(pred_out.loc[:,~pred_out.columns.isin(['predicted','confident_calls','annot_celltype', 'consensus_clus_prediction', 'orig_labels','clus_prediction_confident'])],axis = 1) > 0.3]\n",
    "# adata_temp = adata[adata.obs.index.isin(filtered.index)]\n",
    "# filtered['clus_prediction_confident'] = adata_temp.obs['clus_prediction_confident']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-richards",
   "metadata": {},
   "source": [
    " # Significant contributors to feature effect size per class of model\n",
    "     - Bear in mind these are only top features..\n",
    "    - assess the positive descriminators (markers) of the model\n",
    "    - â€œâ€¦provide information about the magnitude and direction of the difference between two groups or the relationship between two variables.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(top_loadings['class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_loadings[top_loadings['class'].isin(['Tip cell (arterial)','HSC','SPP1+ proliferating neuron proneitors'])].groupby(['class']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classes in ['Tip cell (arterial)','HSC','SPP1+ proliferating neuron proneitors']:\n",
    "    model_class_feature_plots(top_loadings, [str(classes)], 'e^coef')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "# plot_states = ['Tip cell (arterial)','HSC','SPP1+ proliferating neuron proneitors']\n",
    "markers = top_loadings[top_loadings['class'].isin(adata_temp.obs['consensus_clus_prediction'])].groupby(['class']).head(5).groupby(['class'])['feature'].agg(lambda grp: list(grp)).to_dict()\n",
    "sc.pl.dotplot(adata_temp, groupby = 'consensus_clus_prediction', var_names = markers,standard_scale='var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_loadings[top_loadings['class'].isin(['Lymphoid progenitor','Early erythroid (embryonic)','Pre-dermal condensate'])].groupby(['class']).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-puzzle",
   "metadata": {},
   "source": [
    "# Label confidence scoring\n",
    "- Let's study label stability given K-neighborhood assignments\n",
    "\n",
    "**Author notes:** \n",
    "-  Hey! If you're reading this, I've probably messed up somewhere and you're looking for an explanation why :) \n",
    "- Code blocks marked **Prototype** are usually incomplete or a irresponsible lift from another pipeline, if the source pipeline is already distributed/published, I will leave git links associated with the module.\n",
    "- If there are no links, there should be some run notes\n",
    "\n",
    "**Run mode 2 of prototype $alpha$ $beta$ sampling via leverage-score**\n",
    "- Mode 2 was chosen as we want to define a sampling space which satisfies same KNN distribution and density instead of prioritising variability\n",
    "- Neighborhood assignment is done via majority voting\n",
    "- Posterior probability computed and sampling rate for X is determined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-heating",
   "metadata": {},
   "source": [
    "# Bayesian KNN label stability\n",
    "For modelling label uncertainty given neighborhood membership and distances\n",
    "\n",
    "#### Step 1: Generate Binary Neighborhood Membership Matrix\n",
    "The first step is to generate a binary neighborhood membership matrix from the connectivity matrix. This is done with the function get_binary_neigh_matrix(connectivities), which takes a connectivity matrix as input and outputs a binary matrix indicating whether a cell is a neighbor of another cell.\n",
    "\n",
    "The connectivity matrix represents the neighborhood relationships between cells, typically obtained from KNN analysis. In this matrix, each row and column represent a cell, and an entry indicates the 'connectivity' between the corresponding cells.\n",
    "\n",
    "The function transforms the connectivity matrix into a binary matrix by setting all non-zero values to 1, indicating a neighborhood relationship, and all zero values remain as 0, indicating no neighborhood relationship.\n",
    "\n",
    "#### Step 2: Calculate Label Counts\n",
    "Next, the function get_label_counts(neigh_matrix, labels) is used to count the number of occurrences of each label in the neighborhood of each cell. The input to this function is the binary neighborhood membership matrix and a list of labels for each cell.\n",
    "\n",
    "The function returns a matrix in which each row corresponds to a cell, and each column corresponds to a label. Each entry is the count of cells of a particular label in the neighborhood of a given cell.\n",
    "\n",
    "#### Step 3: Compute Distance-Entropy Product\n",
    "In the third step, the function compute_dist_entropy_product(neigh_membership, labels, dist_matrix) computes the product of the average neighborhood distance and the entropy of the label distribution in the neighborhood for each cell and each label.\n",
    "\n",
    "The entropy of a label distribution in a neighborhood is a measure of the diversity or 'mix' of labels in that neighborhood, with higher entropy indicating a more diverse mix of labels. The average neighborhood distance for a cell is the average distance from that cell to all other cells in its neighborhood.\n",
    "\n",
    "By multiplying the entropy with the average distance, this function captures two important aspects of the neighborhood:\n",
    "\n",
    "Entropy: The diversity of labels in a neighborhood. High entropy means the neighborhood is a 'melting pot' of many different labels, while low entropy indicates a neighborhood dominated by a single label.\n",
    "Distance: The spatial proximity of cells in a neighborhood. A high average distance means the cells in a neighborhood are widely dispersed, while a low average distance indicates a compact, closely-knit neighborhood.\n",
    "Thus, the distance-entropy product for a cell provides a measure of the 'stability' of the cell's label, with lower values indicating a stable, consistent label and higher values indicating an unstable, inconsistent label.\n",
    "\n",
    "#### Step 4: Bayesian Sampling and Weight Calculation\n",
    "The final step is the compute_weights function, which uses Bayesian inference to compute a posterior distribution of the distance-entropy product for each label and calculates the weights.\n",
    "\n",
    "In Bayesian inference, we start with a prior distribution that represents our initial belief about the parameter we're interested in, and we update this belief using observed data to get a posterior distribution.\n",
    "\n",
    "In this case, the prior distribution is a normal distribution with mean and standard deviation equal to the mean and standard deviation of the distance-entropy product for the original labels. The observed data is the distance-entropy product for the predicted labels. A normal distribution is a reasonable choice for the prior because the distance-entropy product is a continuous variable that can theoretically take on any real value, and the normal distribution is the most common distribution for such variables.\n",
    "\n",
    "After sampling from the posterior distribution, the weight for each label is calculated as one minus the ratio of the standard deviation of the posterior distribution to the maximum standard deviation across all labels. This means that labels with a larger standard deviation (indicating greater uncertainty about their stability) will have smaller weights, and labels with a smaller standard deviation (indicating less uncertainty) will have larger weights.\n",
    "\n",
    "The weights are returned as a dictionary where each key-value pair corresponds to a label and its weight.\n",
    "\n",
    "#### Step 5: Apply Weights to Probabilities\n",
    "Finally, the weights are applied to the probability dataframe with the function apply_weights(prob_df, weights). The input to this function is a dataframe where each row corresponds to a cell and each column corresponds to a label, with each entry being the probability of the cell being of the label, and a dictionary of weights.\n",
    "\n",
    "This function multiplies each column of the probability dataframe by the corresponding weight, effectively 'boosting' the probabilities of labels with larger weights and 'penalizing' the probabilities of labels with smaller weights. After applying the weights, the function normalizes the probabilities so that they sum to 1 for each cell, returning a dataframe of the same shape as the input but with the probabilities weighted and normalized.\n",
    "\n",
    "Overall, this method provides a principled way to quantify label uncertainty and adjust the probabilities output by a logistic regression model accordingly. It combines the strengths of KNN, which can capture local structure and relationships in the data, and Bayesian inference, which provides a robust framework for dealing with uncertainty and incorporating prior knowledge. By weighting the probabilities according to the stability of the labels, this method can potentially improve the accuracy and interpretability of the logistic regression model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "ultimate-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def get_binary_neigh_matrix(connectivities):\n",
    "    \"\"\"\n",
    "    Converts the connectivities matrix to a binary neighborhood membership matrix.\n",
    "    \"\"\"\n",
    "    return (connectivities > 0).astype(int)\n",
    "\n",
    "def get_label_counts(neigh_matrix, labels):\n",
    "    \"\"\"\n",
    "    Counts the number of occurrences of each label in the neighborhood of each cell.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(neigh_matrix.T.dot(pd.get_dummies(labels)))\n",
    "\n",
    "def compute_dist_entropy_product(neigh_membership, labels, dist_matrix):\n",
    "    \"\"\"\n",
    "    Computes the product of the average neighborhood distance and the entropy\n",
    "    of the label distribution in the neighborhood for each cell and each label.\n",
    "    \"\"\"\n",
    "    # Count the occurrences of each label in the neighborhood of each cell\n",
    "    label_counts = get_label_counts(neigh_membership, labels)\n",
    "\n",
    "    # Compute the entropy of the label distribution in the neighborhood of each cell\n",
    "    entropy_values = label_counts.apply(entropy, axis=1)\n",
    "\n",
    "    # Compute the average neighborhood distance for each cell\n",
    "    avg_distances = dist_matrix.multiply(neigh_membership).mean(axis=1).A1\n",
    "\n",
    "    # Compute the product of the average distance and the entropy for each cell\n",
    "    dist_entropy_product = avg_distances * entropy_values\n",
    "\n",
    "    return dist_entropy_product\n",
    "\n",
    "class WeightsOutput:\n",
    "    def __init__(self, weights, rhats, means, sds):\n",
    "        self.weights = weights\n",
    "        self.rhats = rhats\n",
    "        self.means = means\n",
    "        self.sds = sds\n",
    "\n",
    "def compute_weights(adata, use_rep, original_labels_col, predicted_labels_col):\n",
    "    # Extract the necessary data from the anndata object\n",
    "    obs_met = adata.obs\n",
    "    neigh_membership = get_binary_neigh_matrix(adata.obsp[adata.uns[use_rep]['connectivities_key']])\n",
    "    original_labels = obs_met[original_labels_col]\n",
    "    predicted_labels = obs_met[predicted_labels_col]\n",
    "    dist_matrix = adata.obsp[adata.uns[use_rep]['distances_key']]\n",
    "\n",
    "    # Compute the 'distance-entropy' product for each cell and each label\n",
    "    dist_entropy_product = compute_dist_entropy_product(neigh_membership, predicted_labels, dist_matrix)\n",
    "\n",
    "    # Compute the 'distance-entropy' product for the original labels\n",
    "    dist_entropy_product_orig = compute_dist_entropy_product(neigh_membership, original_labels, dist_matrix)\n",
    "\n",
    "    weights = {}\n",
    "    rhat_values = {}\n",
    "    means = []  # Collect all posterior means\n",
    "    sds = []  # Collect all posterior standard deviations\n",
    "    for label in np.unique(predicted_labels):\n",
    "        print(\"Sampling {} posterior distribution\".format(label))\n",
    "        # Perform Bayesian inference to compute the posterior distribution of the\n",
    "        # 'distance-entropy' product for this label\n",
    "        orig_pos = obs_met[original_labels_col].isin([label])\n",
    "        pred_pos = obs_met[predicted_labels_col].isin([label])\n",
    "        with pm.Model() as model:\n",
    "            #priors\n",
    "            mu = pm.Normal('mu', mu=dist_entropy_product_orig[orig_pos.values].mean(), sd=dist_entropy_product_orig[orig_pos.values].std())\n",
    "            sd = pm.HalfNormal('sd', sd=dist_entropy_product_orig[orig_pos.values].std())\n",
    "            #observations\n",
    "            obs = pm.Normal('obs', mu=mu, sd=sd, observed=dist_entropy_product_orig[pred_pos.values])\n",
    "            \n",
    "            if len(orig_pos) > 10000:\n",
    "                samp_rate = 0.1\n",
    "                smp = int(len(orig_pos)*samp_rate)\n",
    "                tne = smp = int(len(orig_pos)*samp_rate)/2\n",
    "                trace = pm.sample(smp, tune=tne)\n",
    "            else:\n",
    "                trace = pm.sample(1000, tune=500)\n",
    "        # Compute R-hat for this label\n",
    "        rhat = pm.rhat(trace)\n",
    "        rhat_values[label] = {var: rhat[var].data for var in rhat.variables}\n",
    "        # Compute the mean and the standard deviation of the posterior distribution for this label\n",
    "        mean_posterior = pm.summary(trace)['mean']['mu']\n",
    "        sd_posterior = pm.summary(trace)['sd']['sd']\n",
    "        sds.append(sd_posterior)\n",
    "        means.append(mean_posterior)\n",
    "        \n",
    "    # Mean posterior probabilitty models the stability of a label given entropy_distance measures within it's neighborhood\n",
    "    max_mean = max(means)\n",
    "    # SD here models the uncertainty of label entropy_distance measures\n",
    "    max_sd = max(sds)  # Compute the maximum standard deviation\n",
    "    \n",
    "    # Compute the weights as the sum of the normalized mean and the normalized standard deviation. This makes each weight relative to each other\n",
    "    # shift all weights up by epiislon constant\n",
    "    epsilon = 0.01\n",
    "    for label, mean, sd in zip(np.unique(predicted_labels), means, sds):\n",
    "        weights[label] = (1 - mean / max_mean) * (1 - sd / max_sd) + epsilon\n",
    "\n",
    "    return WeightsOutput(weights, rhat_values, means, sds)\n",
    "\n",
    "def apply_weights(prob_df, weights):\n",
    "    \"\"\"\n",
    "    Applies the computed weights to the probability dataframe and normalizes the result.\n",
    "    Parameters:\n",
    "    prob_df (pd.DataFrame): A dataframe where each row corresponds to a cell and each column corresponds to a label. Each entry is the probability of the cell being of the label.\n",
    "    weights (dict): A dictionary where each key-value pair corresponds to a label and its weight.\n",
    "\n",
    "    Returns:\n",
    "    norm_df (pd.DataFrame): A dataframe of the same shape as prob_df, but with the probabilities weighted and normalized.\n",
    "    \"\"\"\n",
    "    # Apply the weights\n",
    "    weighted_df = prob_df.mul(weights.weights)\n",
    "    # Normalize the result\n",
    "    norm_df = weighted_df.div(weighted_df.sum(axis=1), axis=0)\n",
    "    return norm_df\n",
    "\n",
    "weights = compute_weights(adata,use_rep = 'neighbors', original_labels_col ='cell.labels', predicted_labels_col = 'cell.labels')\n",
    "apply_weights(adata.obsm['cell.labels'],weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "generous-ottawa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling CMP posterior distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/deprecat/classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sd, mu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 2 seconds.\n",
      "The acceptance probability does not match the target. It is 0.8882359954302748, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.9152261534843218, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.8984155277447178, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling HSPC_1 posterior distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/deprecat/classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sd, mu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:06&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 7 seconds.\n",
      "The acceptance probability does not match the target. It is 0.948912690284732, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.9932458013873032, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.8991231533910644, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.8955961583272648, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling HSPC_2 posterior distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/deprecat/classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sd, mu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 2 seconds.\n",
      "The acceptance probability does not match the target. It is 0.8822058084837736, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.890976176435348, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.9033886891848927, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.9106460334686525, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling MOP posterior distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/deprecat/classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sd, mu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:01&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 2 seconds.\n",
      "The acceptance probability does not match the target. It is 0.8925428920498023, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Macrophage posterior distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/deprecat/classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sd, mu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:03&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 4 seconds.\n",
      "The acceptance probability does not match the target. It is 0.9146022607460513, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.9775637974453769, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.8968273691723939, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.9335216100559266, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Pre_Macrophage posterior distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/deprecat/classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [sd, mu]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 2 seconds.\n",
      "The acceptance probability does not match the target. It is 0.8793731411008892, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.9069842863259212, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "The acceptance probability does not match the target. It is 0.8885530536399899, but should be close to 0.8. Try to increase the number of tuning steps.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/home/jovyan/my-conda-envs/workhorse/lib/python3.9/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "weights = compute_weights(adata,use_rep = 'neighbors', original_labels_col ='cell.labels', predicted_labels_col = 'cell.labels')\n",
    "apply_weights(adata.obsm['cell.labels'],weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-potato",
   "metadata": {},
   "source": [
    "The proposed method is a Bayesian Gaussian Process (GP) model that uses prior information from a probabilistic classifier and a K-nearest neighbors (KNN) graph to assess label stability in single-cell data. This approach takes advantage of the strengths of both the Bayesian statistical framework and machine learning to provide a flexible and robust way to evaluate label stability in high-dimensional data, such as single-cell transcriptomics.\n",
    "\n",
    "The Bayesian GP model is designed to model the label stability as a latent function over the single-cell data, which is influenced by both the original labels and the predicted labels from the probabilistic classifier. The model uses a Gaussian Process to represent the latent function, which is a flexible non-parametric model that can capture complex patterns in the data. The Gaussian Process is defined by a covariance function, which determines the similarity between the cells based on their features.\n",
    "\n",
    "The covariance function used in the model is the exponential quadratic function, also known as the squared exponential or Gaussian kernel. This function is a popular choice in Gaussian Process models due to its flexibility and its ability to model a wide range of patterns in the data. It is defined as:\n",
    "\n",
    "$$[k(x, x') = \\sigma^2 \\exp\\left(-\\frac{(x - x')^2}{2l^2}\\right)$$\n",
    "\n",
    "where \\(x\\) and \\(x'\\) are two cells, \\(\\sigma^2\\) is the signal variance (which determines the average distance of the function away from its mean), and \\(l\\) is the length-scale (which determines the smoothness of the function).\n",
    "\n",
    "The length-scale and the noise term are hyperparameters of the model, which are learned from the data using Bayesian inference. The length-scale is modeled as a Gamma distribution, which is a common choice for positive-valued parameters. The noise term is modeled as a Half-Cauchy distribution, which is a heavy-tailed distribution that is often used for scale parameters in Bayesian models.\n",
    "\n",
    "The model uses the log-odds of the labels as the observable variable, which is a common choice for binary or multi-class classification problems. The log-odds is defined as the logarithm of the odds ratio, which is the ratio of the probability of the label being true to the probability of the label being false. For a binary classification problem, the log-odds is defined as:\n",
    "\n",
    "\n",
    "$$\\log\\left(\\frac{p}{1 - p}\\right)\\$$\n",
    "\n",
    "where \\(p\\) is the probability of the label being true.\n",
    "\n",
    "The strength of this approach lies in its ability to model the uncertainty in the label assignments and to incorporate prior information from the probabilistic classifier and the KNN graph. By using Bayesian inference, the model is able to provide a measure of uncertainty for the label stability, which can be used to assess the reliability of the label assignments.\n",
    "\n",
    "This model is designed to output a measure of label stability for each cell, which can be interpreted as the likelihood that a cell forms consistent neighborhoods in the KNN graph. This measure of label stability can then be used as weights for the output of the probabilistic classifier, to prioritize labels that are more likely to form consistent neighborhoods.\n",
    "\n",
    "In terms of feasibility, this approach requires the computation of the KNN graph and the inference of the Bayesian GP model, which can be computationally intensive for large datasets. However, the use of modern Bayesian computational techniques, such as variational inference and Markov chain Monte Carlo (MCMC) methods, can make this approach feasible for large single-cell datasets. Furthermore, the model's output can be easily integrated with the output of the probabilistic classifier, providing a way to improve the classifier's performance by taking into account the label stability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-poster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "workhorse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
