{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proof-bridges",
   "metadata": {},
   "source": [
    "# Hypotheses:\n",
    "- There is a time-encoded relationship between similarity of orgabnoid development and fetal skin development\n",
    "\n",
    "# Assumptions:\n",
    "- Measurable differences between both in-vivo and in-vitro systems are attributed to two parameters:\n",
    "\n",
    "     **Measureable Parameters**\n",
    "    - 1. There is a relationship between development time and transcriptomic representation of cell-states\n",
    "    - 2. There is a relationship between time and the cell-state distribution of both in-vitro and in-vivo systems\n",
    "\n",
    "# Objectives:\n",
    "\n",
    "**2 main goals are to:**\n",
    "- Find some time encoding for transcriptomic difference by cell-state\n",
    "- Find if two timepoints have similar distributions\n",
    " \n",
    "# Approach:\n",
    "- Integrate all data using an ldVAE/VAE, if there is indeed some time-specific RNA variation, the model should hopefully learn this, unless we did a very bad job in accoiunting for batch etc.\n",
    "- Train an SVM/LR on the time-encoded cell-state information provided by the low-dimensional embeddings\n",
    "- Find the cell-state-time specific probabillisitc projection probabilities\n",
    "    \n",
    "    **Time_vivo vs time_vitro comparisons**\n",
    "    - We want to find some combination of global cell-state projection probabilities\n",
    "    - We will weigh these probabilities by some measure of distributional similarity between the model and query data (likelihood function)\n",
    "    - Aggregate the posterior projected probabilities for the labels which are matched in training-time for each time-point in the query data.\n",
    "    - Options: \n",
    "        - project trained model on each target timepoint individually\n",
    "        - project trained model across entire fetal/organoid data, then disentangle the probabilities. (aggregated probabilities per cell will still sum to one, we can take the median aggregated probabilities for each cell-state-time label for a given class.)\n",
    "    - Optionally normalise such that summing the aggregated probabilities for a specific training-timepoint should = 1. \n",
    "    \n",
    "    - Since we pass a matrix of 'prior' probabilities (output for LR/SVM) the corrected probability is a single value per celltype/time encoding\n",
    "\n",
    "    **Individual cell-states by time comparisons**\n",
    "    - Here we no longer want to penalise the pribabilities by distribution as we are interested in only the similarity of the transcriptomic landscape\n",
    "    - We take the organised probability by cell-state with time encodings and find the median projection probability for matching or predicted cellstates in the query data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-questionnaire",
   "metadata": {},
   "source": [
    "**Liklihood function**\n",
    "- Bayesian model contructed as a loop for each training time-point (to keeop the observatrions standard). \n",
    "- Prior = SVM/LR probabilities per cellstate-time (weighted by reciprocal of donor and cell-state availability). Provided as seperate matrices for labels by time for each loop\n",
    "- Observation = original observed probability in the training data (skin/organoid)\n",
    "- Output: one posterior probability per cell-state-time label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-window",
   "metadata": {},
   "source": [
    "Write code to apply logistic regression to train a model on expression data from single-cell RNA-seq data in the anndata format. The model should learn time and celltype information and project this onto another data. Use a bayesian framework to output posterior probabilities based on the output from the logistic regression model as priors and the actual celltype by time distribution as observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-punch",
   "metadata": {},
   "source": [
    "# Fskin approach\n",
    "- Train SVM on time-encoded celltype representation of the Fskin data\n",
    "- Create a leverage score sampled set of cells for each celltrype in the SVM model\n",
    "- Do not bother subsetting each data to only cells present in the organoid data\n",
    "\n",
    "- Not subsetting to only cell-tyoes present in all data could introduce issues for cross-compartability, but presents the most impartial view of probabillistic correspondecne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-scientist",
   "metadata": {},
   "source": [
    "# Emprical Bayes framework for SVM deconvolution and classification of single cell and bulk data\n",
    "\n",
    "### Motivations:\n",
    "- Single cell label transfer tasks are a complex task where experimental design parameters or other confounders such as cell-state distribution/sampling are often not accounted for. \n",
    "- Linea\n",
    "\n",
    "- Include experimental design and known biology into the analytical framework:\n",
    "    - Ability to incoperate prior information and uncertainties on cell type distribution into the modelling process, helps us identify the liklihood of cell-state matching assuming the query data is of the same procerssing\n",
    "    - If the query data is not of the same system/distribution, the liklihood function penalises the projection  \n",
    "    - By incorporating priors, the Bayesian probabilistic SVM can explicitly account for uncertainties in the cell-type distribution, which is especially valuable when dealing with noisy or incomplete data. This allows the model to provide probabilistic predictions and quantify the uncertainty associated with each prediction.\n",
    "    \n",
    "- Why SVM?\n",
    "    - Compared to linear regression methods like logistic regression, Bayesian probabilistic SVM offers a more flexible and robust approach that can better capture complex relationships and uncertainties in the data. \n",
    "    - **Less sensitive to feature scaling:** SVM is generally less sensitive to feature scaling than logistic regression making it suitable for cross-modality learning and projection. While logistic regression's performance can be affected by different feature scales, SVM typically normalizes the feature values during the training process, making it less dependent on feature scaling.\n",
    "    - **Non-linear relationships:** SVM can handle non-linear relationships between the input variables and the target variable by using different kernel functions. This flexibility allows SVM to capture more complex patterns and make non-linear decision boundaries, while logistic regression assumes a linear relationship between the features and the target variable.\n",
    "    - **Robustness to outliers:** SVM is generally more robust to outliers in the data compared to logistic regression. SVM aims to maximize the margin between classes, which helps it to be less influenced by outliers that might significantly affect the decision boundary of logistic regression.\n",
    "    - **Effective in high-dimensional spaces:** SVM is particularly effective in high-dimensional spaces, where the number of features is large compared to the number of samples. SVM uses a subset of the training points called support vectors to determine the decision boundary, making it more memory-efficient and computationally efficient in high-dimensional scenarios compared to logistic regression.\n",
    "\n",
    "\n",
    "\n",
    "### Architectural summary:\n",
    "\n",
    "- **Support Vector Machine framework**\n",
    "    - We train a single binary classifier for each class versus all. \n",
    "    - Features are transformed by the Nystroem approximation transformer for the RNF kernel based on a set of randomly sampled features (sampling and eigen decomposition)\n",
    "    - Nystroem transofmred features are input as training data into the SVM, the RNF kernel computes node-node relationships in the n-1 dimensions\n",
    "    - Bayesian oprimisation is used to find an optimal Gamma and Sparcity hyper parameter\n",
    "    - Platt scaled-distance to the hyperplane is used to estimate probabilites belonging to each class (fitting a lgistic regression model which aims to learn the mapping between the decision scores and the true class probabilities per pariwise couplings. )\n",
    "\n",
    "\n",
    "- **Emperical Bayesian framework:**\n",
    "    - Model employs an iterative hypergeomtric sampling step per batch which provides refined cell-type distribution priors which are robust to oversampled/undersampled populations - undersampled populations will have a lower prob, thus if the model returns a low probability observation, we do not penalise it.\n",
    "    - Model employs a set of prior sampling-weights which are the reicprocal of donors per cell tpye. This penalises celltypes which are observed in fewer donors. \n",
    "    - Observed probabilies that are provded by the SVM model \n",
    "    - Posterior probabilities represent the observed probability multiplied by the liklihood of observing the distribution given a weighted prior\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import pymc3 as pm\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import hypergeom\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from numpy import inf\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian ridge regression\n",
    "# Probabillistic SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-smell",
   "metadata": {},
   "source": [
    "# prototype code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-villa",
   "metadata": {},
   "source": [
    "Instead, it uses a probabilistic SVM classifier to estimate the probability that each single-cell belongs to a particular cell type. The SVM classifier is trained on the single-cell data, where each cell type is treated as a separate binary classification problem using a one-vs-rest approach. The resulting probability matrix is then used as input to a Bayesian deconvolution model, which assumes that each bulk sample is a linear combination of the gene expression profiles of the individual cell types, weighted by their relative contributions. The Bayesian model estimates the cell type proportions and gene expression profiles that best explain the observed bulk RNA sequencing data, using prior distributions and a likelihood function. The posterior distribution of the model parameters is then sampled using MCMC methods to obtain a set of posterior samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-landscape",
   "metadata": {},
   "source": [
    "#### method summary\n",
    "\n",
    "Load single-cell sequencing data: The first step is to load the single-cell sequencing data using Scanpy. Let $X$ be the gene expression matrix for the single-cell data, where each row corresponds to a gene and each column corresponds to a single cell.\n",
    "\n",
    "Extract gene expression profiles: Next, extract the gene expression profiles for each cell by converting the Scanpy object to a dense matrix. Let $x_i$ denote the gene expression profile for cell $i$, where $x_i$ is a column vector of length $n$, the number of genes.\n",
    "\n",
    "Load and normalize bulk RNA sequencing data: Load the bulk RNA sequencing data and normalize it to obtain a matrix $Y$ of shape $m \\times n$, where $m$ is the number of samples (e.g., tissues) and $n$ is the number of genes. Let $y_j$ denote the gene expression profile for sample $j$, where $y_j$ is a row vector of length $n$. Normalize $Y$ by dividing each row by its sum, so that the rows sum to 1 and represent proportions.\n",
    "\n",
    "Train probabilistic SVM classifier: Train a probabilistic support vector machine (SVM) classifier on the single-cell data, using the cell types as the target variable. Let $C$ be the set of cell types, and let $y_i$ denote the cell type for cell $i$. We use a one-vs-rest approach, where each cell type is treated as a separate binary classification problem. Let $p_i(c)$ denote the probability that cell $i$ belongs to cell type $c$, as estimated by the SVM classifier. We obtain a probability matrix $P$ of shape $k \\times m$, where $k$ is the number of cell types and $m$ is the number of bulk samples, by applying the SVM classifier to the bulk data.\n",
    "\n",
    "### RBF kernel trick\n",
    "Non-linearity: The RBF kernel allows SVM to handle non-linear relationships between input features and class labels. It can capture complex patterns and decision boundaries that linear models cannot. This flexibility makes it well-suited for datasets where the relationship between features and classes is non-linear.\n",
    "\n",
    "Implicit feature mapping: The RBF kernel implicitly maps the input features into a higher-dimensional feature space, where non-linear relationships may become linear. This mapping allows SVM to effectively separate classes that are not linearly separable in the original feature space.\n",
    "\n",
    "Flexibility in hyperparameter tuning: The RBF kernel has an additional hyperparameter called gamma, which determines the kernel's influence range. A smaller gamma value leads to a wider influence range and smoother decision boundaries, while a larger gamma value results in narrower influence range and more complex decision boundaries. This flexibility allows fine-tuning of the SVM model to capture the desired level of complexity in the data.\n",
    "\n",
    "### Bayesian liklihood\n",
    "Specify prior distributions and likelihood function: Specify a Bayesian model for deconvolving the bulk RNA sequencing data. We assume that each bulk sample is a linear combination of the gene expression profiles of the individual cell types, weighted by their relative contributions. Let $a_c$ be the prior probability that cell type $c$ is present in the bulk sample. We assume a Dirichlet prior for $a$, with concentration parameter $\\alpha$ set to 1 for a non-informative prior. Let $b_{c,j}$ be the gene expression level for gene $j$ in cell type $c$. We assume a normal prior for $b_{c,j}$, with mean 0 and variance 1 for a non-informative prior. We then model the bulk RNA sequencing data as a multivariate normal distribution, with mean vector $\\mu = \\sum_{c=1}^k a_c b_c$ and covariance matrix $\\Sigma = I$, where $b_c$ is the column vector of gene expression levels for cell type $c$.\n",
    "\n",
    "Sample from posterior distribution: Use Markov chain Monte Carlo (MCMC) methods to sample from the posterior distribution of the model parameters, given the data. We use the PyMC3 library to perform the MCMC sampling. Let $\\theta = (a,b)$ denote the set of model parameters. We obtain a set of $N$ posterior samples ${\\theta_i}_{i=1}^N$.\n",
    "\n",
    "Assess convergence and quality of samples: Check the convergence and quality of the posterior samples using diagnostic plots, such as trace plots and autocorrelation plots. This step ensures that the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-bachelor",
   "metadata": {},
   "source": [
    "# Define training data\n",
    "- Let's use the non-linear VAE integration in the first instance\n",
    "- data is capped at 15,000 features based on dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"ldvae_integrated\": \"/nfs/team205/ig7/projects/fetal_skin/160523_probabillistic_projection_organoid_adt_fetl/data/A1_V1_linear_ldvae_scvi_ski_updated_org_adt_build_donor_source_corrected_160523_raw_concatenated_ldvae_xscvi_features.h5ad\",\n",
    "    \"VAE_integrated\": \"/nfs/team205/ig7/projects/fetal_skin/160523_probabillistic_projection_organoid_adt_fetl/data/A1_V1_ldvae_scvi_ski_updated_org_adt_build_donor_source_corrected_160523_raw_concatenated_xscvi_features.h5ad\",\n",
    "    \"full_feature_space\": \"/nfs/team205/ig7/projects/fetal_skin/160523_probabillistic_projection_organoid_adt_fetl/data/A1_V1_ldvae_scvi_ski_updated_org_adt_build_donor_source_corrected_160523_raw_concatenated_full_features.h5ad\",\n",
    "}\n",
    "adata_int = sc.read(datasets[\"VAE_integrated\"], backed=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-dover",
   "metadata": {},
   "source": [
    "# Train on the Fskin compartment of the integrated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_select_col = \"dataset_merge\"\n",
    "data_select = \"-organoid\"\n",
    "train_indexer = adata_int.obs[adata_int.obs.index.str.contains(data_select)].index\n",
    "adata = adata_int[adata_int.obs[data_select_col].isin([data_select])].to_memory()\n",
    "# adata = adata_int[adata_int.obs.index.isin(train_indexer)].to_memory()\n",
    "sc_data = adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-region",
   "metadata": {},
   "source": [
    "# Check for timepoints where <3 of specific celltypes are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"age_annot\"] = (\n",
    "    adata.obs[\"pcw\"].astype(str) + \"_\" + adata.obs[\"annot\"].astype(str)\n",
    ")\n",
    "ct = adata.obs.groupby([\"age_annot\"]).apply(len)\n",
    "print(ct[ct < 3])\n",
    "ct = list(ct[ct < 3].index)\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_var = \"age_annot\"\n",
    "train_x_partition = \"X_scvi\"\n",
    "bless_partition = \"X_scvi\"\n",
    "use_nystroem = False\n",
    "apply_heuristic_vars = [\n",
    "    \"annot\",\n",
    "    \"pcw\",\n",
    "]  # this list of 2 variables defines a category to split the data by and a category to ensure good distributional representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.groupby([train_var]).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data.obs[\"pcw\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sc.pp.filter_cells(sc_data, min_genes=200)\n",
    "# # sc.pp.filter_genes(sc_data, min_cells=3)\n",
    "# sc.pp.normalize_per_cell(sc_data, counts_per_cell_after=1e4)\n",
    "# sc.pp.log1p(sc_data)\n",
    "# sc.pp.highly_variable_genes(sc_data, min_mean=0.1, max_mean=4)\n",
    "# # adata_mac = nameadata_mac adata_mac.var['highly_variable']]\n",
    "# sc.pp.scale(sc_data, max_value=10)\n",
    "# #%% PCA\n",
    "# sc.tl.pca(sc_data, n_comps=50,use_highly_variable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data.obs[\"annot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-pricing",
   "metadata": {},
   "source": [
    "# Apply leverage score sampling\n",
    "### Special heuristic rules::\n",
    "- We sample to a minimum number of vertices (0.1,0.05)\n",
    "- We apply sampling per celltype in this instance to try and learn a time encoding\n",
    "- Distribution of celltypes by time must pass chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Bless\n",
    "import bless as bless\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from numpy import arange\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "random_state = 24\n",
    "qbar = 2\n",
    "max_qbar = 10\n",
    "lam_final = 2\n",
    "min_prop = 0.01\n",
    "min_cells_per_var = 3\n",
    "\n",
    "if apply_heuristic_vars == False:\n",
    "    print(\"qbar param: {} , lambda param: {}\".format(qbar, lam_final))\n",
    "    # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "    r = np.random.RandomState(random_state)\n",
    "    if bless_partition in sc_data.obsm.keys():\n",
    "        tune_train_x = sc_data.obsm[bless_partition][:]\n",
    "        lvg = bless.bless(\n",
    "            tune_train_x,\n",
    "            RBF(length_scale=20),\n",
    "            lam_final=lam_final,\n",
    "            qbar=qbar,\n",
    "            random_state=r,\n",
    "            H=10,\n",
    "            force_cpu=True,\n",
    "        )\n",
    "        adata_tuning = sc_data[lvg.idx]\n",
    "        tune_train_x = adata_tuning.obsm[bless_partition][:]\n",
    "        print(\"Sketched data is {} long\".format(len(adata_tuning.obs)))\n",
    "\n",
    "    else:\n",
    "        tune_train_x = sc_data.X\n",
    "        lvg = bless.bless(\n",
    "            tune_train_x,\n",
    "            RBF(length_scale=20),\n",
    "            lam_final=lam_final,\n",
    "            qbar=qbar,\n",
    "            random_state=r,\n",
    "            H=10,\n",
    "            force_cpu=True,\n",
    "        )\n",
    "        adata_tuning = sc_data[lvg.idx]\n",
    "        tune_train_x = adata_tuning.obsm[sketch_obsm][:]\n",
    "        print(\"Sketched data is {} long\".format(len(adata_tuning.obs)))\n",
    "\n",
    "    while (\n",
    "        (\n",
    "            len(\n",
    "                list(\n",
    "                    set(list(adata_tuning.obs[train_var].unique()))\n",
    "                    ^ set(list(sc_data.obs[train_var].unique()))\n",
    "                )\n",
    "            )\n",
    "            > 0\n",
    "        )\n",
    "        or (min(adata_tuning.obs.groupby([train_var]).apply(len)) < 5)\n",
    "        or (qbar == max_qbar)\n",
    "        or (len(lvg.idx) / len(sc_data.obs) < min_prop)\n",
    "    ):\n",
    "        #     lam_final = lam_final + 1\n",
    "        qbar = qbar + 1\n",
    "        print(\"qbar param: {} , lambda param: {}\".format(qbar, lam_final))\n",
    "        # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "        r = np.random.RandomState(random_state)\n",
    "        if bless_partition in sc_data.obsm.keys():\n",
    "            tune_train_x = sc_data.obsm[bless_partition][:]\n",
    "            lvg = bless.bless(\n",
    "                tune_train_x,\n",
    "                RBF(length_scale=20),\n",
    "                lam_final=lam_final,\n",
    "                qbar=qbar,\n",
    "                random_state=r,\n",
    "                H=10,\n",
    "                force_cpu=True,\n",
    "            )\n",
    "            adata_tuning = sc_data[lvg.idx]\n",
    "            tune_train_x = adata_tuning.obsm[bless_partition][:]\n",
    "            print(\"Sketched data is {} long\".format(len(adata_tuning.obs)))\n",
    "        else:\n",
    "            tune_train_x = sc_data.X\n",
    "            lvg = bless.bless(\n",
    "                tune_train_x,\n",
    "                RBF(length_scale=20),\n",
    "                lam_final=lam_final,\n",
    "                qbar=qbar,\n",
    "                random_state=r,\n",
    "                H=10,\n",
    "                force_cpu=True,\n",
    "            )\n",
    "            adata_tuning = sc_data[lvg.idx]\n",
    "            tune_train_x = adata_tuning.obsm[sketch_obsm][:]\n",
    "            print(\"Sketched data is {} long\".format(len(adata_tuning.obs)))\n",
    "\n",
    "\n",
    "else:\n",
    "    heuristic_param_1 = apply_heuristic_vars[0]\n",
    "    heuristic_param_2 = apply_heuristic_vars[1]\n",
    "    tuned_indx = {}\n",
    "\n",
    "    for class_type in list(sc_data.obs[heuristic_param_1].unique()):\n",
    "        print(class_type)\n",
    "        sc_data_tmp = sc_data[sc_data.obs[heuristic_param_1].isin([class_type])]\n",
    "        ct = sc_data_tmp.obs.groupby([\"age_annot\"]).apply(len)\n",
    "        ct = list(ct[ct < min_cells_per_var].index)\n",
    "        if len(ct) > 0:\n",
    "            print(\n",
    "                \"There are cells which are not significantly present at all stages of heuristic var 2 {}\".format(\n",
    "                    str(ct)\n",
    "                )\n",
    "            )\n",
    "            print(\"removing these cells from training\")\n",
    "            sc_data_tmp = sc_data_tmp[~sc_data_tmp.obs[\"age_annot\"].isin(ct)]\n",
    "\n",
    "        print(\"qbar param: {} , lambda param: {}\".format(qbar, lam_final))\n",
    "        # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "        r = np.random.RandomState(random_state)\n",
    "        if bless_partition in sc_data_tmp.obsm.keys():\n",
    "            print(\"Leverage score from: {}\".format(bless_partition))\n",
    "            tune_train_x = sc_data_tmp.obsm[bless_partition][:]\n",
    "            lvg = bless.bless(\n",
    "                tune_train_x,\n",
    "                RBF(length_scale=10),\n",
    "                lam_final=lam_final,\n",
    "                qbar=qbar,\n",
    "                random_state=r,\n",
    "                H=5,\n",
    "                force_cpu=True,\n",
    "            )\n",
    "\n",
    "            adata_tuning = sc_data_tmp[lvg.idx]\n",
    "            tuned_indx[class_type] = adata_tuning.obs.index\n",
    "            # tune_train_x = adata_tuning.obsm[bless_partition][:]\n",
    "            print(\"Sketched data is {} long\".format(len(adata_tuning.obs)))\n",
    "\n",
    "        else:\n",
    "            tune_train_x = sc_data_tmp.X\n",
    "            lvg = bless.bless(\n",
    "                tune_train_x,\n",
    "                RBF(length_scale=10),\n",
    "                lam_final=lam_final,\n",
    "                qbar=qbar,\n",
    "                random_state=r,\n",
    "                H=10,\n",
    "                force_cpu=True,\n",
    "            )\n",
    "            adata_tuning = sc_data_tmp[lvg.idx]\n",
    "            tuned_indx[class_type] = adata_tuning.obs.index\n",
    "            # tune_train_x = adata_tuning.obsm[sketch_obsm][:]\n",
    "            print(\"Sketched data is {} long\".format(len(adata_tuning.obs)))\n",
    "\n",
    "        # rules\n",
    "        # Sample must contain all heuristic param 2\n",
    "        # Sample must have >3 of the min group\n",
    "        # Sample must have > min_prop\n",
    "        # or we reach max sampling\n",
    "        while (\n",
    "            (\n",
    "                len(\n",
    "                    list(\n",
    "                        set(list(adata_tuning.obs[heuristic_param_2].unique()))\n",
    "                        ^ set(list(sc_data_tmp.obs[heuristic_param_2].unique()))\n",
    "                    )\n",
    "                )\n",
    "                > 0\n",
    "            )\n",
    "            or (\n",
    "                min(adata_tuning.obs.groupby([heuristic_param_2]).apply(len))\n",
    "                < min_cells_per_var\n",
    "            )\n",
    "            or (qbar == max_qbar)\n",
    "            or (len(lvg.idx) / len(sc_data_tmp.obs) < min_prop)\n",
    "        ):\n",
    "            #     lam_final = lam_final + 1\n",
    "            if (\n",
    "                len(\n",
    "                    list(\n",
    "                        set(list(adata_tuning.obs[heuristic_param_2].unique()))\n",
    "                        ^ set(list(sc_data_tmp.obs[heuristic_param_2].unique()))\n",
    "                    )\n",
    "                )\n",
    "                > 0\n",
    "            ):\n",
    "                print(\"missing heuristic 2 variables!\")\n",
    "            if min(adata_tuning.obs.groupby([heuristic_param_2]).apply(len)) < 3:\n",
    "                print(\"One heuristic 2 variable is numbered < 3!\")\n",
    "            if len(lvg.idx) / len(sc_data_tmp.obs) < min_prop:\n",
    "                print(\n",
    "                    \"Sampled population is < than selected min_prop var {}\".format(\n",
    "                        min_prop\n",
    "                    )\n",
    "                )\n",
    "            qbar = qbar + 1\n",
    "            lam_final = lam_final + 1\n",
    "            print(\"qbar param: {} , lambda param: {}\".format(qbar, lam_final))\n",
    "            # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "            r = np.random.RandomState(random_state)\n",
    "            if bless_partition in sc_data_tmp.obsm.keys():\n",
    "                tune_train_x = sc_data_tmp.obsm[bless_partition][:]\n",
    "                lvg = bless.bless(\n",
    "                    tune_train_x,\n",
    "                    RBF(length_scale=10),\n",
    "                    lam_final=lam_final,\n",
    "                    qbar=qbar,\n",
    "                    random_state=r,\n",
    "                    H=5,\n",
    "                    force_cpu=True,\n",
    "                )\n",
    "                adata_tuning = sc_data_tmp[lvg.idx]\n",
    "                tuned_indx[class_type] = adata_tuning.obs.index\n",
    "                # tune_train_x = adata_tuning.obsm[bless_partition][:]\n",
    "                print(\"Sketched data is {} long\".format(len(adata_tuning.obs)))\n",
    "            else:\n",
    "                tune_train_x = sc_data_tmp.X\n",
    "                lvg = bless.bless(\n",
    "                    tune_train_x,\n",
    "                    RBF(length_scale=10),\n",
    "                    lam_final=lam_final,\n",
    "                    qbar=qbar,\n",
    "                    random_state=r,\n",
    "                    H=5,\n",
    "                    force_cpu=True,\n",
    "                )\n",
    "                adata_tuning = sc_data_tmp[lvg.idx]\n",
    "                tuned_indx[class_type] = adata_tuning.obs.index\n",
    "                # tune_train_x = adata_tuning.obsm[sketch_obsm][:]\n",
    "                print(\"Sketched data is {} long\".format(len(adata_tuning.obs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cat = list(\n",
    "    set(list(sc_data.obs[\"annot\"].unique())) ^ set(list(tuned_indx.keys()))\n",
    ")\n",
    "if len(missing_cat) > 0:\n",
    "    print(\"Missing categories! {}\".format(str(missing_cat)))\n",
    "sampled_indx = list(set(chain(*tuned_indx.values())))\n",
    "adata_tuning = sc_data[sc_data.obs.index.isin(sampled_indx)]\n",
    "adata_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load single-cell sequencing data using Scanpy\n",
    "# sc_data = sc.read('/nfs/team205/ig7/projects/SCC_nano_string/khavari.h5ad')\n",
    "# sc_data = sc_data[sc_data.obs['patient'].isin(['P1'])]\n",
    "# Step 2: Extract gene expression profiles for each cell\n",
    "sc_data = adata_tuning.copy()\n",
    "sc_expr = sc_data.X.todense()\n",
    "sc_expr = np.array(sc_expr)\n",
    "sc_labels = sc_data.obs[train_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_labels.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-reunion",
   "metadata": {},
   "source": [
    "# Train probabilistic SVM classifier with nystroeam approximation\n",
    "- Bayesian optimisation\n",
    "\n",
    "\n",
    "#### Nystroem approximation transformer\n",
    "- By using the Nystroem approximation transformer, the SVM model can effectively capture the non-linear relationships in the gene expression data by projecting it into a higher-dimensional feature space. This enables the SVM to learn complex decision boundaries and improve the classification performance compared to using a linear kernel.\n",
    "\n",
    "- Nystroem approximation transformer is a feature mapping technique that approximates the RBF (Radial Basis Function) kernel matrix for the gene expression data. It transforms the original high-dimensional gene expression data into a lower-dimensional feature space, while still capturing the non-linear relationships present in the data.\n",
    "\n",
    "- The Nystroem approximation is based on the concept of random Fourier features, where it approximates the kernel matrix by computing a subset of randomly selected samples (landmark points) from the input data. These landmark points are then used to construct an approximate feature representation of the data.\n",
    "\n",
    "- Landmark Selection:\n",
    "    Randomly select a subset of samples (landmarks) from the original data. The number of landmarks is determined by the n_components parameter.\n",
    "    The landmarks can be chosen uniformly at random or using other sampling strategies like k-means clustering.\n",
    "    \n",
    "- Feature Transformation:\n",
    "    Compute the RBF kernel matrix between the original data and the selected landmarks.\n",
    "    Perform eigenvalue decomposition on the kernel matrix to obtain the eigenvectors and eigenvalues.\n",
    "    Use the eigenvectors corresponding to the largest eigenvalues as the transformed features.\n",
    "    Scale the transformed features by the square root of the corresponding eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-entrance",
   "metadata": {},
   "source": [
    "#### Feature importance\n",
    "One common method for measuring feature importance in SVM models is to use the absolute value of the weights assigned to each feature in the SVM model. The higher the weight, the more important the feature is in predicting the outcome.\n",
    "\n",
    "In scikit-learn, the coef_ attribute of the SVC class can be used to obtain the coefficients of the SVM model. The coefficients are stored in an array of shape (n_classes, n_features) for multi-class classification problems or (1, n_features) for binary classification problems.\n",
    "\n",
    "Here's an example code snippet that demonstrates how to obtain and sort the feature weights from an SVM model:\n",
    "\n",
    "#### get feature weights from SVM model\n",
    "weights = np.abs(clf.coef_[0])\n",
    "\n",
    "#### sort features by weight\n",
    "sorted_indices = np.argsort(weights)[::-1]\n",
    "\n",
    "#### print feature weights in descending order\n",
    "for i in sorted_indices:\n",
    "    print(f'Feature {i}: {weights[i]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_nystroem = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train probabilistic SVM classifier\n",
    "# The fit time scales at least quadruple with the number of samples and may be impractical beyond tens of thousands of samples.\n",
    "\n",
    "# Hyperparameter optimization using Bayesian optimization\n",
    "param_space = {\"C\": (0.1, 10.0, \"log-uniform\"), \"gamma\": (0.01, 10.0, \"log-uniform\")}\n",
    "\n",
    "if use_nystroem == True:\n",
    "    print(\n",
    "        \"Creating a approximate rbf kernel representation using the Nystroem transformer\"\n",
    "    )\n",
    "    # Create Nystroem approximation transformer\n",
    "    nystroem = Nystroem(\n",
    "        kernel=\"rbf\", n_components=1000\n",
    "    )  # Adjust the number of components as needed\n",
    "    # Transform the data using Nystroem approximation\n",
    "    nystroem_features = nystroem.fit_transform(sc_expr)\n",
    "\n",
    "    print(\n",
    "        \"Bayesian optimisation for regularisation and gamma parameters of the rbf kernel\"\n",
    "    )\n",
    "    svm = SVC(\n",
    "        kernel=\"rbf\", probability=True\n",
    "    )  # RBF kernel uses kernal trick to project data into an infinite dimensional space and finds the best hyperplane per class\n",
    "    opt_model = BayesSearchCV(\n",
    "        svm, param_space, n_iter=30, cv=3, scoring=\"f1_weighted\", n_jobs=-1\n",
    "    )\n",
    "    opt_model.fit(nystroem_features, sc_labels)\n",
    "    # Best hyperparameters\n",
    "    best_params = opt_model.best_params_\n",
    "    print(\"Fitting SVM model\")\n",
    "    # Train SVM classifier with RBF kernel using the best hyperparameters\n",
    "    svm = SVC(kernel=\"rbf\", probability=True, **best_params)\n",
    "    svm.fit(nystroem_features, sc_labels)\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"Warning! Proceeding without Nystroem transformer, increased samples here can quadruple the compute time!\"\n",
    "    )\n",
    "    print(\n",
    "        \"Bayesian optimisation for regularisation and gamma parameters of the rbf kernel\"\n",
    "    )\n",
    "    svm = SVC(\n",
    "        kernel=\"rbf\", probability=True\n",
    "    )  # RBF kernel uses kernal trick to project data into an infinite dimensional space and finds the best hyperplane per class\n",
    "    opt_model = BayesSearchCV(\n",
    "        svm, param_space, n_iter=10, cv=3, scoring=\"f1_weighted\", n_jobs=-1\n",
    "    )\n",
    "    opt_model.fit(sc_expr, sc_labels)\n",
    "    # Best hyperparameters\n",
    "    best_params = opt_model.best_params_\n",
    "    print(\"Fitting SVM model\")\n",
    "    # Train SVM classifier with RBF kernel using the best hyperparameters\n",
    "    svm = SVC(kernel=\"rbf\", probability=True, **best_params)\n",
    "    svm.fit(sc_expr, sc_labels)\n",
    "\n",
    "# Plot Bayesian optimization iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    range(len(opt_model.cv_results_[\"mean_test_score\"])),\n",
    "    opt_model.cv_results_[\"mean_test_score\"],\n",
    ")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean F1_weighted Test Score\")\n",
    "plt.title(\"Bayesian Optimization Iterations\")\n",
    "# Find the best iteration\n",
    "best_iteration = np.argmax(opt_model.cv_results_[\"mean_test_score\"])\n",
    "best_score = opt_model.cv_results_[\"mean_test_score\"][best_iteration]\n",
    "# Mark the best iteration\n",
    "plt.scatter(\n",
    "    best_iteration,\n",
    "    best_score,\n",
    "    marker=\"o\",\n",
    "    color=\"red\",\n",
    "    label=\"Best Iteration C:{} gamma:{}\".format(best_params[\"C\"], best_params[\"gamma\"]),\n",
    ")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(\n",
    "    sc_labels, svm.predict(nystroem.transform(sc_expr)), output_dict=True\n",
    ")\n",
    "report = pd.DataFrame(report).T\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out = pd.DataFrame(\n",
    "    svm.predict_proba(nystroem.transform(sc_expr)),\n",
    "    index=sc_data.obs.index,\n",
    "    columns=svm.classes_,\n",
    ")\n",
    "pred_out[\"predicted\"] = svm.predict(nystroem.transform(sc_expr))\n",
    "pred_out[\"orig_labels\"] = sc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_probs = (\n",
    "    pred_out.loc[:, pred_out.columns != \"predicted\"].groupby(\"orig_labels\").median()\n",
    ")\n",
    "model_mean_probs = model_mean_probs * 100\n",
    "# model_mean_probs = model_mean_probs.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "crs_tbl = model_mean_probs.copy()\n",
    "# Sort df columns by rows\n",
    "index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "crs_tbl = crs_tbl.loc[index_order]\n",
    "crs_tbl = crs_tbl[col_order]\n",
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.set(font_scale=0.5)\n",
    "g = sns.heatmap(\n",
    "    crs_tbl,\n",
    "    cmap=\"viridis_r\",\n",
    "    annot=False,\n",
    "    vmin=0,\n",
    "    vmax=np.max(np.max(crs_tbl)),\n",
    "    linewidths=1,\n",
    "    center=np.max(np.max(crs_tbl)) / 2,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Original labels\")\n",
    "plt.xlabel(\"SVM Classification\")\n",
    "# plt.savefig('./ldvae_ver5_lr_model_means_subclusters.pdf',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the SVM model to a file\n",
    "model_file = \"svm_organoid_sk_time_nystroem_X.pkl\"\n",
    "with open(model_file, \"wb\") as file:\n",
    "    pickle.dump(svm, file)\n",
    "\n",
    "# nystroeam_file = 'nystroeam_file_X.pkl'\n",
    "# with open(nystroeam_file, 'wb') as file:\n",
    "#     pickle.dump(nystroem, file)\n",
    "#     # Load the SVM model from the file\n",
    "# with open(model_file, 'rb') as file:\n",
    "#     loaded_svm = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-moral",
   "metadata": {},
   "source": [
    "# Convert Nystroem features back into gene expression features using the feature lookup matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance\n",
    "perm_importance = permutation_importance(\n",
    "    svm,\n",
    "    nystroem_features,\n",
    "    sc_labels,\n",
    "    scoring=\"f1_weighted\",\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Get feature importances for each class\n",
    "feature_importances = perm_importance.importances\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "df_feature_importance = pd.DataFrame(columns=[\"Class\", \"Feature\", \"Importance\"])\n",
    "\n",
    "# Iterate over each class\n",
    "for class_label, feature_scores in enumerate(feature_importances):\n",
    "    # Get the indices of the most important features for the current class\n",
    "    top_features_indices = feature_scores.argsort()[::-1]\n",
    "\n",
    "    # Iterate over the top features\n",
    "    for feature_idx in top_features_indices:\n",
    "        importance = feature_scores[feature_idx]\n",
    "        feature_name = f\"Feature {feature_idx}\"\n",
    "\n",
    "        # Add the feature information to the DataFrame\n",
    "        df_feature_importance = df_feature_importance.append(\n",
    "            {\"Class\": class_label, \"Feature\": feature_name, \"Importance\": importance},\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "# Print the DataFrame with the most important features for each class\n",
    "print(df_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-overall",
   "metadata": {},
   "source": [
    "# Deconvulution loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "consolidated-repeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/knn_bayes/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:524: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[key_n_counts] = counts_per_cell\n",
      "/home/jovyan/my-conda-envs/knn_bayes/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:352: RuntimeWarning: invalid value encountered in log1p\n",
      "  np.log1p(X, out=X)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load and normalize bulk RNA sequencing data\n",
    "bulk_data = sc.read(\n",
    "    \"/nfs/team205/ig7/projects/SCC_nano_string/010523_project_restart/nanostring_geom_normalised_counts_object_040523.h5ad\"\n",
    ")\n",
    "bulk_data = bulk_data[:, list(sc_data.var.index)]\n",
    "bulk_expr = bulk_data.X\n",
    "bulk_data.X = np.nan_to_num(bulk_data.X, nan=0, posinf=0)\n",
    "sc.pp.normalize_per_cell(bulk_data, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(bulk_data)\n",
    "# sc.pp.highly_variable_genes(bulk_data, min_mean=0.1, max_mean=4)\n",
    "# adata_mac = nameadata_mac adata_mac.var['highly_variable']]\n",
    "sc.pp.scale(bulk_data, max_value=10)\n",
    "bulk_data.X = np.nan_to_num(bulk_data.X, nan=0, posinf=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-composer",
   "metadata": {},
   "source": [
    "# Format nanostring data for bayesian hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "auburn-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_data.obs[\"risk_stat\"] = bulk_data.obs[\"HR_strat\"].copy()\n",
    "bulk_data.obs[\"tum.norm\"] = bulk_data.obs[\"shortname\"].copy()\n",
    "bulk_stat = [\n",
    "    \"Bow AK\",\n",
    "    \"HR*\",\n",
    "    \"HR\",\n",
    "    \"KA\",\n",
    "    \"LOCAL RECURRANCE\",\n",
    "    \"LR\",\n",
    "    \"MET\",\n",
    "    \"NEG LYMPH\",\n",
    "    \"NON UV\",\n",
    "    \"PERI\",\n",
    "]\n",
    "map_stat = [\n",
    "    \"remove\",\n",
    "    \"high\",\n",
    "    \"high\",\n",
    "    \"remove\",\n",
    "    \"LOCAL RECURRANCE\",\n",
    "    \"low\",\n",
    "    \"high\",\n",
    "    \"remove\",\n",
    "    \"low\",\n",
    "    \"low\",\n",
    "]\n",
    "bulk_site = [\"Disease\", \"Control\", \"peri\"]\n",
    "map_site = [\"Tumor\", \"Normal\", \"Normal\"]\n",
    "status_dic = dict(zip(bulk_stat, map_stat))\n",
    "site_dic = dict(zip(bulk_site, map_site))\n",
    "bulk_data.obs[\"risk_stat\"] = bulk_data.obs[\"risk_stat\"].map(status_dic)\n",
    "bulk_data.obs[\"tum.norm\"] = bulk_data.obs[\"tum.norm\"].map(site_dic)\n",
    "\n",
    "bulk_data.obs[batch_id] = \"patient\"\n",
    "bulk_data = bulk_data[~bulk_data.obs[\"risk_stat\"].isin([\"remove\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "former-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_probs = pd.DataFrame()\n",
    "for index in bulk_data.obs.index:\n",
    "    bulk_expr_tmp = bulk_data[bulk_data.obs.index.isin([index])].X\n",
    "    bulk_probs_tmp = pd.DataFrame(svm.predict_proba(nystroem.transform(bulk_expr_tmp)))\n",
    "    bulk_probs_tmp.index = [index]\n",
    "    bulk_probs_tmp.columns = svm.classes_\n",
    "    bulk_probs = pd.concat([bulk_probs, bulk_probs_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "atlantic-remedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 57 × 1365\n",
       "    obs: 'Pathway_Name', 'Immune_Name', 'shortname', 'stage', 'tissue', 'additional', 'samplecode', 'patient', 'HR_strat', 'immunotype', 'immunocompromised', 'Sample_type', 'n_counts', 'risk_stat', 'tum.norm'\n",
       "    var: 'Genes', 'Immune_panel', 'Pathway_panel', 'non-specific_genes', 'In_processed_object', 'Immune_HR', 'Immune_HR2', 'Immune_MET', 'Immune_HR3', 'Immune_HR4', 'Immune_HR5', 'Immune_MET1', 'Immune_NEG LYMPH', 'Immune_HR6', 'Immune_MET2', 'Immune_LR', 'Immune_NEG LYMPH1', 'Immune_MET3', 'Immune_NEG LYMPH2', 'Immune_Local recurrance 2', 'Immune_MET4', 'Immune_NEG LYMPH3', 'Immune_HR7', 'Immune_MET5', 'Immune_NEG LYMPH4', 'Immune_HR8', 'Immune_HR9', 'Immune_MET6', 'Immune_MET7', 'Immune_HR10', 'Immune_MET8', 'Immune_HR11', 'Immune_MET9', 'Immune_NEG LYMPH5', 'Immune_HR12', 'Immune_LR1', 'Immune_LR2', 'Immune_LR3', 'Immune_LR4', 'Immune_LR5', 'Immune_HR13', 'Immune_PERI', 'Immune_HR14', 'Immune_PERI1', 'Immune_KA', 'Immune_LR6', 'Immune_PERI2', 'Immune_LR7', 'Immune_NON UV', 'Immune_NON UV1', 'Immune_HR15', 'Immune_LR12', 'Immune_HR17', 'Immune_HR18', 'Immune_HR19', 'Immune_MET10', 'Immune_PERI3', 'Immune_LR8', 'Immune_LR9', 'Immune_HR20', 'Immune_HR21', 'Immune_PERI4', 'Immune_PERI5', 'Immune_Bow AK', 'Immune_HR22', 'Immune_HR23', 'Immune_LR10', 'Immune_LR11', 'Immune_NON UV2', 'Immune_Local recurrance 1', 'Pathway_HR', 'Pathway_HR2', 'Pathway_MET', 'Pathway_HR3', 'Pathway_HR5', 'Pathway_HR4', 'Pathway_MET1', 'Pathway_NEG LYMPH', 'Pathway_HR6', 'Pathway_MET2', 'Pathway_LR', 'Pathway_MET6', 'Pathway_NEG LYMPH1', 'Pathway_MET3', 'Pathway_NEG LYMPH2', 'Pathway_Local recurrance 2', 'Pathway_MET4', 'Pathway_NEG LYMPH3', 'Pathway_HR7', 'Pathway_MET5', 'Pathway_NEG LYMPH4', 'Pathway_HR8', 'Pathway_HR9', 'Pathway_MET7', 'Pathway_HR10', 'Pathway_MET8', 'Pathway_HR11', 'Pathway_MET9', 'Pathway_NEG LYMPH5', 'Pathway_HR12', 'Pathway_LR1', 'Pathway_LR2', 'Pathway_LR3', 'Pathway_LR4', 'Pathway_LR5', 'Pathway_HR13', 'Pathway_PERI', 'Pathway_HR14', 'Pathway_PERI1', 'Pathway_KA', 'Pathway_LR6', 'Pathway_PERI2', 'Pathway_LR7', 'Pathway_NON UV', 'Pathway_NON UV1', 'Pathway_HR15', 'Pathway_LR12', 'Pathway_HR17', 'Pathway_HR18', 'Pathway_HR19', 'Pathway_MET10', 'Pathway_PERI3', 'Pathway_LR8', 'Pathway_LR9', 'Pathway_HR20', 'Pathway_HR21', 'Pathway_PERI4', 'Pathway_PERI5', 'Pathway_Bow AK', 'Pathway_HR22', 'Pathway_HR23', 'Pathway_LR10', 'Pathway_LR11', 'Pathway_NON UV2', 'Pathway_Local recurrance 1', 'mean', 'std'\n",
       "    uns: 'log1p'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bulk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "occasional-technical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HR                    0.160189\n",
       "HR10                  0.030329\n",
       "HR11                  1.000000\n",
       "HR12                  0.052385\n",
       "HR13                  0.052748\n",
       "HR14                  0.082181\n",
       "HR15                  0.030101\n",
       "LR12                  0.113069\n",
       "HR17                  0.063624\n",
       "HR18                  0.031314\n",
       "HR19                  0.040267\n",
       "HR2                   0.019209\n",
       "HR20                  0.035332\n",
       "HR21                  0.121674\n",
       "HR22                  0.450647\n",
       "HR23                  0.023007\n",
       "Local recurrance 1    0.064468\n",
       "HR3                   0.077955\n",
       "HR4                   0.042243\n",
       "HR5                   0.081418\n",
       "HR6                   0.203901\n",
       "HR7                   0.384844\n",
       "HR8                   0.232766\n",
       "HR9                   0.020267\n",
       "Local recurrance 2    0.055963\n",
       "LR                    0.067588\n",
       "LR1                   0.036681\n",
       "LR10                  0.058592\n",
       "LR11                  0.047342\n",
       "LR2                   0.131132\n",
       "LR3                   0.102267\n",
       "LR4                   0.023304\n",
       "LR5                   0.080649\n",
       "LR6                   0.033711\n",
       "LR7                   0.028088\n",
       "LR8                   0.164154\n",
       "LR9                   0.031255\n",
       "MET                   0.082347\n",
       "MET1                  0.035796\n",
       "MET10                 0.034367\n",
       "MET2                  0.090202\n",
       "MET3                  0.022367\n",
       "MET4                  0.002585\n",
       "MET5                  0.044885\n",
       "MET6                  0.102935\n",
       "MET7                  0.107833\n",
       "MET8                  0.160074\n",
       "MET9                  0.133823\n",
       "NON UV                0.043814\n",
       "NON UV1               0.050048\n",
       "NON UV2               0.000000\n",
       "PERI                  0.105400\n",
       "PERI1                 0.152011\n",
       "PERI2                 0.006371\n",
       "PERI3                 0.016868\n",
       "PERI4                 0.010729\n",
       "PERI5                 0.124788\n",
       "Name: TSK, dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bulk_probs[\"TSK\"] - (np.min(bulk_probs[\"TSK\"]))) / (\n",
    "    np.max(bulk_probs[\"TSK\"]) - np.min(bulk_probs[\"TSK\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "blond-terrace",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (4142736286.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [28]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def deconvolve_bulk(model,bulk_data,meta_var)\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "def deconvolve_bulk(model,bulk_data,meta_vars)\n",
    "    # This function takes an input of stacked bulk data in anndata format and returns a dataframe output containing predicted cell-state contribution probability\n",
    "    \n",
    "    # model = SVM model\n",
    "    # Bulk data = bulk data formated as anndata, assumes that each anndata.obs row contains a bulk sample\n",
    "    # meta_var = sample metadata that matches between datasets to help educate bayesian priors. e.g site and disease info\n",
    "    bulk_proba_mat = pd.DataFrame()\n",
    "    svm = model\n",
    "    for bulk_\n",
    "    \n",
    "    bulk_expr = bulk_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_data.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-saying",
   "metadata": {},
   "source": [
    "# Empirical bayesian framework probability refinement\n",
    "- split the training data by the meta_vars (disease/site info) and extract seperate distribution information for each celltstate\n",
    "- Empirical observations as we draw our priors from the sc data using a hypergeometric sampler which satisfies a chi-square test. \n",
    "\n",
    "#### Hypergeometric sampler\n",
    "- Since we cannot assume to have captured the same level of detail in the target modaility\n",
    "- We choose to sample the sc training data severaql times at x proportion to provide representative distributions corresponding to possible sets of cells which may be recovered in smaller runs\n",
    "\n",
    "\n",
    "#### Sampling weights\n",
    "- cell_type_distributions_per_donor is a numpy array or pandas DataFrame where each row represents a donor and each column represents a cell type. The values in the array indicate the presence or absence of each cell type in each donor.\n",
    "\n",
    "- The num_donors_per_cell_type variable counts the number of donors in which each cell type is observed by summing the occurrences greater than 0 along the donor axis.\n",
    "\n",
    "- The sampling_weights variable is then computed as the reciprocal of the number of donors per cell type. This assigns higher weights to cell types observed in fewer donors and lower weights to cell types observed in more donors.\n",
    "\n",
    "- Finally, the sampling_weights are normalized to ensure they sum up to 1, allowing them to be directly used as weights in the Bayesian inference.\n",
    "\n",
    "- You can then use the compute_sampling_weights function to compute the sampling weights and pass them to the compute_posterior_probabilities function as the sampling_weights argument.\n",
    "\n",
    "#### Priors\n",
    "- the prior distribution prior is defined using the aggregated cell type distributions obtained from multiple samples. The cell_type_distributions input should be a 2D array where each row represents the cell type distribution of a sample.\n",
    "\n",
    "- The observed probabilities are iterated, and for each observed probability, a likelihood is defined using the Multinomial distribution with the prior distribution as the parameter. Bayesian inference is performed using the Hamiltonian Monte Carlo (HMC) algorithm via the pm.sample function. The posterior probability is then computed as the mean of the trace for the prior distribution.\n",
    "\n",
    "- Note that this code assumes you have preprocessed the observed probabilities and cell type distributions appropriately before passing them to the compute_posterior_probabilities function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "separated-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0\n",
      "Chi-square statistic: 9743.76344235169 , p-value: 3.780426359559009e-18\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 1 iterations\n",
      "Current iteration: 1\n",
      "Chi-square statistic: 9895.22786877249 , p-value: 8.327922857105144e-19\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 2 iterations\n",
      "Current iteration: 2\n",
      "Chi-square statistic: 9620.045605559972 , p-value: 5.187923477346367e-15\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 3 iterations\n",
      "Current iteration: 3\n",
      "Chi-square statistic: 9057.946806818076 , p-value: 1.282628518578592e-13\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 4 iterations\n",
      "Current iteration: 4\n",
      "Chi-square statistic: 9244.523252151628 , p-value: 2.2150863218889413e-18\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 5 iterations\n",
      "Current iteration: 5\n",
      "Chi-square statistic: 9903.746596328805 , p-value: 1.3178969939952312e-26\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 6 iterations\n",
      "Current iteration: 6\n",
      "Chi-square statistic: 10015.470198271025 , p-value: 1.25233365139141e-18\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 7 iterations\n",
      "Current iteration: 7\n",
      "Chi-square statistic: 9354.855976367995 , p-value: 3.75862866934125e-07\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 8 iterations\n",
      "Current iteration: 8\n",
      "Chi-square statistic: 9287.23488013294 , p-value: 4.1873761293340753e-13\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 9 iterations\n",
      "Current iteration: 9\n",
      "Chi-square statistic: 9902.71762377501 , p-value: 9.412069756937883e-16\n",
      "Suceeded in drawing k= 10 hypergeometric samples using defined proportion = 0.2 in n_iter = 10 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:163: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  prior = (empirical_priors[key].pivot(batch_id,train_var,values = 'prop').fillna(0))#.droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:171: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  p_tmp =(p_tmp.pivot(batch_id,train_var).fillna(0)).droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:183: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  p_tmp =(p_tmp.pivot(batch_id,train_var).fillna(0)).droplevel(0,axis =1)\n",
      "/tmp/ipykernel_2210/918053593.py:189: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  p_tmp_pv =(p_tmp.pivot(batch_id,train_var).fillna(0)).droplevel(0,axis =1)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_vars = [\"risk_stat\", \"tum.norm\"]  # this could combine 'tum.norm' and sampling site\n",
    "batch_id = \"patient\"  # This could also be 'patient'\n",
    "train_var = train_var\n",
    "# define the maximum number of hypergeometric samples to use in the computation of empirical priors\n",
    "max_iter = 100\n",
    "# Define k_samples which represents preffered number of hypergeometric samples per batch\n",
    "k_samples = 10\n",
    "# Define the poportion to sample with each hypergeometric pass\n",
    "sampling_prop = 0.2\n",
    "\n",
    "# Define the var_combination which matches sets of meta_vars from the SC data to assign priors\n",
    "bulk_vars = [\"risk_stat\", \"tum.norm\"]\n",
    "\n",
    "# Define all unique cell-classes\n",
    "cell_states = sc_labels.unique()\n",
    "\n",
    "############ function block ############\n",
    "# Sampling empirical priors to be used in the bayesian framework\n",
    "# Piors are sampled per batch and tested against the combined representation via chi-square\n",
    "# Sampling is meant to represent the native sampling rate of a given phenotype representation\n",
    "\n",
    "\n",
    "def hypergeometric_sampling(obs, proportion):\n",
    "    \"\"\"\n",
    "    Perform hypergeometric sampling to select a defined proportion of cells from an AnnData object.\n",
    "\n",
    "    Parameters:\n",
    "        anndata_obj (AnnData): Annotated data object containing cell information.\n",
    "        proportion (float): Proportion of cells to sample, between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Selected indices.\n",
    "    \"\"\"\n",
    "    total_cells = obs.shape[0]\n",
    "    num_indices = int(proportion * total_cells)\n",
    "    indices = np.arange(total_cells)\n",
    "\n",
    "    # Calculate the probability of each index based on the total number of cells\n",
    "    probabilities = np.ones(total_cells) / total_cells\n",
    "\n",
    "    # Perform hypergeometric sampling to select indices\n",
    "    selected_indices = np.random.choice(\n",
    "        indices, size=num_indices, replace=False, p=probabilities\n",
    "    )\n",
    "\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "def draw_emprirical_bayes_priors(\n",
    "    bayesian_obs, cell_states, batch_id, train_var, max_iter, k_samples, sampling_prop\n",
    "):\n",
    "    # Initialise a dictionary to store the celltype distributions from each sampling set\n",
    "    sampled_distributions = {}\n",
    "    # initialise a counter for successful sampling and current iteration\n",
    "    successes = 0\n",
    "    iteration = 0\n",
    "    # Disable warnings\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # compute an original distribution across all cells\n",
    "    p_tmp = bayesian_obs.groupby([batch_id, train_var]).apply(len).reset_index()\n",
    "    # p_tmp =(p_tmp.pivot(batch_id,train_var).fillna(0)).reset_index().melt(id_vars=batch_id)\n",
    "    missing_classes = list(set(list(p_tmp[train_var].unique())) ^ set((cell_states)))\n",
    "    # add any missing cell-states back in\n",
    "    p_tmp_pv = (p_tmp.pivot(batch_id, train_var).fillna(0)).droplevel(0, axis=1)\n",
    "    p_tmp_pv[missing_classes] = 0\n",
    "    p_tmp = p_tmp_pv.reset_index().melt(id_vars=batch_id)\n",
    "    p_tmp[\"prop\"] = p_tmp[\"value\"] / p_tmp.groupby(batch_id)[\"value\"].transform(\"sum\")\n",
    "    while successes < k_samples and iteration < max_iter:\n",
    "        print(\"Current iteration: {}\".format(iteration))\n",
    "\n",
    "        # iteratively draw a proportion of cells from a hypergeometric distribution\n",
    "        tmp = pd.DataFrame()\n",
    "        for batch in bayesian_obs[batch_id].unique():\n",
    "            tmp = pd.concat(\n",
    "                [\n",
    "                    tmp,\n",
    "                    pd.DataFrame(\n",
    "                        bayesian_obs[bayesian_obs[batch_id].isin([batch])]\n",
    "                        .iloc[\n",
    "                            hypergeometric_sampling(\n",
    "                                (bayesian_obs[bayesian_obs[batch_id].isin([batch])]),\n",
    "                                sampling_prop,\n",
    "                            )\n",
    "                        ]\n",
    "                        .groupby([batch_id, train_var])\n",
    "                        .apply(len)\n",
    "                        .reset_index()\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        missing_classes = list(\n",
    "            set(list(tmp[train_var].unique())) ^ set(list(p_tmp[train_var].unique()))\n",
    "        )\n",
    "        # add any missing cell-states back in\n",
    "        tmp_pv = (tmp.pivot(batch_id, train_var).fillna(0)).droplevel(0, axis=1)\n",
    "        tmp_pv[missing_classes] = 0\n",
    "        tmp = tmp_pv.reset_index().melt(id_vars=batch_id)\n",
    "        tmp[\"prop\"] = tmp[\"value\"] / tmp.groupby(batch_id)[\"value\"].transform(\"sum\")\n",
    "        # This is one sampling loop, ensure that samples from this loop are representative of the main data\n",
    "        # Take the mean probability of succeesfully sampled data between batches,per loop and test with chi-square test\n",
    "\n",
    "        # Create a contingency table with the original and sampled distributions\n",
    "        observed = np.array([tmp[\"prop\"]])\n",
    "        expected = np.array([p_tmp[\"prop\"]])\n",
    "        # Create contingency table for original and sampled distributions\n",
    "        contingency_table = pd.crosstab(expected, observed)\n",
    "        # Add a small constant value to expected frequencies\n",
    "        contingency_table += 0.001\n",
    "\n",
    "        # Perform chi-square test\n",
    "        chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "        print(\"Chi-square statistic: {} , p-value: {}\".format(chi2, p_value))\n",
    "        # If the sampling loop is successful (p<0.05), we add it to the observed priors list\n",
    "        # We take the successfully sampled probabilities as input for priors\n",
    "        if p_value < 0.05:\n",
    "            successes = successes + 1\n",
    "            sampled_distributions[iteration] = tmp\n",
    "            # Print the chi-square statistic and p-value\n",
    "\n",
    "        iteration = iteration + 1\n",
    "        if iteration == max_iter:\n",
    "            print(\"max iteration hit, proceeding anyways\")\n",
    "        else:\n",
    "            print(\n",
    "                \"Suceeded in drawing k= {} hypergeometric samples using defined proportion = {} in n_iter = {} iterations\".format(\n",
    "                    k_samples, sampling_prop, iteration\n",
    "                )\n",
    "            )\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    return sampled_distributions\n",
    "\n",
    "\n",
    "def estimate_posterior_probabilities(priors, observations):\n",
    "    num_samples, num_cell_types = observations.shape\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Priors for cell type contributions\n",
    "        cell_type_contributions = pm.Dirichlet(\n",
    "            \"cell_type_contributions\", a=priors, shape=num_cell_types\n",
    "        )\n",
    "\n",
    "        # Likelihood of observations\n",
    "        for i in range(num_samples):\n",
    "            pm.Bernoulli(\n",
    "                \"likelihood_\" + str(i),\n",
    "                p=cell_type_contributions,\n",
    "                observed=observations[i],\n",
    "            )\n",
    "\n",
    "        # Sample from the posterior distribution\n",
    "        trace = pm.sample(draws=2000, tune=1000)\n",
    "        # Extract posterior probabilities\n",
    "        posteriors = pd.DataFrame(\n",
    "            pm.trace_to_dataframe(trace, varnames=[\"cell_type_contributions\"])\n",
    "        )\n",
    "    return posteriors\n",
    "\n",
    "\n",
    "# sampling_weights is computed as the reciprocal of the number of donors per cell type\n",
    "# Assigns higher weights to cell types observed in more donors and lower weights to cell types observed in fewer donors\n",
    "# The idea here is to find common celltypes across donors which are representative of the state=\n",
    "def compute_sampling_weights(cell_type_distributions_per_donor):\n",
    "    # In this code, cell_type_distributions_per_donor is a numpy array or pandas DataFrame where each row represents a donor and each column represents a cell type. The values in the array indicate the presence or absence of each cell type in each donor.\n",
    "    # The num_donors_per_cell_type variable counts the number of donors in which each cell type is observed by summing the occurrences greater than 0 along the donor axis.\n",
    "    # The sampling_weights variable is then computed as the reciprocal of the number of donors per cell type. This assigns higher weights to cell types observed in more donors and lower weights to cell types observed in fewer donors.\n",
    "    # Finally, the sampling_weights are normalized to ensure they sum up to 1, allowing them to be directly used as weights in the Bayesian inference.\n",
    "    # You can then use the compute_sampling_weights function to compute the sampling weights and pass them to the compute_posterior_probabilities function as the sampling_weights argument.\n",
    "\n",
    "    # The idea here is to find common celltypes across donors which are representative of the state\n",
    "\n",
    "    # Count the number of donors in which each cell type is observed\n",
    "    num_donors_per_cell_type = np.sum(cell_type_distributions_per_donor > 0, axis=0)\n",
    "    # Compute the sampling weights based on the number of donors\n",
    "    sampling_weights = 1 / num_donors_per_cell_type\n",
    "    sampling_weights[sampling_weights == inf] = 0\n",
    "    # Normalize the sampling weights to sum up to 1\n",
    "    sampling_weights /= np.sum(sampling_weights)\n",
    "    return sampling_weights\n",
    "\n",
    "\n",
    "############ function block ############\n",
    "\n",
    "# create a loop which grabs unique combinations of each meta var (risk strat and site)\n",
    "var_combi = adata.obs[meta_vars].value_counts().reset_index(name=\"count\")\n",
    "var_combi[\"combi\"] = var_combi[meta_vars].apply(\n",
    "    lambda row: \"_\".join(row.values.astype(str)), axis=1\n",
    ")\n",
    "obs = adata.obs.copy()\n",
    "obs[\"bayesian_prior_meta\"] = obs[meta_vars].apply(\n",
    "    lambda row: \"_\".join(row.values.astype(str)), axis=1\n",
    ")\n",
    "\n",
    "for var_combi_idx in var_combi.index:\n",
    "    combi = var_combi.loc[var_combi_idx, \"combi\"]\n",
    "    # Grab data which matches a metadata class of patient stratification, maybe include sampling region too\n",
    "    bayesian_obs = obs[obs[\"bayesian_prior_meta\"].isin([combi])]\n",
    "    # if lenght is <3, consider passing without liklihood correction\n",
    "\n",
    "# Draw empirical samples for pirors\n",
    "bayesian_obs = obs[obs[\"bayesian_prior_meta\"].isin([\"high_Tumor\"])]\n",
    "empirical_priors = draw_emprirical_bayes_priors(\n",
    "    bayesian_obs, cell_states, batch_id, train_var, max_iter, k_samples, sampling_prop\n",
    ")\n",
    "priors_df = pd.DataFrame()\n",
    "for key in empirical_priors.keys():\n",
    "    prior = (\n",
    "        empirical_priors[key].pivot(batch_id, train_var, values=\"prop\").fillna(0)\n",
    "    )  # .droplevel(0,axis =1)\n",
    "    prior.index = prior.index.astype(str) + \"_\" + str(key)\n",
    "    priors_df = pd.concat([priors_df, prior])\n",
    "\n",
    "# Compute sampling weights\n",
    "# - Weights are celltype availability per donor from the original distribution\n",
    "# - We penalise celltype probabilities which are represented in fewer donors\n",
    "p_tmp = bayesian_obs.groupby([batch_id, train_var]).apply(len).reset_index()\n",
    "p_tmp = (p_tmp.pivot(batch_id, train_var).fillna(0)).droplevel(0, axis=1)\n",
    "# compute an original distribution across all cells\n",
    "p_tmp = bayesian_obs.groupby([batch_id, train_var]).apply(len).reset_index()\n",
    "# p_tmp =(p_tmp.pivot(batch_id,train_var).fillna(0)).reset_index().melt(id_vars=batch_id)\n",
    "missing_classes = list(set(list(p_tmp[train_var].unique())) ^ set((cell_states)))\n",
    "# # add any missing cell-states back in\n",
    "# p_tmp_pv =(p_tmp.pivot(batch_id,train_var).fillna(0)).droplevel(0,axis =1)\n",
    "# p_tmp_pv[missing_classes] = 0\n",
    "# p_tmp_pv\n",
    "\n",
    "# Create sampling weights\n",
    "p_tmp = bayesian_obs.groupby([batch_id, train_var]).apply(len).reset_index()\n",
    "p_tmp = (p_tmp.pivot(batch_id, train_var).fillna(0)).droplevel(0, axis=1)\n",
    "# compute an original distribution across all cells\n",
    "p_tmp = bayesian_obs.groupby([batch_id, train_var]).apply(len).reset_index()\n",
    "# p_tmp =(p_tmp.pivot(batch_id,train_var).fillna(0)).reset_index().melt(id_vars=batch_id)\n",
    "missing_classes = list(set(list(p_tmp[train_var].unique())) ^ set((cell_states)))\n",
    "# add any missing cell-states back in\n",
    "p_tmp_pv = (p_tmp.pivot(batch_id, train_var).fillna(0)).droplevel(0, axis=1)\n",
    "p_tmp_pv[missing_classes] = 0\n",
    "p_tmp = p_tmp_pv\n",
    "\n",
    "sampling_weights = compute_sampling_weights(p_tmp)\n",
    "\n",
    "# Bayesian HMC\n",
    "priors = pd.DataFrame(bulk_probs)\n",
    "observations = priors_df\n",
    "posteriors = estimate_posterior_probabilities(\n",
    "    np.array(priors).flatten(), observations.values\n",
    ")\n",
    "posteriors.columns = observations.columns\n",
    "# posteriors = pd.DataFrame(posteriors,columns=priors.columns)\n",
    "print(posteriors)\n",
    "print(posteriors.mean(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-effort",
   "metadata": {},
   "source": [
    "# bayesian imputation of gene expression profile per cell-state\n",
    "A Bayesian framework for imputing gene expression in bulk data using single-cell data involves modeling the gene expression values as a Gaussian process. The basic idea is to model the latent gene expression values in the bulk data as a function of the latent gene expression values in the single-cell data, and use a Bayesian approach to estimate the parameters of the Gaussian process. The resulting model can be used to predict the gene expression values for each cell type in the bulk data.\n",
    "\n",
    "Let's define some notation. Let $y_b$ be the observed gene expression values for a bulk sample, $X_b$ be the design matrix for the bulk sample, and $\\beta_b$ be the coefficients in a linear regression model relating the gene expression values in the bulk sample to the latent gene expression values in the single-cell data. Let $y_s$ be the observed gene expression values for a single-cell sample, $X_s$ be the design matrix for the single-cell sample, and $\\beta_s$ be the coefficients in a linear regression model relating the gene expression values in the single-cell sample to the latent gene expression values.\n",
    "\n",
    "We can model the latent gene expression values in the bulk sample as a Gaussian process with a mean function that is a linear combination of the latent gene expression values in the single-cell sample:\n",
    "\n",
    "$$ f_b \\sim \\mathcal{GP}(\\mathbf{X}_b\\beta_b, k_b(\\mathbf{Z},\\mathbf{Z})), $$\n",
    "\n",
    "where $\\mathbf{Z}$ is the matrix of latent gene expression values in the single-cell sample, $k_b(\\cdot,\\cdot)$ is a kernel function for the bulk sample, and $\\beta_b$ is a vector of coefficients that relate the latent gene expression values in the single-cell sample to the observed gene expression values in the bulk sample.\n",
    "\n",
    "We can then use a Bayesian approach to estimate the parameters of the Gaussian process. Specifically, we can compute the posterior distribution of the latent gene expression values in the bulk sample given the observed gene expression values and the latent gene expression values in the single-cell sample:\n",
    "\n",
    "$$ p(\\mathbf{f}_b | \\mathbf{y}_b, \\mathbf{Z}) \\propto p(\\mathbf{y}_b | \\mathbf{f}_b) p(\\mathbf{f}_b | \\mathbf{X}_b, \\mathbf{Z}), $$\n",
    "\n",
    "where $p(\\mathbf{y}_b | \\mathbf{f}_b)$ is the likelihood of the observed gene expression values given the latent gene expression values, and $p(\\mathbf{f}_b | \\mathbf{X}_b, \\mathbf{Z})$ is the prior distribution of the latent gene expression values.\n",
    "\n",
    "The posterior distribution can be computed using the standard Bayesian inference formula:\n",
    "\n",
    "$$ p(\\mathbf{f}_b | \\mathbf{y}_b, \\mathbf{Z}) = \\frac{p(\\mathbf{y}_b | \\mathbf{f}_b) p(\\mathbf{f}_b | \\mathbf{X}_b, \\mathbf{Z})}{p(\\mathbf{y}_b | \\mathbf{X}_b, \\mathbf{Z})}, $$\n",
    "\n",
    "where $p(\\mathbf{y}_b | \\mathbf{X}_b, \\mathbf{Z})$ is the marginal likelihood of the observed gene expression values.\n",
    "\n",
    "Once we have computed the posterior distribution of the latent gene expression values in the bulk sample, we can use it to predict the gene expression values for each cell type in the bulk sample:\n",
    "\n",
    "$$ \\hat{\\mathbf{y}}_b^{(c)} = \\mathbf{X}_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-gazette",
   "metadata": {},
   "source": [
    "### \n",
    "In this section of the code, we define a function called impute_gene_expression() that performs gene expression imputation for a specific cell type using a Bayesian framework. The function takes as input a boolean array called ct_mask that indicates which cells in the single-cell reference data are of the target cell type, a Pandas DataFrame called bulk_expr that contains the bulk RNA sequencing data, a trained SVM classifier called clf, and prior mean and variance arrays called prior_mean and prior_var. The function first fits the SVM classifier to the single-cell reference data for the target cell type using the fit() method. It then computes the posterior mean and variance for each gene in the target cell type using the Bayesian framework described earlier. Finally, it imputes the gene expression values for the target cell type using the posterior mean and variance, and returns them as a Pandas DataFrame.\n",
    "\n",
    "After defining the impute_gene_expression() function, we initialize the prior mean and variance arrays using the mean and variance of the gene expression values in the single-cell reference data. We then initialize an empty dictionary called imputed_expr_dict to store the imputed gene expression values for each cell type.\n",
    "\n",
    "```\n",
    "\n",
    "In this section of the code, we define a function called `impute_gene_expression()` that performs gene expression imputation for a specific cell type using a Bayesian framework. The function takes as input a boolean array called `ct_mask` that indicates which cells in the single-cell reference data are of the target cell type, a Pandas DataFrame called `bulk_expr` that contains the bulk RNA sequencing data, a trained SVM classifier called `clf`, and prior mean and variance arrays called `prior_mean` and `prior_var`. The function first fits the SVM classifier to the single-cell reference data for the target cell type using the `fit()` method. It then computes the posterior mean and variance for each gene in the target cell type using the Bayesian framework described earlier. Finally, it imputes the gene expression values for the target cell type using the posterior mean and variance, and returns them as a Pandas DataFrame.\n",
    "\n",
    "After defining the `impute_gene_expression()` function, we initialize the prior mean and variance arrays using the mean and variance of the gene expression values in the single-cell reference data. We then initialize an empty dictionary called `imputed_expr_dict` to store the imputed gene expression values for each cell type.\n",
    "\n",
    "We then iterate over each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-sociology",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1908105408.py, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Gene expression imputation using Bayesian framework\n",
    "\n",
    "\n",
    "# First, we define a function to compute the gene expression imputation for a given cell type\n",
    "def impute_gene_expression(ct_mask, bulk_expr, clf, prior_mean, prior_var):\n",
    "    \"\"\"\n",
    "    Imputes gene expression values for a specific cell type using a Bayesian framework.\n",
    "\n",
    "    Args:\n",
    "        ct_mask (np.array): Boolean array indicating which cells in the single-cell reference data are of the target cell type\n",
    "        bulk_expr (pd.DataFrame): Bulk RNA sequencing data\n",
    "        clf (sklearn.svm.SVC): Trained support vector machine classifier\n",
    "        prior_mean (np.array): Prior mean for Bayesian imputation\n",
    "        prior_var (np.array): Prior variance for Bayesian imputation\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed gene expression values for the target cell type in the bulk RNA sequencing data\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit SVM classifier to single-cell reference data for the target cell type\n",
    "    clf.fit(sc_expr[ct_mask], np.ones(np.sum(ct_mask)))\n",
    "\n",
    "    # Compute posterior mean and variance for each gene in the target cell type\n",
    "    # using Bayesian framework\n",
    "    posterior_mean = prior_mean + clf.decision_function(bulk_expr) @ clf.dual_coef_.T\n",
    "    posterior_var = prior_var - np.sum(clf.coef_**2, axis=0) / (2 * clf.C)\n",
    "\n",
    "    # Impute gene expression values using the posterior mean and variance\n",
    "    imputed_expr = np.random.normal(loc=posterior_mean, scale=np.sqrt(posterior_var))\n",
    "\n",
    "    # Return imputed gene expression values as a dataframe\n",
    "    return pd.DataFrame(imputed_expr, columns=bulk_expr.columns)\n",
    "\n",
    "\n",
    "# Define prior mean and variance for Bayesian imputation\n",
    "prior_mean = sc_data.X.mean(axis=0)\n",
    "prior_var = sc_data.X.var(axis=0)\n",
    "\n",
    "# Initialize dictionary to store imputed gene expression values for each cell type\n",
    "imputed_expr_dict = {}\n",
    "\n",
    "# Iterate over each cell type\n",
    "for ct in cell_types:\n",
    "    ct_mask = sc_data.obs[\"level3_celltype\"] == ct\n",
    "\n",
    "    # Compute imputed gene expression values for the cell type using the previously defined function\n",
    "    imputed_expr = impute_gene_expression(\n",
    "        ct_mask, bulk_expr, clf, prior_mean, prior_var\n",
    "    )\n",
    "\n",
    "    # Store imputed gene expression values in dictionary\n",
    "    imputed_expr_dict[ct] = imputed_expr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-thomson",
   "metadata": {},
   "source": [
    "# Probability description\n",
    "\n",
    "In this case, the probabilistic SVM classifier is trained using all single-cell gene expression data, treating each cell as a separate binary classification problem, with positive labels indicating that the cell belongs to a particular cell type, and negative labels indicating that it does not. The SVM model then learns a decision boundary that separates the positive and negative samples in the feature space, with the goal of maximizing the margin between the two classes while minimizing the classification error.\n",
    "\n",
    "Once the SVM model is trained, it can be used to predict the probability that a new sample (e.g., bulk RNA-seq data) belongs to each cell type. This is done by computing the distance of the new sample to the decision boundary, and then transforming this distance into a probability estimate using a sigmoid function, as described in my earlier response.\n",
    "\n",
    "Therefore, instead of training separate SVM classifiers for each cell type, a single classifier is trained on all the data, with each cell type treated as a separate class.\n",
    "\n",
    "\n",
    "The distance of a new sample to the decision boundary in an SVM classifier is typically computed using the signed distance function. This function measures the distance of the sample to the decision boundary, and is positive if the sample is on the positive side of the boundary (i.e., closer to the positive class) and negative if the sample is on the negative side of the boundary (i.e., closer to the negative class). The signed distance function can be expressed as:\n",
    "\n",
    "d(x)= i=1∑N sv​​α i​y i​ K(x,x i​)+b\n",
    "\n",
    "where $d(\\mathbf{x})$ is the signed distance of the sample $\\mathbf{x}$ to the decision boundary, $N_{sv}$ is the number of support vectors, $\\alpha_i$ and $y_i$ are the Lagrange multipliers and labels of the support vectors, respectively, $K(\\mathbf{x}, \\mathbf{x_i})$ is the kernel function evaluated at the sample and support vector $\\mathbf{x_i}$, and $b$ is the bias term.\n",
    "\n",
    "The signed distance function can then be transformed into a probability estimate using a sigmoid function.\n",
    "\n",
    "#### Why the isotonic regressor method is not used:\n",
    "\n",
    "The isotonic regression method is a different approach that can be used for calibration of probabilistic predictions. It is used to transform a set of uncalibrated probability estimates into calibrated probabilities that are more reliable and better calibrated with respect to the true probability. In contrast, the signed distance function in an SVM classifier is not a probabilistic prediction, but rather a measure of the distance of a sample to the decision boundary.\n",
    "\n",
    "\n",
    "The isotonic regression method is not directly applicable for the task of probabilistic deconvolution of bulk RNA-seq data using single-cell RNA-seq data as input. The main purpose of isotonic regression is to transform uncalibrated probability estimates into calibrated probabilities that are more reliable and better calibrated with respect to the true probability.\n",
    "\n",
    "In the workflow described, the SVM classifier already outputs calibrated probability estimates for the different cell types, which are obtained by transforming the signed distance function using a sigmoid function. Therefore, there is no need to apply isotonic regression for calibration of the probability estimates.\n",
    "\n",
    "However, isotonic regression may be useful in other contexts where calibration of probability estimates is required, such as in classification tasks where the output of the classifier is a set of uncalibrated probability estimates. In such cases, isotonic regression can be used to transform the probability estimates into calibrated probabilities that are more reliable and better calibrated with respect to the true probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-lafayette",
   "metadata": {},
   "source": [
    "d(x)= i=1∑N sv​​α i​y i​ K(x,x i​)+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-poker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-sleeve",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knn_bayes",
   "language": "python",
   "name": "knn_bayes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
