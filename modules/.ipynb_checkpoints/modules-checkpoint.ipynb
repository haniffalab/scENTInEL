{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thrown-roman",
   "metadata": {},
   "source": [
    "# Author notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR multi-tissue cross-comparison\n",
    "\n",
    "##### Ver:: A2_V6\n",
    "##### Author(s) : Issac Goh\n",
    "##### Date : 220823;YYMMDD\n",
    "### Author notes\n",
    "#    - Current defaults scrpae data from web, so leave as default and run\n",
    "#    - slices model and anndata to same feature shape, scales anndata object\n",
    "#    - added some simple benchmarking\n",
    "#    - creates dynamic cutoffs for probability score (x*sd of mean) in place of more memory intensive confidence scoring\n",
    "#    - Does not have majority voting set on as default, but module does exist\n",
    "#    - Multinomial logistic relies on the (not always realistic) assumption of independence of irrelevant alternatives whereas a series of binary logistic predictions does not. collinearity is assumed to be relatively low, as it becomes difficult to differentiate between the impact of several variables if this is not the case\n",
    "#    - Feel free to feed this model latent representations which capture non-linear relationships, the model will attempt to resolve any linearly seperable features. Feature engineering can be applied here.\n",
    "    \n",
    "### Features to add\n",
    "#    - Add ability to consume anndata zar format for sequential learning\n",
    "### Modes to run in\n",
    "#    - Run in training mode\n",
    "#    - Run in projection mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-skill",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "\n",
    "# import pkg_resources\n",
    "# required = {'harmonypy','sklearn','scanpy','pandas', 'numpy', 'scipy', 'matplotlib', 'seaborn' ,'scipy'}\n",
    "# installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "# missing = required - installed\n",
    "# if missing:\n",
    "#    print(\"Installing missing packages:\" )\n",
    "#    print(missing)\n",
    "#    python = sys.executable\n",
    "#    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "import sys\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "#from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import psutil\n",
    "import random\n",
    "import threading\n",
    "import tracemalloc\n",
    "import itertools\n",
    "import math\n",
    "import warnings\n",
    "import sklearn.metrics as metrics\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "#from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import psutil\n",
    "import random\n",
    "import threading\n",
    "import tracemalloc\n",
    "import itertools\n",
    "import math\n",
    "import warnings\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "import mygene\n",
    "import gseapy as gp\n",
    "import mygene\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-poker",
   "metadata": {},
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_model_loaders\n",
    "\n",
    "def load_models(model_dict,model_run):\n",
    "    if (Path(model_dict[model_run])).is_file():\n",
    "        # Load data (deserialize)\n",
    "        model = pkl.load(open(model_dict[model_run], \"rb\"))\n",
    "        return model\n",
    "    elif 'http' in model_dict[model_run]:\n",
    "        print('Loading model from web source')\n",
    "        r_get = requests.get(model_dict[model_run])\n",
    "        fpath = './model_temp.sav'\n",
    "        open(fpath , 'wb').write(r_get.content)\n",
    "        model = pkl.load(open(fpath, \"rb\"))\n",
    "        return model\n",
    "\n",
    "def load_adatas(adatas_dict,data_merge, data_key_use,QC_normalise):\n",
    "    if data_merge == True:\n",
    "        # Read\n",
    "        gene_intersect = {} # unused here\n",
    "        adatas = {}\n",
    "        for dataset in adatas_dict.keys():\n",
    "            if 'https' in adatas_dict[dataset]:\n",
    "                print('Loading anndata from web source')\n",
    "                adatas[dataset] = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[dataset])\n",
    "            adatas[dataset] = sc.read(data[dataset])\n",
    "            adatas[dataset].var_names_make_unique()\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            adatas[dataset].obs['dataset_merge'] = dataset\n",
    "            gene_intersect[dataset] = list(adatas[dataset].var.index)\n",
    "        adata = list(adatas.values())[0].concatenate(list(adatas.values())[1:],join='inner')\n",
    "        return adatas, adata\n",
    "    elif data_merge == False:\n",
    "        if 'https' in adatas_dict[data_key_use]:\n",
    "            print('Loading anndata from web source')\n",
    "            adata = sc.read('./temp_adata.h5ad',backup_url=adatas_dict[data_key_use])\n",
    "        else: \n",
    "            adata = sc.read(adatas_dict[data_key_use])\n",
    "    if QC_normalise == True:\n",
    "        print('option to apply standardisation to data detected, performing basic QC filtering')\n",
    "        sc.pp.filter_cells(adata, min_genes=200)\n",
    "        sc.pp.filter_genes(adata, min_cells=3)\n",
    "        sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "        \n",
    "    return adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-anger",
   "metadata": {},
   "source": [
    "# raw_preprocessing_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_feature_selection_dimensionality_reduction\n",
    "\n",
    "# Feature selection by dispersion\n",
    "def subset_top_hvgs(adata_lognorm, n_top_genes):\n",
    "    dispersion_norm = adata_lognorm.var['dispersions_norm'].values.astype('float32')\n",
    "\n",
    "    dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]\n",
    "    dispersion_norm[\n",
    "                ::-1\n",
    "            ].sort()  # interestingly, np.argpartition is slightly slower\n",
    "\n",
    "    disp_cut_off = dispersion_norm[n_top_genes - 1]\n",
    "    gene_subset = adata_lognorm.var['dispersions_norm'].values >= disp_cut_off\n",
    "    return(adata_lognorm[:,gene_subset])\n",
    "\n",
    "# Prep data for VAE-based dim reduction\n",
    "def prep_scVI(adata, \n",
    "              n_hvgs = 5000,\n",
    "              remove_cc_genes = True,\n",
    "              remove_tcr_bcr_genes = False\n",
    "             ):\n",
    "    ## Remove cell cycle genes\n",
    "    if remove_cc_genes:\n",
    "        adata = panfetal_utils.remove_geneset(adata,genes.cc_genes)\n",
    "\n",
    "    ## Remove TCR/BCR genes\n",
    "    if remove_tcr_bcr_genes:\n",
    "        adata = panfetal_utils.remove_geneset(adata, genes.IG_genes)\n",
    "        adata = panfetal_utils.remove_geneset(adata, genes.TCR_genes)\n",
    "        \n",
    "    ## HVG selection\n",
    "    adata = subset_top_hvgs(adata, n_top_genes=n_hvgs)\n",
    "    return(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-ireland",
   "metadata": {},
   "source": [
    "# general_utlities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "# resource usage logger\n",
    "class DisplayCPU(threading.Thread):\n",
    "    def run(self):\n",
    "        tracemalloc.start()\n",
    "        starting, starting_peak = tracemalloc.get_traced_memory()\n",
    "        self.running = True\n",
    "        self.starting = starting\n",
    "        currentProcess = psutil.Process()\n",
    "        cpu_pct = []\n",
    "        peak_cpu = 0\n",
    "        while self.running:\n",
    "            peak_cpu = 0\n",
    "#           time.sleep(3)\n",
    "#             print('CPU % usage = '+''+ str(currentProcess.cpu_percent(interval=1)))\n",
    "#             cpu_pct.append(str(currentProcess.cpu_percent(interval=1)))\n",
    "            cpu = currentProcess.cpu_percent()\n",
    "        # track the peak utilization of the process\n",
    "            if cpu > peak_cpu:\n",
    "                peak_cpu = cpu\n",
    "                peak_cpu_per_core = peak_cpu/psutil.cpu_count()\n",
    "        self.peak_cpu = peak_cpu\n",
    "        self.peak_cpu_per_core = peak_cpu_per_core\n",
    "        \n",
    "    def stop(self):\n",
    "#        cpu_pct = DisplayCPU.run(self)\n",
    "        self.running = False\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        return current, peak\n",
    "\n",
    "# Frequency redistribution mode for assigning classes by categorical detected communities\n",
    "def freq_redist_68CI(adata,clusters_reassign):\n",
    "    if freq_redist != False:\n",
    "        print('Frequency redistribution commencing')\n",
    "        cluster_prediction = \"consensus_clus_prediction\"\n",
    "        lr_predicted_col = 'predicted'\n",
    "        pred_out[clusters_reassign] = adata.obs[clusters_reassign].astype(str)\n",
    "        reassign_classes = list(pred_out[clusters_reassign].unique())\n",
    "        lm = 1 # lambda value\n",
    "        pred_out[cluster_prediction] = pred_out[clusters_reassign]\n",
    "        for z in pred_out[clusters_reassign][pred_out[clusters_reassign].isin(reassign_classes)].unique():\n",
    "            df = pred_out\n",
    "            df = df[(df[clusters_reassign].isin([z]))]\n",
    "            df_count = pd.DataFrame(df[lr_predicted_col].value_counts())\n",
    "            # Look for classificationds > 68CI\n",
    "            if len(df_count) > 1:\n",
    "                df_count_temp = df_count[df_count[lr_predicted_col]>int(int(df_count.mean()) + (df_count.std()*lm))]\n",
    "                if len(df_count_temp >= 1):\n",
    "                    df_count = df_count_temp\n",
    "            #print(df_count)     \n",
    "            freq_arranged = df_count.index\n",
    "            cat = freq_arranged[0]\n",
    "        #Make the cluster assignment first\n",
    "            pred_out[cluster_prediction] = pred_out[cluster_prediction].astype(str)\n",
    "            pred_out.loc[pred_out[clusters_reassign] == z, [cluster_prediction]] = cat\n",
    "        # Create assignments for any classification >68CI\n",
    "            for cats in freq_arranged:\n",
    "                #print(cats)\n",
    "                cats_assignment = cats#.replace(data1,'') + '_clus_prediction'\n",
    "                pred_out.loc[(pred_out[clusters_reassign] == z) & (pred_out[lr_predicted_col] == cats),[cluster_prediction]] = cats_assignment\n",
    "        min_counts = pd.DataFrame((pred_out[cluster_prediction].value_counts()))\n",
    "        reassign = list(min_counts.index[min_counts[cluster_prediction]<=2])\n",
    "        pred_out[cluster_prediction] = pred_out[cluster_prediction].str.replace(str(''.join(reassign)),str(''.join(pred_out.loc[pred_out[clusters_reassign].isin(list(pred_out.loc[(pred_out[cluster_prediction].isin(reassign)),clusters_reassign])),lr_predicted_col].value_counts().head(1).index.values)))\n",
    "        return pred_out\n",
    "    \n",
    "# Module to produce report for projection accuracy metrics on tranductive runs     \n",
    "def report_f1(model,train_x, train_label):\n",
    "    ## Report accuracy score\n",
    "    \n",
    "    # cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=1)\n",
    "    # # evaluate the model and collect the scores\n",
    "    # n_scores = cross_val_score(lr, train_x, train_label, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # # report the model performance\n",
    "    # print('Mean Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "    # Report Precision score\n",
    "    metric = pd.DataFrame((metrics.classification_report(train_label, model.predict(train_x), digits=2,output_dict=True))).T\n",
    "    cm = confusion_matrix(train_label, model.predict(train_x))\n",
    "    #cm = confusion_matrix(train_label, model.predict_proba(train_x))\n",
    "    df_cm = pd.DataFrame(cm, index = model.classes_,columns = model.classes_)\n",
    "    df_cm = (df_cm / df_cm.sum(axis=0))*100\n",
    "    plt.figure(figsize = (20,15))\n",
    "    sn.set(font_scale=1) # for label size\n",
    "    pal = sns.diverging_palette(240, 10, n=10)\n",
    "    #plt.suptitle(('Mean Accuracy 5 fold: %.3f std: %.3f' % (np.mean(n_scores),  np.std(n_scores))), y=1.05, fontsize=18)\n",
    "    #Plot precision recall and recall\n",
    "    table = plt.table(cellText=metric.values,colWidths = [1]*len(metric.columns),\n",
    "    rowLabels=metric.index,\n",
    "    colLabels=metric.columns,\n",
    "    cellLoc = 'center', rowLoc = 'center',\n",
    "    loc='bottom', bbox=[0.25, -0.6, 0.5, 0.3])\n",
    "    table.scale(1, 2)\n",
    "    table.set_fontsize(10)\n",
    "\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},cmap=pal) # font size\n",
    "    print(metrics.classification_report(train_label, model.predict(train_x), digits=2))\n",
    "\n",
    "def compute_label_log_losses(df, true_label, pred_columns):\n",
    "    \"\"\"\n",
    "    Compute log loss (cross-entropy loss).\n",
    "    \n",
    "    Parameters:\n",
    "    df : dataframe containing the predicted probabilities and original labels as columns\n",
    "    true_label : column or array-like of shape (n_samples,) containg cateogrical labels\n",
    "    pred_columns : columns or array-like of shape (n_samples, n_clases) containg predicted probabilities\n",
    "\n",
    "    converts to:\n",
    "    y_true : array-like of shape (n_samples,) True labels. The binary labels in a one-vs-rest fashion.\n",
    "    y_pred : array-like of shape (n_samples, n_classes) Predicted probabilities. \n",
    "        \n",
    "    Returns:\n",
    "    log_loss : dictionary of celltype key and float\n",
    "    weights : float\n",
    "    \"\"\"\n",
    "    log_losses = {}\n",
    "    y_true = (pd.get_dummies(df[true_label]))\n",
    "    y_pred = df[pred_col]\n",
    "    loss = log_loss(np.array(y_true), np.array(y_pred))\n",
    "    for label in range(y_true.shape[1]):\n",
    "        log_loss_label = log_loss(np.array(y_true)[:, label], np.array(y_pred)[:, label])\n",
    "        log_losses[list(y_true.columns)[label]] = (log_loss_label)\n",
    "    weights = 1/np.array(list(log_losses.values()))\n",
    "    weights /= np.sum(weights)\n",
    "    weights = np.array(weights)\n",
    "    return loss, log_losses, weights\n",
    "\n",
    "def regression_results(df, true_label, pred_label, pred_columns):\n",
    "    # Regression metrics\n",
    "    y_true = df[true_label]\n",
    "    y_pred = df[pred_label]\n",
    "    loss, log_losses, weights = compute_label_log_losses(df, true_label, pred_columns)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "#     r2=metrics.r2_score(y_true, y_pred)\n",
    "    print('Cross entropy loss: ', round(loss,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    print('label Cross entropy loss: ')\n",
    "    print(log_losses)  \n",
    "    return loss, log_losses, weights\n",
    "\n",
    "\n",
    "# ENSDB-HGNC Option 1 \n",
    "#from gseapy.parser import Biomart\n",
    "#bm = Biomart()\n",
    "## view validated marts#\n",
    "#marts = bm.get_marts()\n",
    "## view validated dataset\n",
    "#datasets = bm.get_datasets(mart='ENSEMBL_MART_ENSEMBL')\n",
    "## view validated attributes\n",
    "#attrs = bm.get_attributes(dataset='hsapiens_gene_ensembl')\n",
    "## view validated filters\n",
    "#filters = bm.get_filters(dataset='hsapiens_gene_ensembl')\n",
    "## query results\n",
    "#queries = ['ENSG00000125285','ENSG00000182968'] # need to be a python list\n",
    "#results = bm.query(dataset='hsapiens_gene_ensembl',\n",
    "#                       attributes=['ensembl_gene_id', 'external_gene_name', 'entrezgene_id', 'go_id'],\n",
    "#                       filters={'ensemble_gene_id': queries}\n",
    "                      \n",
    "# ENSDB-HGNC Option 2\n",
    "def convert_hgnc(input_gene_list):\n",
    "    import mygene\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    geneList = input_gene_list \n",
    "    geneSyms = mg.querymany(geneList , scopes='ensembl.gene', fields='symbol', species='human')\n",
    "    return(pd.DataFrame(geneSyms))\n",
    "# Example use: convert_hgnc(['ENSG00000148795', 'ENSG00000165359', 'ENSG00000150676'])\n",
    "\n",
    "# Scanpy_degs_to_long_format\n",
    "def convert_scanpy_degs(input_dataframe):\n",
    "    if 'concat' in locals() or 'concat' in globals():\n",
    "        del(concat)\n",
    "    degs = input_dataframe\n",
    "    n = degs.loc[:, degs.columns.str.endswith(\"_n\")]\n",
    "    n = pd.melt(n)\n",
    "    p = degs.loc[:, degs.columns.str.endswith(\"_p\")]\n",
    "    p = pd.melt(p)\n",
    "    l = degs.loc[:, degs.columns.str.endswith(\"_l\")]\n",
    "    l = pd.melt(l)\n",
    "    n = n.replace(regex=r'_n', value='')\n",
    "    n = n.rename(columns={\"variable\": \"cluster\", \"value\": \"gene\"})\n",
    "    p = (p.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"p_val\"})\n",
    "    l = (l.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"logfc\"})\n",
    "    return(pd.concat([n,p,l],axis=1))\n",
    "#Usage: convert_scanpy_degs(scanpy_degs_file)\n",
    "\n",
    "# Clean convert gene list to list\n",
    "def as_gene_list(input_df,gene_col):\n",
    "    gene_list = input_df[gene_col]\n",
    "    glist = gene_list.squeeze().str.strip().tolist()\n",
    "    return(glist)\n",
    "\n",
    "# No ranking enrichr\n",
    "def enrich_no_rank(input_gene_list,database,species=\"Human\",description=\"enr_no_rank\",outdir = \"./enr\",cutoff=0.5):\n",
    "    # list, dataframe, series inputs are supported\n",
    "    enr = gp.enrichr(gene_list=input_gene_list,\n",
    "                     gene_sets=database,\n",
    "                     organism=species, # don't forget to set organism to the one you desired! e.g. Yeast\n",
    "                     #description=description,\n",
    "                     outdir=outdir,\n",
    "                     # no_plot=True,\n",
    "                     cutoff=cutoff # test dataset, use lower value from range(0,1)\n",
    "                    )\n",
    "    return(enr)\n",
    "    #Usge: enrich_no_rank(gene_as_list)\n",
    "    \n",
    "# Custom genelist test #input long format degs or dictionary of DEGS\n",
    "def custom_local_GO_enrichment(input_gene_list,input_gmt,input_gmt_key_col,input_gmt_values,description=\"local_go\",Background='hsapiens_gene_ensembl',Cutoff=0.5):\n",
    "    \n",
    "    #Check if GMT input is a dictionary or long-format input\n",
    "    if isinstance(input_gmt, dict):\n",
    "        print(\"input gmt is a dictionary, proceeding\")\n",
    "        dic = input_gmt\n",
    "    else:\n",
    "        print(\"input gmt is not a dictionary, if is pandas df,please ensure it is long-format proceeding to convert to dictionary\")\n",
    "        dic =  input_gmt.groupby([input_gmt_key_col])[input_gmt_values].agg(lambda grp: list(grp)).to_dict()\n",
    "        \n",
    "    enr_local = gp.enrichr(gene_list=input_gene_list,\n",
    "                 description=description,\n",
    "                 gene_sets=dic,\n",
    "                 background=Background, # or the number of genes, e.g 20000\n",
    "                 cutoff=Cutoff, # only used for testing.\n",
    "                 verbose=True)\n",
    "    return(enr_local)\n",
    "    #Example_usage: custom_local_GO_enrichment(input_gene_list,input_gmt,input_gmt_key_col,input_gmt_values) #input gmt can be long-format genes and ontology name or can be dictionary of the same   \n",
    "\n",
    "    \n",
    "# Pre-ranked GS enrichment\n",
    "def pre_ranked_enr(input_gene_list,gene_and_ranking_columns,database ='GO_Biological_Process_2018',permutation_num = 1000, outdir = \"./enr_ranked\",cutoff=0.25,min_s=5,max_s=1000):\n",
    "    glist = input_gene_list[gene_and_ranking_columns]\n",
    "    pre_res = gp.prerank(glist, gene_sets=database,\n",
    "                     threads=4,\n",
    "                     permutation_num=permutation_num, # reduce number to speed up testing\n",
    "                     outdir=outdir,\n",
    "                     seed=6,\n",
    "                     min_size=min_s,\n",
    "                     max_size=max_s)\n",
    "    return(pre_res)\n",
    "    #Example usage: pre_ranked_hyper_geom(DE, gene_and_ranking_columns = [\"gene\",\"logfc\"],database=['KEGG_2016','GO_Biological_Process_2018'])\n",
    "\n",
    "    \n",
    "# GSEA module for permutation test of differentially regulated genes\n",
    "# gene set enrichment analysis (GSEA), which scores ranked genes list (usually based on fold changes) and computes permutation test to check if a particular gene set is more present in the Up-regulated genes, \n",
    "# among the DOWN_regulated genes or not differentially regulated.\n",
    "#NES = normalised enrichment scores accounting for geneset size\n",
    "def permutation_ranked_enr(input_DE,cluster_1,cluster_2,input_DE_clust_col,input_ranking_col ,input_gene_col ,database = \"GO_Biological_Process_2018\"):\n",
    "    input_DE = input_DE[input_DE[input_DE_clust_col].isin([cluster_1,cluster_2])]\n",
    "    #Make set2 negative values to represent opposing condition\n",
    "    input_DE[input_ranking_col].loc[input_DE[input_DE_clust_col].isin([cluster_2])] = -input_DE[input_ranking_col].loc[input_DE[input_DE_clust_col].isin([cluster_2])]\n",
    "    enr_perm = pre_ranked_enr(input_DE,[input_gene_col,input_ranking_col],database,permutation_num = 100, outdir = \"./enr_ranked_perm\",cutoff=0.5)\n",
    "    return(enr_perm)\n",
    "    #Example usage:permutation_ranked_enr(input_DE = DE, cluster_1 = \"BM\",cluster_2 = \"YS\",input_DE_clust_col = \"cluster\",input_ranking_col = \"logfc\",input_gene_col = \"gene\",database = \"GO_Biological_Process_2018\")\n",
    "    #input long-format list of genes and with a class for permutaion, the logfc ranking should have been derived at the same time\n",
    "\n",
    "\n",
    "#Creating similarity matrix from nested gene lists\n",
    "def create_sim_matrix_from_enr(input_df,nested_gene_column=\"Genes\",seperator=\";\",term_col=\"Term\"):\n",
    "#    input_df[gene_col] = input_df[gene_col].astype(str)\n",
    "#    input_df[gene_col] = input_df[gene_col].str.split(\";\")\n",
    "#    uni_val = list(input_df.index.unique())\n",
    "#    sim_mat = pd.DataFrame(index=uni_val, columns=uni_val)\n",
    "#    exploded_df = input_df.explode(gene_col)\n",
    "#    # Ugly loop for cosine gs similarity matrix (0-1)\n",
    "#    for i in (uni_val):\n",
    "#        row = exploded_df[exploded_df.index.isin([i])]\n",
    "#        for z in (uni_val):\n",
    "#            col = exploded_df[exploded_df.index.isin([z])]\n",
    "#            col_ls = col[gene_col]\n",
    "#            row_ls = row[gene_col]\n",
    "#            sim = len(set(col_ls) & set(row_ls)) / float(len(set(col_ls) | set(row_ls)))\n",
    "#            sim_mat.loc[i , z] = sim\n",
    "\n",
    "#    Check term col in columns else, check index as it\\s sometimes heree\n",
    "    if not term_col in list(input_df.columns):\n",
    "        input_df[term_col] = input_df.index\n",
    "\n",
    "#    Create a similarity matrix by cosine similarity\n",
    "    input_df = input_df.copy()\n",
    "    gene_col = nested_gene_column #\"ledge_genes\"\n",
    "    input_df[gene_col] = input_df[gene_col].astype(str)\n",
    "    input_df[gene_col] = input_df[gene_col].str.split(seperator)\n",
    "    term_vals = list(input_df[term_col].unique())\n",
    "    uni_val = list(input_df[term_col].unique())\n",
    "    sim_mat = pd.DataFrame(index=uni_val, columns=uni_val)\n",
    "    exploded_df = input_df.explode(gene_col)\n",
    "    arr = np.array(input_df[gene_col])\n",
    "    vals = list(exploded_df[nested_gene_column])\n",
    "    import scipy.sparse as sparse\n",
    "    def binarise(sets, full_set):\n",
    "        \"\"\"Return sparse binary matrix of given sets.\"\"\"\n",
    "        return sparse.csr_matrix([[x in s for x in full_set] for s in sets])\n",
    "    # Turn the matrix into a sparse boleen matrix of binarised values\n",
    "    sparse_matrix = binarise(arr, vals)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(sparse_matrix)\n",
    "    sim_mat = pd.DataFrame(similarities)\n",
    "    sim_mat.index = uni_val\n",
    "    sim_mat.columns = uni_val\n",
    "    return(sim_mat)\n",
    "#Example usage : sim_mat = create_sim_matrix_from_enr(enr.res2d)\n",
    "\n",
    "\n",
    "#Creating similarity matrix from GO terms\n",
    "def create_sim_matrix_from_term(input_df,nested_gene_column=\"Term\",seperator=\" \",term_col=\"Term\"):\n",
    "\n",
    "#    Check term col in columns else, check index as it\\s sometimes heree\n",
    "    if not term_col in list(input_df.columns):\n",
    "        input_df[term_col] = input_df.index\n",
    "\n",
    "#    Create a similarity matrix by cosine similarity\n",
    "    input_df = input_df.copy()\n",
    "    gene_col = nested_gene_column #\"ledge_genes\"\n",
    "    #input_df[gene_col] = input_df[gene_col].astype(str)\n",
    "    input_df[gene_col] = input_df[gene_col].str.split(seperator)\n",
    "    term_vals = list(input_df[term_col].unique())\n",
    "    uni_val = list(input_df.index.unique())\n",
    "    sim_mat = pd.DataFrame(index=uni_val, columns=uni_val)\n",
    "    exploded_df = input_df.explode(gene_col)\n",
    "    arr = np.array(input_df[gene_col])\n",
    "    vals = list(exploded_df[nested_gene_column])\n",
    "    import scipy.sparse as sparse\n",
    "    def binarise(sets, full_set):\n",
    "        \"\"\"Return sparse binary matrix of given sets.\"\"\"\n",
    "        return sparse.csr_matrix([[x in s for x in full_set] for s in sets])\n",
    "    sparse_matrix = binarise(arr, vals)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(sparse_matrix)\n",
    "    sim_mat = pd.DataFrame(similarities)\n",
    "    sim_mat.index = uni_val\n",
    "    sim_mat.columns = uni_val\n",
    "    return(sim_mat)\n",
    "\n",
    "#Creating similarity matrix from GO terms\n",
    "def create_sim_matrix_from_term2(input_df,nested_gene_column=\"Term\",seperator=\" \",term_col=\"Term\"):\n",
    "#    Horrifically bad cosine similairty estimate for word frequency\n",
    "#    Check term col in columns else, check index as it\\s sometimes heree\n",
    "    if not term_col in list(input_df.columns):\n",
    "        input_df[term_col] = input_df.index\n",
    "    input_df = input_df.copy()\n",
    "    gene_col = nested_gene_column #\"ledge_genes\"\n",
    "    #input_df[gene_col] = input_df[gene_col].astype(str)\n",
    "    term_vals = list(input_df[term_col].unique())\n",
    "    input_df[gene_col] = input_df[gene_col].str.split(seperator)\n",
    "    uni_val = list(input_df.index.unique())\n",
    "    sim_mat = pd.DataFrame(index=uni_val, columns=uni_val)\n",
    "    exploded_df = input_df.explode(gene_col)\n",
    "\n",
    "    nan_value = float(\"NaN\")\n",
    "    exploded_df.replace(\"\", nan_value, inplace=True)\n",
    "    exploded_df.dropna(subset = [gene_col], inplace=True)\n",
    "    arr = np.array(input_df[gene_col])\n",
    "\n",
    "    vals = list(exploded_df[nested_gene_column])\n",
    "\n",
    "    import scipy.sparse as sparse\n",
    "    def binarise(sets, full_set):\n",
    "        \"\"\"Return sparse binary matrix of given sets.\"\"\"\n",
    "        return sparse.csr_matrix([[x in s for x in full_set] for s in sets])\n",
    "    sparse_matrix = binarise(arr, vals)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(sparse_matrix)\n",
    "    sim_mat = pd.DataFrame(similarities)\n",
    "    sim_mat.index = uni_val\n",
    "    sim_mat.columns = uni_val\n",
    "    return(sim_mat)\n",
    "    #Example usage : sim_mat = create_sim_matrix_from_enr(enr.res2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-fountain",
   "metadata": {},
   "source": [
    "# main ML probabillistic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_probabillistic_training_projection_modules\n",
    "    \n",
    "# projection module\n",
    "def reference_projection(adata, model, dyn_std,partial_scale):\n",
    "    \n",
    "    class adata_temp:\n",
    "        pass\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    print('Determining model flavour')\n",
    "    try:\n",
    "        model_lr =  model['Model']\n",
    "        print('Consuming celltypist model')\n",
    "    except:# hasattr(model, 'coef_'):\n",
    "        print('Consuming non-celltypist model')\n",
    "        model_lr =  model\n",
    "    print(model_lr)\n",
    "    if train_x_partition == 'X':\n",
    "        print('Matching reference genes in the model')\n",
    "        k_x = np.isin(list(adata.var.index), list(model_lr.features))\n",
    "        if k_x.sum() == 0:\n",
    "            raise ValueError(f\"ðŸ›‘ No features overlap with the model. Please provide gene symbols\")\n",
    "        print(f\"ðŸ§¬ {k_x.sum()} features used for prediction\")\n",
    "        #slicing adata\n",
    "        k_x_idx = np.where(k_x)[0]\n",
    "        # adata_temp = adata[:,k_x_idx]\n",
    "        adata_temp.var = adata[:,k_x_idx].var\n",
    "        adata_temp.X = adata[:,k_x_idx].X\n",
    "        adata_temp.obs = adata[:,k_x_idx].obs\n",
    "        lr_idx = pd.DataFrame(model_lr.features, columns=['features']).reset_index().set_index('features').loc[list(adata_temp.var.index)].values\n",
    "        # adata_arr = adata_temp.X[:,list(lr_idexes['index'])]\n",
    "        # slice and reorder model\n",
    "        ni, fs, cf = model_lr.n_features_in_, model_lr.features, model_lr.coef_\n",
    "        model_lr.n_features_in_ = lr_idx.size\n",
    "        model_lr.features = np.array(model_lr.features)[lr_idx]\n",
    "        model_lr.coef_ = np.squeeze(model_lr.coef_[:,lr_idx]) #model_lr.coef_[:, lr_idx]\n",
    "        if partial_scale == True:\n",
    "            print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "            # Partial scaling alg\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "            n = adata_temp.X.shape[0]  # number of rows\n",
    "            # set dyn scale packet size\n",
    "            x_len = len(adata_temp.var)\n",
    "            y_len = len(adata.obs)\n",
    "            if y_len < 100000:\n",
    "                dyn_pack = int(x_len/10)\n",
    "                pack_size = dyn_pack\n",
    "            else:\n",
    "                # 10 pack for every 100,000\n",
    "                dyn_pack = int((y_len/100000)*10)\n",
    "                pack_size = int(x_len/dyn_pack)\n",
    "            batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "            index = 0  # helper-var\n",
    "            while index < n:\n",
    "                partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "                partial_x = adata_temp.X[index:index+partial_size]\n",
    "                scaler.partial_fit(partial_x)\n",
    "                index += partial_size\n",
    "            adata_temp.X = scaler.transform(adata_temp.X)\n",
    "    # model projections\n",
    "    print('Starting reference projection!')\n",
    "    if train_x_partition == 'X':\n",
    "        train_x = adata_temp.X\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "        \n",
    "    elif train_x_partition in list(adata.obsm.keys()): \n",
    "        print('{low_dim: this partition modality is still under development!}')\n",
    "        train_x = adata.obsm[train_x_partition]\n",
    "        pred_out = pd.DataFrame(model_lr.predict(train_x),columns = ['predicted'],index = list(adata.obs.index))\n",
    "        proba =  pd.DataFrame(model_lr.predict_proba(train_x),columns = model_lr.classes_,index = list(adata.obs.index))\n",
    "        pred_out = pred_out.join(proba)\n",
    "    \n",
    "    else:\n",
    "        print('{this partition modality is still under development!}')\n",
    "    ## insert modules for low dim below\n",
    "\n",
    "    # Simple dynamic confidence calling\n",
    "    pred_out['confident_calls'] = pred_out['predicted']\n",
    "    pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'] = pred_out.loc[pred_out.max(axis=1)<(pred_out.mean(axis=1) + (1*pred_out.std(axis=1))),'confident_calls'].astype(str) + '_uncertain'\n",
    "    # means_ = self.model.scaler.mean_[lr_idx] if self.model.scaler.with_mean else 0\n",
    "    return(pred_out,train_x,model_lr,adata_temp)\n",
    "\n",
    "# Modified LR train module, does not work with low-dim by default anymore, please use low-dim adapter\n",
    "def LR_train(adata, train_x, train_label, penalty='elasticnet', sparcity=0.2,max_iter=200,l1_ratio =0.2,tune_hyper_params =False,n_splits=5, n_repeats=3,l1_grid = [0.01,0.2,0.5,0.8], c_grid = [0.01,0.2,0.4,0.6]):\n",
    "    if tune_hyper_params == True:\n",
    "        train_labels=train_label\n",
    "        results = tune_lr_model(adata, train_x_partition = train_x, random_state = 42,  train_labels = train_labels, n_splits=n_splits, n_repeats=n_repeats,l1_grid = l1_grid, c_grid = c_grid)\n",
    "        print('hyper_params tuned')\n",
    "        sparcity = results.best_params_['C']\n",
    "        l1_ratio = results.best_params_['l1_ratio']\n",
    "    \n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, n_jobs=thread_num)\n",
    "    if (penalty == \"l1\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual = True, solver = 'liblinear',multi_class = 'ovr', n_jobs=thread_num ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'ovr', n_jobs=thread_num)\n",
    "    if train_x == 'X':\n",
    "        subset_train = adata.obs.index\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#        train_label = train_label[subset_train]\n",
    "        train_x = adata.X#[adata.obs.index.isin(list(adata.obs[subset_train].index))]\n",
    "#        predict_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_predict].index))]\n",
    "    elif train_x in adata.obsm.keys():\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[train_label].values\n",
    "#        predict_label = train_label[subset_predict]\n",
    "#         train_label = train_label[subset_train]\n",
    "        train_x = adata.obsm[train_x]\n",
    "#        predict_x = train_x\n",
    "#        train_x = train_x[subset_train, :]\n",
    "        # Define prediction parameters\n",
    "#        predict_x = predict_x[subset_predict]\n",
    "#        predict_x = pd.DataFrame(predict_x)\n",
    "#        predict_x.index = adata.obs[subset_predict].index\n",
    "    # Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "    model = lr.fit(train_x, train_label)\n",
    "    model.features = np.array(adata.var.index)\n",
    "    return model\n",
    "\n",
    "def tune_lr_model(adata, train_x_partition = 'X', random_state = 42, use_bayes_opt=True, train_labels = None, n_splits=5, n_repeats=3,l1_grid = [0.1,0.2,0.5,0.8], c_grid = [0.1,0.2,0.4,0.6]):\n",
    "    import bless as bless\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from numpy import arange\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from skopt import BayesSearchCV\n",
    "\n",
    "    # If latent rep is provided, randomly sample data in spatially aware manner for initialisation\n",
    "    r = np.random.RandomState(random_state)\n",
    "    if train_x_partition in adata.obsm.keys():\n",
    "        lvg = bless.bless(tune_train_x, RBF(length_scale=20), lam_final = 2, qbar = 2, random_state = r, H = 10, force_cpu=True)\n",
    "    #     try:\n",
    "    #         import cupy\n",
    "    #         lvg_2 = bless(adata.obsm[train_x_partition], RBF(length_scale=10), 10, 10, r, 10, force_cpu=False)\n",
    "    #     except ImportError:\n",
    "    #         print(\"cupy not found, defaulting to numpy\")\n",
    "        adata_tuning = adata[lvg.idx]\n",
    "        tune_train_x = adata_tuning.obsm[train_x_partition][:]\n",
    "    else:\n",
    "        print('no latent representation provided, random sampling instead')\n",
    "        prop = 0.1\n",
    "        random_vertices = []\n",
    "        n_ixs = int(len(adata.obs) * prop)\n",
    "        random_vertices = random.sample(list(range(len(adata.obs))), k=n_ixs)\n",
    "        adata_tuning = adata[random_vertices]\n",
    "        tune_train_x = adata_tuning.X\n",
    "        \n",
    "    if not train_labels == None:\n",
    "        tune_train_label = adata_tuning.obs[train_labels]\n",
    "    elif train_labels == None:\n",
    "        try:\n",
    "            print('no training labels provided, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        except:\n",
    "            print('no training labels provided, no neighbors, defaulting to unsuperived leiden clustering, updates will change this to voronoi greedy sampling')\n",
    "            sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            sc.tl.leiden(adata_tuning)\n",
    "        tune_train_label = adata_tuning.obs['leiden']\n",
    "    ## tune regularization for multinomial logistic regression\n",
    "    print('starting tuning loops')\n",
    "    X = tune_train_x\n",
    "    y = tune_train_label\n",
    "    grid = dict()\n",
    "    # define model\n",
    "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "    #model = LogisticRegression(penalty = penalty, max_iter =  200, dual=False,solver = 'saga', multi_class = 'multinomial',)\n",
    "    model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, n_jobs=4)\n",
    "    if (penalty == \"l1\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, dual = True, solver = 'liblinear',multi_class = 'multinomial', n_jobs=4 ) # one-vs-rest\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        model = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  100, dual=False,solver = 'saga',l1_ratio=l1_ratio,multi_class = 'multinomial', n_jobs=4) # use multinomial class if probabilities are descrete\n",
    "        grid['l1_ratio'] = l1_grid\n",
    "    grid['C'] = c_grid\n",
    "    \n",
    "    if use_bayes_opt == True:\n",
    "        # define search space\n",
    "        search_space = {'C': (np.min(c_grid), np.max(c_grid), 'log-uniform'), \n",
    "                        'l1_ratio': (np.min(l1_grid), np.max(l1_grid), 'uniform') if 'elasticnet' in penalty else None}\n",
    "        # define search\n",
    "        search = BayesSearchCV(model, search_space, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "        # perform the search\n",
    "        results = search.fit(X, y)\n",
    "    else:\n",
    "        # define search\n",
    "        search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "        # perform the search\n",
    "        results = search.fit(X, y)\n",
    "    # summarize\n",
    "    print('MAE: %.3f' % results.best_score_)\n",
    "    print('Config: %s' % results.best_params_)\n",
    "    return results\n",
    "\n",
    "def prep_training_data(adata_temp,feat_use,batch_key, model_key, batch_correction=False, var_length = 7500,penalty='elasticnet',sparcity=0.2,max_iter = 200,l1_ratio = 0.1,partial_scale=True,train_x_partition ='X',theta = 3,tune_hyper_params=False ):\n",
    "    model_name = model_key + '_lr_model'\n",
    "    print('performing highly variable gene selection')\n",
    "    sc.pp.highly_variable_genes(adata_temp, batch_key = batch_key, subset=False)\n",
    "    adata_temp = subset_top_hvgs(adata_temp,var_length)\n",
    "    #scale the input data\n",
    "    if partial_scale == True:\n",
    "        print('scaling input data, default option is to use incremental learning and fit in mini bulks!')\n",
    "        # Partial scaling alg\n",
    "        #adata_temp.X = (adata_temp.X)\n",
    "        scaler = StandardScaler(with_mean=False)\n",
    "        n = adata_temp.X.shape[0]  # number of rows\n",
    "        # set dyn scale packet size\n",
    "        x_len = len(adata_temp.var)\n",
    "        y_len = len(adata_temp.obs)\n",
    "        if y_len < 100000:\n",
    "            dyn_pack = int(x_len/10)\n",
    "            pack_size = dyn_pack\n",
    "        else:\n",
    "            # 10 pack for every 100,000\n",
    "            dyn_pack = int((y_len/100000)*10)\n",
    "            pack_size = int(x_len/dyn_pack)\n",
    "        batch_size =  1000#pack_size#500  # number of rows in each call to partial_fit\n",
    "        index = 0  # helper-var\n",
    "        while index < n:\n",
    "            partial_size = min(batch_size, n - index)  # needed because last loop is possibly incomplete\n",
    "            partial_x = adata_temp.X[index:index+partial_size]\n",
    "            scaler.partial_fit(partial_x)\n",
    "            index += partial_size\n",
    "        adata_temp.X = scaler.transform(adata_temp.X)\n",
    "#     else:\n",
    "#         sc.pp.scale(adata_temp, zero_center=True, max_value=None, copy=False, layer=None, obsm=None)\n",
    "    if (train_x_partition != 'X') & (train_x_partition in adata_temp.obsm.keys()):\n",
    "        print('train partition is not in OBSM, defaulting to PCA')\n",
    "        # Now compute PCA\n",
    "        sc.pp.pca(adata_temp, n_comps=100, use_highly_variable=True, svd_solver='arpack')\n",
    "        sc.pl.pca_variance_ratio(adata_temp, log=True,n_pcs=100)\n",
    "        \n",
    "        # Batch correction options\n",
    "        # The script will test later which Harmony values we should use \n",
    "        if(batch_correction == \"Harmony\"):\n",
    "            print(\"Commencing harmony\")\n",
    "            adata_temp.obs['lr_batch'] = adata_temp.obs[batch_key]\n",
    "            batch_var = \"lr_batch\"\n",
    "            # Create hm subset\n",
    "            adata_hm = adata_temp[:]\n",
    "            # Set harmony variables\n",
    "            data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "            meta_data = adata_hm.obs\n",
    "            vars_use = [batch_var]\n",
    "            # Run Harmony\n",
    "            ho = hm.run_harmony(data_mat, meta_data, vars_use,theta=theta)\n",
    "            res = (pd.DataFrame(ho.Z_corr)).T\n",
    "            res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "            # Insert coordinates back into object\n",
    "            adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "            adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "            # Run neighbours\n",
    "            #sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "            adata_temp = adata_hm[:]\n",
    "            del adata_hm\n",
    "        elif(batch_correction == \"BBKNN\"):\n",
    "            print(\"Commencing BBKNN\")\n",
    "            sc.external.pp.bbknn(adata_temp, batch_key=batch_var, approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=15) \n",
    "        print(\"adata1 and adata2 are now combined and preprocessed in 'adata' obj - success!\")\n",
    "\n",
    "\n",
    "    # train model\n",
    "#    train_x = adata_temp.X\n",
    "    #train_label = adata_temp.obs[feat_use]\n",
    "    print('proceeding to train model')\n",
    "    model = LR_train(adata_temp, train_x = train_x_partition, train_label=feat_use, penalty=penalty, sparcity=sparcity,max_iter=max_iter,l1_ratio = l1_ratio,tune_hyper_params = tune_hyper_params)\n",
    "    model.features = list(adata_temp.var.index)\n",
    "    return model\n",
    "\n",
    "def compute_weighted_impact(varm_file, top_loadings, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Computes the weighted impact of the features of a low-dimensional model.\n",
    "\n",
    "    Parameters:\n",
    "    varm_file (str): The path to the file containing the variable loadings of the model.\n",
    "    top_loadings (pd.DataFrame): A dataframe containing the top loadings for each class.\n",
    "    threshold (float): The p-value threshold for significance.\n",
    "\n",
    "    Returns:\n",
    "    top_loadings_lowdim (pd.DataFrame): A dataframe containing the top weighted impacts for each class.\n",
    "    \"\"\"\n",
    "    # Load the variable loadings from the file\n",
    "    model_varm = pd.read_csv(varm_file, index_col=0)\n",
    "\n",
    "    # Map the feature names to the column names of the variable loadings\n",
    "    feature_set = dict(zip(sorted(top_loadings['feature'].unique()), model_varm.columns))\n",
    "\n",
    "    # Melt the variable loadings dataframe and add a column for p-values\n",
    "    varm_melt = pd.melt(model_varm.reset_index(), id_vars='index')\n",
    "    varm_melt['pvals'] = np.nan\n",
    "\n",
    "    # Compute the p-values for each variable\n",
    "    for variable in varm_melt['variable'].unique():\n",
    "        varm_loadings = varm_melt[varm_melt['variable'] == variable]\n",
    "        med = np.median(varm_loadings['value'])\n",
    "        mad = np.median(np.abs(varm_loadings['value'] - med))\n",
    "        pvals = scipy.stats.norm.sf(varm_loadings['value'], loc=med, scale=1.4826*mad)\n",
    "        varm_melt.loc[varm_melt['variable'] == variable, 'pvals'] = pvals\n",
    "\n",
    "    # Filter the variables based on the p-value threshold\n",
    "    varm_sig = varm_melt[varm_melt['pvals'] < threshold]\n",
    "\n",
    "    # Compute the weighted impact for each feature of each class\n",
    "    top_loadings_lowdim = pd.DataFrame(columns=['class', 'feature', 'weighted_impact', 'e^coef_pval', 'e^coef', 'is_significant_sf'])\n",
    "    top_loadings_lw = top_loadings.groupby('class').head(10)\n",
    "    top_loadings_lw['feature'] = top_loadings_lw['feature'].map(feature_set)\n",
    "\n",
    "    for classes in top_loadings_lw['class'].unique():\n",
    "        for feature in top_loadings_lw.loc[top_loadings_lw['class'] == classes, ['feature', 'e^coef']].values:\n",
    "            temp_varm_sig = varm_sig[varm_sig['variable'] == feature[0]]\n",
    "            temp_varm_sig['weighted_impact'] = temp_varm_sig['value'] * feature[1]\n",
    "            temp_varm_sig = temp_varm_sig[['index', 'weighted_impact']]\n",
    "            temp_varm_sig.columns = ['feature', 'weighted_impact']\n",
    "            temp_varm_sig['class'] = classes\n",
    "            temp_varm_sig['e^coef_pval'] = top_loadings_lw.loc[(top_loadings_lw['class'] == classes) & (top_loadings_lw['feature'] == feature[0]), 'e^coef_pval'].values[0]\n",
    "            temp_varm_sig['e^coef'] = top_loadings_lw.loc[(top_loadings_lw['class'] == classes) & (top_loadings_lw['feature'] == feature[0]), 'e^coef'].values[0]\n",
    "            temp_varm_sig['is_significant_sf'] = top_loadings_lw.loc[(top_loadings_lw['class'] == classes) & (top_loadings_lw['feature'] == feature[0]), 'is_significant_sf'].values[0]\n",
    "            top_loadings_lowdim = pd.concat([top_loadings_lowdim, temp_varm_sig], ignore_index=True)\n",
    "    # Return the top 100 features with the highest weighted impact for each class\n",
    "    top_loadings_lowdim = top_loadings_lowdim.sort_values('weighted_impact', ascending=False).groupby('class').head(100)\n",
    "    return top_loadings_lowdim\n",
    "\n",
    "\n",
    "def get_binary_neigh_matrix(connectivities):\n",
    "    \"\"\"\n",
    "    Converts the connectivities matrix to a binary neighborhood membership matrix.\n",
    "    \"\"\"\n",
    "    return (connectivities > 0).astype(int)\n",
    "\n",
    "def get_label_counts(neigh_matrix, labels):\n",
    "    \"\"\"\n",
    "    Counts the number of occurrences of each label in the neighborhood of each cell.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(neigh_matrix.T.dot(pd.get_dummies(labels)))\n",
    "\n",
    "def compute_dist_entropy_product(neigh_membership, labels, dist_matrix):\n",
    "    \"\"\"\n",
    "    Computes the product of the average neighborhood distance and the entropy\n",
    "    of the label distribution in the neighborhood for each cell and each label.\n",
    "    \"\"\"\n",
    "    # Count the occurrences of each label in the neighborhood of each cell\n",
    "    label_counts = get_label_counts(neigh_membership, labels)\n",
    "\n",
    "    # Compute the entropy of the label distribution in the neighborhood of each cell\n",
    "    entropy_values = label_counts.apply(entropy, axis=1)\n",
    "\n",
    "    # Compute the average neighborhood distance for each cell\n",
    "    avg_distances = dist_matrix.multiply(neigh_membership).mean(axis=1).A1\n",
    "\n",
    "    # Compute the product of the average distance and the entropy for each cell\n",
    "    dist_entropy_product = avg_distances * entropy_values\n",
    "\n",
    "    return dist_entropy_product\n",
    "\n",
    "class WeightsOutput:\n",
    "    def __init__(self, weights, rhats, means, sds):\n",
    "        self.weights = weights\n",
    "        self.rhats = rhats\n",
    "        self.means = means\n",
    "        self.sds = sds\n",
    "\n",
    "def compute_weights(adata, use_rep, original_labels_col, predicted_labels_col):\n",
    "    # Extract the necessary data from the anndata object\n",
    "    obs_met = adata.obs\n",
    "    neigh_membership = get_binary_neigh_matrix(adata.obsp[adata.uns[use_rep]['connectivities_key']])\n",
    "    original_labels = obs_met[original_labels_col]\n",
    "    predicted_labels = obs_met[predicted_labels_col]\n",
    "    dist_matrix = adata.obsp[adata.uns[use_rep]['distances_key']]\n",
    "\n",
    "    # Compute the 'distance-entropy' product for each cell and each label\n",
    "    dist_entropy_product = compute_dist_entropy_product(neigh_membership, predicted_labels, dist_matrix)\n",
    "\n",
    "    # Compute the 'distance-entropy' product for the original labels\n",
    "    dist_entropy_product_orig = compute_dist_entropy_product(neigh_membership, original_labels, dist_matrix)\n",
    "\n",
    "    weights = {}\n",
    "    rhat_values = {}\n",
    "    means = []  # Collect all posterior means\n",
    "    sds = []  # Collect all posterior standard deviations\n",
    "    for label in np.unique(predicted_labels):\n",
    "        print(\"Sampling {} posterior distribution\".format(label))\n",
    "        # Perform Bayesian inference to compute the posterior distribution of the\n",
    "        # 'distance-entropy' product for this label\n",
    "        orig_pos = obs_met[original_labels_col].isin([label])\n",
    "        pred_pos = obs_met[predicted_labels_col].isin([label])\n",
    "        with pm.Model() as model:\n",
    "            #priors\n",
    "            mu = pm.Normal('mu', mu=dist_entropy_product_orig[orig_pos.values].mean(), sd=dist_entropy_product_orig[orig_pos.values].std())\n",
    "            sd = pm.HalfNormal('sd', sd=dist_entropy_product_orig[orig_pos.values].std())\n",
    "            #observations\n",
    "            obs = pm.Normal('obs', mu=mu, sd=sd, observed=dist_entropy_product_orig[pred_pos.values])\n",
    "            \n",
    "            if len(orig_pos) > 10000:\n",
    "                samp_rate = 0.1\n",
    "                smp = int(len(orig_pos)*samp_rate)\n",
    "                tne = smp = int(len(orig_pos)*samp_rate)/2\n",
    "                trace = pm.sample(smp, tune=tne)\n",
    "            else:\n",
    "                trace = pm.sample(1000, tune=500)\n",
    "        # Compute R-hat for this label\n",
    "        rhat = pm.rhat(trace)\n",
    "        rhat_values[label] = {var: rhat[var].data for var in rhat.variables}\n",
    "        # Compute the mean and the standard deviation of the posterior distribution for this label\n",
    "        mean_posterior = pm.summary(trace)['mean']['mu']\n",
    "        sd_posterior = pm.summary(trace)['sd']['sd']\n",
    "        sds.append(sd_posterior)\n",
    "        means.append(mean_posterior)\n",
    "        \n",
    "    # Mean posterior probabilitty models the stability of a label given entropy_distance measures within it's neighborhood\n",
    "    max_mean = max(means)\n",
    "    # SD here models the uncertainty of label entropy_distance measures\n",
    "    max_sd = max(sds)  # Compute the maximum standard deviation\n",
    "    \n",
    "    # Compute the weights as the sum of the normalized mean and the normalized standard deviation. This makes each weight relative to each other\n",
    "    # shift all weights up by epiislon constant\n",
    "    epsilon = 0.01\n",
    "    for label, mean, sd in zip(np.unique(predicted_labels), means, sds):\n",
    "        weights[label] = (1 - mean / max_mean) * (1 - sd / max_sd) + epsilon\n",
    "\n",
    "    return WeightsOutput(weights, rhat_values, means, sds)\n",
    "\n",
    "def apply_weights(prob_df, weights):\n",
    "    \"\"\"\n",
    "    Applies the computed weights to the probability dataframe and normalizes the result.\n",
    "    Parameters:\n",
    "    prob_df (pd.DataFrame): A dataframe where each row corresponds to a cell and each column corresponds to a label. Each entry is the probability of the cell being of the label.\n",
    "    weights (dict): A dictionary where each key-value pair corresponds to a label and its weight.\n",
    "\n",
    "    Returns:\n",
    "    norm_df (pd.DataFrame): A dataframe of the same shape as prob_df, but with the probabilities weighted and normalized.\n",
    "    \"\"\"\n",
    "    # Apply the weights\n",
    "    weighted_df = prob_df.mul(weights.weights)\n",
    "    # Normalize the result\n",
    "    norm_df = weighted_df.div(weighted_df.sum(axis=1), axis=0)\n",
    "    return norm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-communications",
   "metadata": {},
   "source": [
    "# Feature_assessment_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature_assessment_modules\n",
    "\n",
    "def long_format_features(top_loadings):\n",
    "    p = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_e^coef\")]\n",
    "    p = pd.melt(p)\n",
    "    n = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_feature\")]\n",
    "    n = pd.melt(n)\n",
    "    l = top_loadings.loc[:, top_loadings.columns.str.endswith(\"_coef\")]\n",
    "    l = pd.melt(l)\n",
    "    n = n.replace(regex=r'_feature', value='')\n",
    "    n = n.rename(columns={\"variable\": \"class\", \"value\": \"feature\"})\n",
    "    p = (p.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"e^coef\"})\n",
    "    l = (l.drop([\"variable\"],axis = 1)).rename(columns={ \"value\": \"coef\"})\n",
    "    concat = pd.concat([n,p,l],axis=1)\n",
    "    return concat\n",
    "\n",
    "### Feature importance notes\n",
    "#- If we increase the x feature one unit, then the prediction will change e to the power of its weight. We can apply this rule to the all weights to find the feature importance.\n",
    "#- We will calculate the Euler number to the power of its coefficient to find the importance.\n",
    "#- To sum up an increase of x feature by one unit increases the odds of being versicolor class by a factor of x[importance] when all other features remain the same.\n",
    "#- For low-dim, we look at the distribution of e^coef per class, we extract the \n",
    "# class coef_extract:\n",
    "#     def __init__(self, model,features, pos):\n",
    "# #         self.w = list(itertools.chain(*(model.coef_[pos]).tolist())) #model.coef_[pos]\n",
    "#         self.w = model.coef_[class_pred_pos]\n",
    "#         self.features = features \n",
    "\n",
    "def model_feature_sf(long_format_feature_importance, coef_use):\n",
    "        long_format_feature_importance[str(coef_use) + '_pval'] = 'NaN'\n",
    "        for class_lw in long_format_feature_importance['class'].unique():\n",
    "            df_loadings = long_format_feature_importance[long_format_feature_importance['class'].isin([class_lw])]\n",
    "            comps = coef_use #'e^coef'\n",
    "            U = np.mean(df_loadings[comps])\n",
    "            std = np.std(df_loadings[comps])\n",
    "            med =  np.median(df_loadings[comps])\n",
    "            mad = np.median(np.absolute(df_loadings[comps] - np.median(df_loadings[comps])))\n",
    "            # Survival function scaled by 1.4826 of MAD (approx norm)\n",
    "            pvals = scipy.stats.norm.sf(df_loadings[comps], loc=med, scale=1.4826*mad) # 95% CI of MAD <10,000 samples\n",
    "            #pvals = scipy.stats.norm.sf(df_loadings[comps], loc=U, scale=1*std)\n",
    "            df_loadings[str(comps) +'_pval'] = pvals\n",
    "            long_format_feature_importance.loc[long_format_feature_importance.index.isin(df_loadings.index)] = df_loadings\n",
    "        long_format_feature_importance['is_significant_sf'] = False\n",
    "        long_format_feature_importance.loc[long_format_feature_importance[coef_use+ '_pval']<0.05,'is_significant_sf'] = True\n",
    "        return long_format_feature_importance\n",
    "# Apply SF to e^coeff mat data\n",
    "#         pval_mat = pd.DataFrame(columns = mat.columns)\n",
    "#         for class_lw in mat.index:\n",
    "#             df_loadings = mat.loc[class_lw]\n",
    "#             U = np.mean(df_loadings)\n",
    "#             std = np.std(df_loadings)\n",
    "#             med =  np.median(df_loadings)\n",
    "#             mad = np.median(np.absolute(df_loadings - np.median(df_loadings)))\n",
    "#             pvals = scipy.stats.norm.sf(df_loadings, loc=med, scale=1.96*U)\n",
    "\n",
    "class estimate_important_features: # This calculates feature effect sizes of the model\n",
    "    def __init__(self, model, top_n):\n",
    "        print('Estimating feature importance')\n",
    "        classes =  list(model.classes_)\n",
    "         # get feature names\n",
    "        try:\n",
    "            model_features = list(itertools.chain(*list(model.features)))\n",
    "        except:\n",
    "            warnings.warn('no features recorded in data, naming features by position')\n",
    "            print('if low-dim lr was submitted, run linear decoding function to obtain true feature set')\n",
    "            model_features = list(range(0,model.coef_.shape[1]))\n",
    "            model.features = model_features\n",
    "        print('Calculating the Euler number to the power of coefficients')\n",
    "        impt_ = pow(math.e,model.coef_)\n",
    "        try:\n",
    "            self.euler_pow_mat = pd.DataFrame(impt_,columns = list(itertools.chain(*list(model.features))),index = list(model.classes_))\n",
    "        except:\n",
    "            self.euler_pow_mat = pd.DataFrame(impt_,columns = list(model.features),index = list(model.classes_))\n",
    "        self.top_n_features = pd.DataFrame(index = list(range(0,top_n)))\n",
    "        # estimate per class feature importance\n",
    "        \n",
    "        print('Estimating feature importance for each class')\n",
    "        mat = self.euler_pow_mat\n",
    "        for class_pred_pos in list(range(0,len(mat.T.columns))):\n",
    "            class_pred = list(mat.T.columns)[class_pred_pos]\n",
    "            #     print(class_pred)\n",
    "            temp_mat =  pd.DataFrame(mat.T[class_pred])\n",
    "            temp_mat['coef'] = model.coef_[class_pred_pos]\n",
    "            temp_mat = temp_mat.sort_values(by = [class_pred], ascending=False)\n",
    "            temp_mat = temp_mat.reset_index()\n",
    "            temp_mat.columns = ['feature','e^coef','coef']\n",
    "            temp_mat = temp_mat[['feature','e^coef','coef']]\n",
    "            temp_mat.columns =str(class_pred)+ \"_\" + temp_mat.columns\n",
    "            self.top_n_features = pd.concat([self.top_n_features,temp_mat.head(top_n)], join=\"inner\",ignore_index = False, axis=1)\n",
    "            self.to_n_features_long = model_feature_sf(long_format_features(self.top_n_features),'e^coef')\n",
    "            \n",
    "    \n",
    "    # plot class-wise features\n",
    "def model_class_feature_plots(top_loadings, classes, comps, p_lim, max_len,title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    for class_temp in classes:\n",
    "        class_lw = class_temp\n",
    "        long_format = top_loadings\n",
    "        df_loadings = long_format[long_format['class'].isin([class_lw])]\n",
    "        plt.hist(df_loadings[comps])\n",
    "        \n",
    "        if len(((df_loadings[comps][df_loadings[str(p_lim) +'_pval']<0.05]).unique())) < max_len:\n",
    "            for i in ((df_loadings[comps][df_loadings[str(p_lim) +'_pval']<0.05]).unique()):\n",
    "                plt.axvline(x=i,color='red')\n",
    "        elif len(((df_loadings[comps][df_loadings[str(p_lim) +'_pval']<0.05]).unique())) > max_len:\n",
    "            for i in ((df_loadings[comps].nlargest(max_len)).unique()):\n",
    "                plt.axvline(x=i,color='red')\n",
    "        med = np.median(df_loadings[comps])\n",
    "        plt.axvline(x=med,color='blue')\n",
    "        plt.xlabel('feature_importance', fontsize=12)\n",
    "        plt.title(title)\n",
    "        #plt.axvline(x=med,color='pinkp_lim\n",
    "        df_loadings[comps][df_loadings[str(p_lim) +'_pval']<0.05]\n",
    "        print(len(df_loadings[comps][df_loadings[str(p_lim) +'_pval']<0.05]))\n",
    "        #Plot feature ranking\n",
    "        if len(((df_loadings[comps][df_loadings[str(p_lim) +'_pval']<0.05]).unique())) < max_len:\n",
    "            plot_loading = pd.DataFrame(pd.DataFrame(df_loadings[comps][df_loadings[str(p_lim) +'_pval']<0.05]).iloc[:,0].sort_values(ascending=False))\n",
    "        elif len(((df_loadings[comps][df_loadings[str(p_lim) +'_pval']<0.05]).unique())) > max_len:\n",
    "            plot_loading = pd.DataFrame(pd.DataFrame(df_loadings[comps].nlargest(max_len)).iloc[:,0].sort_values(ascending=False))\n",
    "        table = plt.table(cellText=plot_loading.values,colWidths = [1]*len(plot_loading.columns),\n",
    "        rowLabels= list(df_loadings['feature'][df_loadings.index.isin(plot_loading.index)].reindex(plot_loading.index)), #plot_loading.index,\n",
    "        colLabels=plot_loading.columns,\n",
    "        cellLoc = 'center', rowLoc = 'center',\n",
    "        loc='right', bbox=[1.4, -0.05, 0.5,1])\n",
    "        table.scale(1, 2)\n",
    "        table.set_fontsize(10)\n",
    "\n",
    "# this module calculates how well the features learnt by the model are distibuted amongst classes and the query data\n",
    "def calculate_feature_distribution(adata,model, top_loadings, var='predicted'):\n",
    "    adata_model = adata\n",
    "    adata_model.obs['model_topn_feature_impact'] = 0\n",
    "\n",
    "    # Loop\n",
    "    long_format = top_loadings\n",
    "    for class_temp in model.classes_:\n",
    "        mask = adata_model.obs[var].isin([class_temp])\n",
    "        adata_model[mask].X = sparse.csr_matrix((np.multiply((adata_model[mask].X.todense()), np.array(long_format.loc[class_temp]))))\n",
    "        # Impact of top n features\n",
    "        adata_model.obs.loc[mask, 'model_topn_feature_impact'] = np.log(np.sum(\n",
    "            np.multiply(\n",
    "                adata_model[mask][:, list(long_format[long_format['class'].isin([class_temp])].feature)].X.todense(), \n",
    "                np.array(long_format[long_format['class'].isin([class_temp])]['e^coef'])\n",
    "            ), axis=1\n",
    "        ))\n",
    "    # Total impact of all model features\n",
    "    adata_model.obs['model_class_impact_total'] = np.log([item for sublist in (np.sum(adata_model.X,axis=1)).tolist() for item in sublist])\n",
    "    \n",
    "    return adata_model.obs[['model_topn_feature_impact', 'model_class_impact_total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-primary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "historical-standard",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main_plotting_modules\n",
    "\n",
    "def plot_label_probability_heatmap(pred_out):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the median probabilities of each label for each original label.\n",
    "    \n",
    "    Parameters:\n",
    "    pred_out (pd.DataFrame): A dataframe where each row corresponds to a cell, each column corresponds to a label, and each entry is the probability of the cell being of the label. The dataframe should also contain a 'predicted' column with the predicted label of each cell and an 'orig_labels' column with the original label of each cell.\n",
    "    \"\"\"\n",
    "    model_mean_probs = pred_out.loc[:, pred_out.columns != 'predicted'].groupby('orig_labels').median()\n",
    "    model_mean_probs = model_mean_probs * 100\n",
    "    model_mean_probs = model_mean_probs.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "    crs_tbl = model_mean_probs.copy()\n",
    "\n",
    "    # Sort dataframe columns by rows\n",
    "    index_order = list(crs_tbl.max(axis=1).sort_values(ascending=False).index)\n",
    "    col_order = list(crs_tbl.max(axis=0).sort_values(ascending=False).index)\n",
    "    crs_tbl = crs_tbl.loc[index_order]\n",
    "    crs_tbl = crs_tbl[col_order]\n",
    "\n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.set(font_scale=0.5)\n",
    "    g = sns.heatmap(crs_tbl, cmap='viridis_r',  annot=False, vmin=0, vmax=max(np.max(crs_tbl)), linewidths=1, center=max(np.max(crs_tbl))/2, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "\n",
    "    plt.ylabel(\"Original labels\")\n",
    "    plt.xlabel(\"Training labels\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_crosstab_heatmap(adata, x, y):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the cross-tabulation of two categorical attributes.\n",
    "\n",
    "    Parameters:\n",
    "    adata (anndata.AnnData): The annotated data matrix.\n",
    "    x (str): The name of the attribute to use for the rows of the cross-tabulation.\n",
    "    y (str): The name of the attribute to use for the columns of the cross-tabulation.\n",
    "    \"\"\"\n",
    "    # Compute the cross-tabulation\n",
    "    x_attr = adata.obs[x]\n",
    "    y_attr = adata.obs[y]\n",
    "    crs = pd.crosstab(x_attr, y_attr)\n",
    "\n",
    "    # Normalize each column to sum to 100\n",
    "    crs = crs.div(crs.sum(axis=0), axis=1).multiply(100).round(2)\n",
    "\n",
    "    # Sort the rows and columns of the cross-tabulation\n",
    "    index_order = list(crs.max(axis=1).sort_values(ascending=False).index)\n",
    "    col_order = list(crs.max(axis=0).sort_values(ascending=False).index)\n",
    "    crs = crs.loc[index_order, col_order]\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(20,15))\n",
    "    sns.set(font_scale=0.8)\n",
    "    g = sns.heatmap(crs, cmap='viridis_r', vmin=0, vmax=100, linewidths=1, center=50, square=True, cbar_kws={\"shrink\": 0.3})\n",
    "    plt.ylabel(\"Original labels\")\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.show()\n",
    "\n",
    "def analyze_and_plot_feat_gsea(top_loadings_lowdim, class_name, max_len=20, pre_ranked=True, database='GO_Biological_Process_2021', cutoff=0.25, min_s=5):\n",
    "    \"\"\"\n",
    "    Analyzes and plots the top loadings of a low-dimensional model.\n",
    "\n",
    "    Parameters:\n",
    "    top_loadings_lowdim (pd.DataFrame): A dataframe containing the top loadings for each class.\n",
    "    class_name (str): The name of the class to analyze and plot.\n",
    "    max_len (int): The maximum number of features to plot.\n",
    "    pre_ranked (bool): Whether the data is pre-ranked.\n",
    "    database (str): The name of the database to query for enrichment analysis.\n",
    "    cutoff (float): The cutoff value for enrichment analysis.\n",
    "    min_s (int): The minimum number of genes in a category to consider for enrichment analysis.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Filter the top loadings for the given class\n",
    "    top_loadings_u = top_loadings_lowdim[top_loadings_lowdim['class'] == class_name].head(max_len)\n",
    "    top_loadings_u['gene'] = top_loadings_u['feature']\n",
    "\n",
    "    # Perform enrichment analysis\n",
    "    if not pre_ranked == True:\n",
    "        glist = as_gene_list(top_loadings_u, \"gene\")\n",
    "        enr = enrich_no_rank(glist, [database])\n",
    "    else:\n",
    "        enr = pre_ranked_enr(top_loadings_u, [\"gene\", \"weighted_impact\"], permutation_num=1000, database=database, cutoff=cutoff, min_s=min_s)\n",
    "\n",
    "    # Print the enrichment score range\n",
    "    print(\"Normalised enrichment score ranges, for continuous phenotype tests, a positive value indicates correlation with phenotype profile or top of the ranked list, negative values indicate inverse correlation with profile or correlation with the bottom of the list\")\n",
    "    print(enr.res2d.shape)\n",
    "    # Plot the enrichment results\n",
    "    terms = enr.res2d.Term\n",
    "    axs = enr.plot(terms=terms[1:10], legend_kws={'loc': (1.2, 0)}, show_ranking=True, figsize=(3, 4))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "workhorse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
